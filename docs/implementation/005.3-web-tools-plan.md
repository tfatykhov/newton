# 005.3 Web Tools — Implementation Plan (Post-Review)

**Based on:** 3-agent review (nous-web-arch, nous-web-impl, nous-web-devil)
**Fixes incorporated:** 4 P0, 9 P1, 6 P2 from synthesis of 20+ findings

## Review Summary

All 3 reviewers independently flagged the same top 3 crash bugs (3/3 convergence = highest confidence):
- **P0-A**: Handler signature mismatch (`args: dict` vs `**kwargs` unpacking)
- **P0-B**: Return format wrong (plain dict vs MCP format)
- **P0-C**: httpx client undefined, no lifecycle, credential leak risk if shared

Additional high-confidence findings (2/3 convergence):
- **P0-D**: SSRF vulnerability (no IP/hostname blocklist)
- **P1-A**: Env var prefix mismatch (needs `validation_alias`)
- **P1-B**: Empty API key → silent 401

---

## Files Changed

| File | Change |
|------|--------|
| `nous/config.py` | Add `brave_search_api_key`, `web_search_daily_limit`, `web_fetch_max_chars` with correct aliases |
| `nous/api/web_tools.py` | **NEW** — web_search + web_fetch handlers, SSRF protection, rate limiting, registration |
| `nous/api/runner.py` | Update FRAME_TOOLS + frame instructions for web tools |
| `nous/main.py` | Register web tools + manage web httpx client lifecycle |
| `docker-compose.yml` | Add BRAVE_SEARCH_API_KEY env var |
| `.env.example` | Document BRAVE_SEARCH_API_KEY |
| `tests/test_web_tools.py` | **NEW** — tests for search, fetch, SSRF, rate limiting, extraction |

---

## Phase A: Config Additions

**File: `nous/config.py`** — Add after `workspace_dir` (line 59):

```python
# Web tools
brave_search_api_key: str = Field("", validation_alias="BRAVE_SEARCH_API_KEY")
web_search_daily_limit: int = 100  # Max web searches per day
web_fetch_max_chars: int = 10000  # Default max chars for web_fetch
```

Note: `validation_alias` matches the pattern used by `anthropic_api_key` (line 38) and DB fields (lines 16-20) for unprefixed env vars.

---

## Phase B: Web Tools Module

**File: `nous/api/web_tools.py`** — NEW file

### B1: SSRF Protection

```python
"""Web tools for the Nous agent: web_search and web_fetch.

Gives the agent web access capabilities, gated by cognitive frames.
Uses a separate httpx client (NOT AgentRunner's — that has API credentials).
"""

from __future__ import annotations

import ipaddress
import logging
import socket
import time
from typing import Any

import httpx

from nous.api.tools import ToolDispatcher
from nous.config import Settings

logger = logging.getLogger(__name__)

# Rate limit state (in-memory, resets on restart)
_rate_limit: dict[str, Any] = {"date": "", "count": 0}

# Blocked IP ranges for SSRF protection
_BLOCKED_NETWORKS = [
    ipaddress.ip_network("127.0.0.0/8"),       # Loopback
    ipaddress.ip_network("10.0.0.0/8"),         # RFC1918
    ipaddress.ip_network("172.16.0.0/12"),      # RFC1918
    ipaddress.ip_network("192.168.0.0/16"),     # RFC1918
    ipaddress.ip_network("169.254.0.0/16"),     # Link-local
    ipaddress.ip_network("::1/128"),            # IPv6 loopback
    ipaddress.ip_network("fc00::/7"),           # IPv6 unique local
    ipaddress.ip_network("fe80::/10"),          # IPv6 link-local
]

# Blocked hostnames (Docker internal services, etc.)
_BLOCKED_HOSTNAMES = {"localhost", "postgres", "nous", "redis", "0.0.0.0"}


def _mcp_response(text: str) -> dict[str, Any]:
    """Build MCP-format response."""
    return {"content": [{"type": "text", "text": text}]}


def _is_url_safe(url: str) -> tuple[bool, str]:
    """Check if URL is safe from SSRF attacks.

    Resolves hostname to IP and checks against blocked ranges.
    Returns (is_safe, error_message).
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        hostname = parsed.hostname

        if not hostname:
            return False, "Could not parse hostname from URL"

        # Check blocked hostnames
        if hostname.lower() in _BLOCKED_HOSTNAMES:
            return False, f"Blocked hostname: {hostname}"

        # Resolve to IP and check ranges
        try:
            addr_infos = socket.getaddrinfo(hostname, None)
        except socket.gaierror:
            return False, f"Could not resolve hostname: {hostname}"

        for addr_info in addr_infos:
            ip = ipaddress.ip_address(addr_info[4][0])
            for network in _BLOCKED_NETWORKS:
                if ip in network:
                    return False, f"URL resolves to blocked IP range ({network})"

        return True, ""
    except Exception as e:
        return False, f"URL validation error: {e}"


def _check_rate_limit(settings: Settings) -> str | None:
    """Check and increment daily rate limit.

    Returns error message if limit exceeded, None if OK.
    """
    today = time.strftime("%Y-%m-%d")

    if _rate_limit["date"] != today:
        _rate_limit["date"] = today
        _rate_limit["count"] = 0

    limit = settings.web_search_daily_limit
    current = _rate_limit["count"]

    if current >= limit:
        return f"Daily web search limit reached ({limit}). Resets tomorrow."

    _rate_limit["count"] = current + 1

    if current >= int(limit * 0.8):
        logger.warning("Web search rate limit at %d/%d (%.0f%%)", current + 1, limit, (current + 1) / limit * 100)

    return None
```

### B2: web_search Handler

```python
async def _web_search(
    query: str,
    count: int = 5,
    freshness: str | None = None,
    *,
    _settings: Settings,
    _http: httpx.AsyncClient,
) -> dict[str, Any]:
    """Search via Brave Search API."""
    try:
        # Check API key
        if not _settings.brave_search_api_key:
            return _mcp_response("Error: BRAVE_SEARCH_API_KEY not configured. Set this environment variable to enable web search.")

        # Check rate limit
        rate_error = _check_rate_limit(_settings)
        if rate_error:
            return _mcp_response(f"Rate limit: {rate_error}")

        count = min(count, 10)

        params: dict[str, Any] = {"q": query, "count": count}
        if freshness:
            freshness_map = {"day": "pd", "week": "pw", "month": "pm"}
            mapped = freshness_map.get(freshness)
            if mapped:
                params["freshness"] = mapped

        response = await _http.get(
            "https://api.search.brave.com/res/v1/web/search",
            params=params,
            headers={
                "Accept": "application/json",
                "Accept-Encoding": "gzip",
                "X-Subscription-Token": _settings.brave_search_api_key,
            },
            timeout=10,
        )

        if response.status_code != 200:
            return _mcp_response(f"Search failed (HTTP {response.status_code}). Check BRAVE_SEARCH_API_KEY if 401.")

        data = response.json()
        results = []
        for item in data.get("web", {}).get("results", [])[:count]:
            results.append({
                "title": item.get("title", ""),
                "url": item.get("url", ""),
                "snippet": item.get("description", ""),
            })

        if not results:
            return _mcp_response(f"No results found for: {query}")

        # Format as readable text for LLM
        lines = [f"Search results for: {query}\n"]
        for i, r in enumerate(results, 1):
            lines.append(f"{i}. {r['title']}")
            lines.append(f"   URL: {r['url']}")
            lines.append(f"   {r['snippet']}\n")

        return _mcp_response("\n".join(lines))

    except httpx.TimeoutException:
        return _mcp_response("Web search timed out. Try again.")
    except httpx.ConnectError as e:
        return _mcp_response(f"Could not connect to search service: {e}")
    except Exception as e:
        logger.exception("web_search error")
        return _mcp_response(f"Search error: {e}")
```

### B3: web_fetch Handler

```python
async def _web_fetch(
    url: str,
    max_chars: int | None = None,
    *,
    _settings: Settings,
    _http: httpx.AsyncClient,
) -> dict[str, Any]:
    """Fetch URL and extract readable content."""
    try:
        # Validate URL scheme
        if not url.startswith(("http://", "https://")):
            return _mcp_response("URL must start with http:// or https://")

        # SSRF protection
        is_safe, error = _is_url_safe(url)
        if not is_safe:
            return _mcp_response(f"Blocked: {error}")

        effective_max = min(max_chars or _settings.web_fetch_max_chars, 50000)

        response = await _http.get(
            url,
            headers={"User-Agent": "Nous/0.1 (cognitive agent)"},
            follow_redirects=True,
            timeout=15,
        )

        content_type = response.headers.get("content-type", "")

        # Reject binary content
        is_text = any(t in content_type for t in ["text/", "application/json", "application/xml", "application/xhtml"])
        if content_type and not is_text:
            return _mcp_response(f"Cannot extract text from binary content (content-type: {content_type})")

        if "html" in content_type or "xhtml" in content_type:
            text = _extract_readable(response.text)
        else:
            text = response.text

        if len(text) > effective_max:
            text = text[:effective_max] + "\n\n[... truncated]"

        return _mcp_response(f"Content from {url} ({len(text)} chars):\n\n{text}")

    except httpx.TimeoutException:
        return _mcp_response(f"Fetch timed out for: {url}")
    except httpx.ConnectError as e:
        return _mcp_response(f"Could not connect to {url}: {e}")
    except Exception as e:
        logger.exception("web_fetch error")
        return _mcp_response(f"Fetch error: {e}")
```

### B4: HTML Extraction

```python
def _extract_readable(html: str) -> str:
    """Extract readable text from HTML using stdlib."""
    import html as html_module
    import re

    # Remove script, style, noscript, nav, header, footer tags
    text = re.sub(
        r'<(script|style|noscript|nav|header|footer)[^>]*>.*?</\1>',
        '', html, flags=re.DOTALL | re.IGNORECASE
    )
    # Remove HTML comments
    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', ' ', text)
    # Decode entities
    text = html_module.unescape(text)
    # Clean whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text
```

### B5: Tool Schemas + Registration

```python
_WEB_SEARCH_SCHEMA: dict[str, Any] = {
    "type": "object",
    "description": "Search the web for current information. Returns titles, URLs, and snippets.",
    "properties": {
        "query": {"type": "string", "description": "Search query string"},
        "count": {
            "type": "integer",
            "description": "Number of results (1-10, default 5)",
            "minimum": 1,
            "maximum": 10,
            "default": 5,
        },
        "freshness": {
            "type": "string",
            "description": "Filter by recency: 'day', 'week', 'month', or omit for all time",
            "enum": ["day", "week", "month"],
        },
    },
    "required": ["query"],
}

_WEB_FETCH_SCHEMA: dict[str, Any] = {
    "type": "object",
    "description": "Fetch and extract readable content from a URL. Returns clean text.",
    "properties": {
        "url": {"type": "string", "description": "URL to fetch (must be http or https)"},
        "max_chars": {
            "type": "integer",
            "description": "Maximum characters to return (default from config, max 50000)",
            "maximum": 50000,
        },
    },
    "required": ["url"],
}


def register_web_tools(
    dispatcher: ToolDispatcher,
    settings: Settings,
    http_client: httpx.AsyncClient,
) -> None:
    """Register web tools (web_search, web_fetch) with the dispatcher.

    Creates closure wrappers that inject settings and httpx client.
    Uses a SEPARATE httpx client from AgentRunner (no auth headers).
    """
    async def _search(query: str, count: int = 5, freshness: str | None = None) -> dict[str, Any]:
        return await _web_search(query, count, freshness, _settings=settings, _http=http_client)

    async def _fetch(url: str, max_chars: int | None = None) -> dict[str, Any]:
        return await _web_fetch(url, max_chars, _settings=settings, _http=http_client)

    dispatcher.register("web_search", _search, _WEB_SEARCH_SCHEMA)
    dispatcher.register("web_fetch", _fetch, _WEB_FETCH_SCHEMA)
```

---

## Phase C: Runner Updates

**File: `nous/api/runner.py`**

### C1: FRAME_TOOLS (replace lines 34-41)

```python
FRAME_TOOLS: dict[str, list[str]] = {
    "conversation": ["record_decision", "learn_fact", "recall_deep", "create_censor", "bash", "read_file", "write_file", "web_search", "web_fetch"],
    "question": ["recall_deep", "bash", "read_file", "write_file", "record_decision", "learn_fact", "create_censor", "web_search", "web_fetch"],
    "decision": ["record_decision", "recall_deep", "create_censor", "bash", "read_file", "web_search", "web_fetch"],
    "creative": ["learn_fact", "recall_deep", "write_file", "web_search"],
    "task": ["*"],  # All tools
    "debug": ["record_decision", "recall_deep", "bash", "read_file", "learn_fact", "web_search", "web_fetch"],
}
```

### C2: Frame Instructions (update `_get_frame_instructions`)

Add web tool mentions to relevant frames. After existing frame instruction blocks, add `web_search`/`web_fetch` mentions:

- **question frame**: Add "Use `web_search` and `web_fetch` for questions about current events or topics not in memory."
- **task frame**: Add "Use `web_search` and `web_fetch` for research."
- **debug frame**: Add "Use `web_search` and `web_fetch` to look up documentation or error messages."
- **conversation frame** (add new block): "Use `web_search` and `web_fetch` to find current information when needed."

---

## Phase D: Main Wiring

**File: `nous/main.py`**

### D1: Import (add to imports at top)

```python
from nous.api.web_tools import register_web_tools
```

### D2: Create web httpx client in `create_components()` (after line 63)

```python
    # Web tools httpx client (separate from runner — no API auth headers)
    web_http = httpx.AsyncClient(
        timeout=httpx.Timeout(connect=10, read=30, write=10, pool=10),
        limits=httpx.Limits(max_connections=5, max_keepalive_connections=2),
    )
    register_web_tools(dispatcher, settings, web_http)
```

### D3: Add to components dict (add to return dict)

```python
    "web_http": web_http,
```

### D4: Shutdown (add to `shutdown_components()` before runner close)

```python
    web_http = components.get("web_http")
    if web_http:
        await web_http.aclose()
```

### D5: Startup log (add after existing credential warning, ~line 232)

```python
    if not settings.brave_search_api_key:
        logger.warning("BRAVE_SEARCH_API_KEY not set — web_search will be unavailable")
```

---

## Phase E: Docker + Env

### E1: docker-compose.yml — Add to nous service environment (after NOUS_WORKSPACE_DIR line)

```yaml
      - BRAVE_SEARCH_API_KEY=${BRAVE_SEARCH_API_KEY:-}
```

### E2: .env.example — Add after OPENAI_API_KEY line

```bash
# Web search (optional — web_search tool disabled if omitted)
BRAVE_SEARCH_API_KEY=your_brave_search_key_here
```

---

## Phase F: Tests

**File: `tests/test_web_tools.py`** — NEW

Test categories:
1. **SSRF protection**: `_is_url_safe()` blocks localhost, 127.0.0.1, 169.254.x.x, Docker hostnames; allows public IPs
2. **Rate limiting**: Counter increments, blocks at limit, resets on new day
3. **web_search**: Mock Brave API response, empty results, missing API key, timeout
4. **web_fetch**: Mock HTML page, binary content rejection, SSRF block, truncation, timeout
5. **HTML extraction**: Script/style removal, entity decoding, comment removal, whitespace normalization
6. **Registration**: `register_web_tools()` registers both tools with dispatcher, closures capture settings/client

Use `unittest.mock.AsyncMock` to patch httpx responses (no `respx` dependency needed).

---

## Acceptance Criteria

- [ ] `web_search` returns formatted results from Brave API
- [ ] `web_search` returns clear error when BRAVE_SEARCH_API_KEY not set
- [ ] `web_search` enforces daily rate limit
- [ ] `web_fetch` extracts readable text from HTML pages
- [ ] `web_fetch` rejects binary content types
- [ ] `web_fetch` blocks SSRF attempts (localhost, private IPs, Docker hostnames)
- [ ] Both tools use MCP response format
- [ ] Both tools handle timeouts and network errors gracefully
- [ ] Both tools use closure pattern matching existing builtin_tools.py
- [ ] Separate httpx client (no credential leak from AgentRunner)
- [ ] FRAME_TOOLS correctly updated for all frames
- [ ] Config uses validation_alias for BRAVE_SEARCH_API_KEY
- [ ] docker-compose.yml and .env.example updated
- [ ] All tests pass
