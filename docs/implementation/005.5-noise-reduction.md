# 005.5 Noise Reduction — Episode Dedup, Significance Filtering, Decision Quality

**Status:** PLANNED
**Priority:** P1 — Memory quality directly affects retrieval quality
**Prerequisites:** 005.1 (Smart Context), 005.2 (Direct API)
**Issues:** #17 (episode dedup), #18 (decision noise), #19 (episode significance)
**Estimated Effort:** 3-4 hours
**Branch:** `feat/005.5-noise-reduction`

## Problem

Nous stores too much noise in both Heart (episodes) and Brain (decisions):

1. **Duplicate episodes** (#17): "Hello / What can you do?" stored 12+ times. Every chat turn creates an episode with no dedup check. 36 total episodes, majority noise.
2. **Decision noise** (#18): ~70% of recorded Brain decisions are status reports or activity logs, not real decisions. "Git clone success" is not a decision.
3. **Trivial episodes** (#19): Simple greetings, one-word responses, and trivial exchanges all create episodes. Dilutes retrieval quality.

## Solution

Three independent filters, each applied at a different stage:

```
User message arrives
    │
    ├── pre_turn() ─── [SIGNIFICANCE FILTER] ─── Should we create an episode?
    │                        │                         │
    │                        │ Yes                     │ No → skip episode
    │                        ▼
    │                   [DEDUP CHECK] ─── Similar to recent episode (48h)?
    │                        │                    │
    │                        │ No (new)           │ Yes → skip creation
    │                        ▼
    │                   Create episode
    │
    └── Tool loop ─── record_decision called?
                           │
                      [DECISION FILTER] ─── Is this a real decision?
                           │                    │
                           │ Yes                │ No → warn + skip
                           ▼
                      Record decision
```

## Files Changed

| File | Change | Lines |
|------|--------|-------|
| `nous/cognitive/layer.py` | Add `_should_create_episode()`, add dedup check before `start_episode()`, track session metadata | ~60 |
| `nous/api/runner.py` | Tighten frame instructions for decision/task/debug frames | ~30 |
| `nous/brain/brain.py` | Add `_is_noise_decision()` pre-check in `record()` | ~25 |
| `nous/cognitive/schemas.py` | Add `SessionMetadata` dataclass for turn/tool tracking | ~15 |
| `tests/test_noise_reduction.py` | NEW — all three filter tests | ~200 |

**Total:** ~330 lines new/modified

## Phase A: Episode Significance Filter (#19) (~1h)

### A1: Session Metadata Tracking

Add to `schemas.py` (requires `from dataclasses import dataclass, field` import):

```python
@dataclass
class SessionMetadata:
    """Tracks session-level signals for episode significance."""
    turn_count: int = 0
    tools_used: set[str] = field(default_factory=set)  # P2: set not list for O(1) lookup
    total_user_chars: int = 0
    total_assistant_chars: int = 0
    has_explicit_remember: bool = False
```

**Import note:** `layer.py` must add `SessionMetadata` to its import from `nous.cognitive.schemas`.

### A2: Significance Check in CognitiveLayer

Add to `layer.py`:

```python
# New instance variable in __init__
self._session_metadata: dict[str, SessionMetadata] = {}  # session_id -> metadata

# Significance threshold constants (module-level in layer.py)
_MIN_CONTENT_LENGTH = 200  # Combined user+assistant chars
_MIN_TURNS_WITHOUT_TOOLS = 1  # R: off-by-one fix — turn_count is incremented in post_turn,
                               # so during turn 2's pre_turn, turn_count==1
```

Add method:

```python
def _should_create_episode(self, session_id: str, user_input: str) -> bool:
    """Determine if this interaction is significant enough for an episode.

    Creates episode when ANY of:
    - First turn of session (turn_count == 0)
    - Session has 2+ turns (multi-turn conversation)
    - Tools were used (indicates real work)
    - Combined content exceeds 200 chars AND turn_count >= 1
    - User explicitly asks to remember something

    Always creates on first turn of a session to avoid losing
    the start of significant conversations. The episode will be
    retroactively discarded at end_session if it stays trivial.

    R-P0-1: Check turn_count == 0 (not meta is None) because
    pre_turn tracking creates metadata via setdefault() BEFORE
    this method runs. meta is never None after first pre_turn.
    """
    meta = self._session_metadata.get(session_id)
    if meta is None or meta.turn_count == 0:
        # First turn — always create (will filter at end if trivial)
        return True

    # Explicit remember request
    if meta.has_explicit_remember:
        return True

    # Tools were used — real work happened
    if meta.tools_used:
        return True

    # Multi-turn conversation
    if meta.turn_count >= _MIN_TURNS_WITHOUT_TOOLS:
        return True

    # Content threshold (need at least 1 prior turn)
    # R-P1-1: Don't add len(user_input) — already in meta.total_user_chars
    total_chars = meta.total_user_chars + meta.total_assistant_chars
    if total_chars >= _MIN_CONTENT_LENGTH and meta.turn_count >= 1:
        return True

    return False
```

### A3: Wire into pre_turn and post_turn

In `pre_turn()`, replace the current episode creation block (lines 170-182):

```python
# 5. EPISODE — start if no active episode AND interaction is significant
if session_id not in self._active_episodes:
    if self._should_create_episode(session_id, user_input):
        try:
            episode_input = EpisodeInput(
                summary=user_input[:200],
                frame_used=frame.frame_id,
                trigger="user_message",
            )
            episode = await self._heart.start_episode(episode_input, session=session)
            self._active_episodes[session_id] = str(episode.id)
        except Exception:
            logger.warning("Failed to start episode for session %s", session_id)
```

In `post_turn()`, update session metadata (add after usage tracking, before event emit):

```python
# Update session metadata for significance tracking
meta = self._session_metadata.setdefault(session_id, SessionMetadata())
meta.turn_count += 1
meta.total_assistant_chars += len(turn_result.response_text)
# Track tool usage (set — O(1) add)
for tr in turn_result.tool_results:
    meta.tools_used.add(tr.tool_name)
```

In `pre_turn()`, track user input chars (add near start):

```python
# Track user input for significance
meta = self._session_metadata.setdefault(session_id, SessionMetadata())
meta.total_user_chars += len(user_input)
# Check for explicit remember request
if any(kw in user_input.lower() for kw in ("remember this", "remember that", "don't forget", "save this")):
    meta.has_explicit_remember = True
```

### A4: Discard Trivial Episodes at Session End

In `end_session()`, before ending the episode, check if it was trivial:

```python
# 1. End active episode (or discard if trivial)
episode_id = self._active_episodes.pop(session_id, None)
meta = self._session_metadata.pop(session_id, None)

if episode_id:
    try:
        # Discard trivial episodes: single turn, no tools, short content
        is_trivial = (
            meta is not None
            and meta.turn_count <= 1
            and not meta.tools_used
            and (meta.total_user_chars + meta.total_assistant_chars) < _MIN_CONTENT_LENGTH
        )

        if is_trivial:
            # Soft-delete the episode instead of keeping noise
            await self._heart.deactivate_episode(UUID(episode_id), session=session)
            logger.debug("Discarded trivial episode %s", episode_id)
        else:
            lessons = None
            if reflection:
                lessons = [reflection[:500]]
            await self._heart.end_episode(
                UUID(episode_id),
                outcome="success",
                lessons_learned=lessons,
                session=session,
            )
    except Exception:
        logger.warning("Failed to end episode %s", episode_id)
```

### A5: Add deactivate_episode to Heart/EpisodeManager

Add to `episodes.py`:

```python
async def deactivate(self, episode_id: UUID, session: AsyncSession | None = None) -> None:
    """Soft-delete an episode (set active=False)."""
    if session is None:
        async with self.db.session() as session:
            await self._deactivate(episode_id, session)
            await session.commit()
            return
    await self._deactivate(episode_id, session)

async def _deactivate(self, episode_id: UUID, session: AsyncSession) -> None:
    episode = await self._get_episode_orm(episode_id, session)
    if episode:
        episode.active = False
        await session.flush()
```

Add delegation in `heart.py`:

```python
async def deactivate_episode(self, episode_id: UUID, session: AsyncSession | None = None) -> None:
    """Soft-delete a trivial episode."""
    await self._episodes.deactivate(episode_id, session=session)
```

## Phase B: Episode Deduplication (#17) (~1h)

### B1: Dedup Check Before Episode Creation

Add to `layer.py`, called from the episode creation path:

```python
async def _is_duplicate_episode(
    self,
    summary: str,
    session: AsyncSession | None = None,
) -> bool:
    """Check if a similar recent episode already exists.

    Returns True if a recent episode (within 48h) with >0.85 cosine
    similarity exists, meaning we should skip creating a new episode.

    R-P0-2: Returns bool, NOT episode_id. We never store reused IDs in
    _active_episodes because end_session would corrupt/delete the
    original episode.

    R-P1-2: Uses direct cosine similarity via EmbeddingProvider, NOT
    hybrid_search (which returns 0.7*vector + 0.3*keyword combined scores
    that max at ~0.79 for perfect vector match — making 0.85 unreachable).

    R-P1-3: Filters to episodes started within last 48 hours to avoid
    matching ancient episodes about similar topics.
    """
    if not self._heart._episodes.embeddings:
        return False  # No embeddings available — skip dedup

    try:
        # Generate embedding for current input
        query_embedding = await self._heart._episodes.embeddings.embed(summary)

        # Search recent episodes with direct cosine similarity
        results = await self._heart.search_recent_episodes_by_embedding(
            query_embedding,
            hours=48,
            limit=1,
            session=session,
        )
        if results and results[0][1] > 0.85:
            logger.debug(
                "Found duplicate episode (%.2f cosine similarity), skipping creation",
                results[0][1],
            )
            return True
    except Exception:
        logger.warning("Episode dedup check failed, proceeding with creation")
    return False
```

### B2: Wire Dedup into Episode Creation (supersedes A3)

This is the FINAL version of the episode creation block in `pre_turn()`.
Replaces lines 170-182 of `layer.py`:

```python
# 5. EPISODE — start if no active episode AND interaction is significant
if session_id not in self._active_episodes:
    if self._should_create_episode(session_id, user_input):
        try:
            # B1: Check for duplicate — skip creation if found
            # R-P0-2: Do NOT store existing episode IDs in _active_episodes
            # because end_session would corrupt the original episode.
            if await self._is_duplicate_episode(user_input[:200], session=session):
                logger.debug("Skipping episode creation — duplicate found")
            else:
                episode_input = EpisodeInput(
                    summary=user_input[:200],
                    frame_used=frame.frame_id,
                    trigger="user_message",
                )
                episode = await self._heart.start_episode(episode_input, session=session)
                self._active_episodes[session_id] = str(episode.id)
        except Exception:
            logger.warning("Failed to start episode for session %s", session_id)
```

### B3: Direct Cosine Episode Search in Heart

Add to `episodes.py` — a targeted method for dedup that uses direct cosine
similarity instead of hybrid_search (which combines vector + keyword scores):

```python
async def search_recent_by_embedding(
    self,
    query_embedding: list[float],
    hours: int = 48,
    limit: int = 1,
    session: AsyncSession | None = None,
) -> list[tuple[UUID, float]]:
    """Find recent episodes by direct cosine similarity.

    Returns list of (episode_id, cosine_similarity) tuples.
    Only searches episodes within the given time window.
    """
    if session is None:
        async with self.db.session() as session:
            return await self._search_recent_by_embedding(
                query_embedding, hours, limit, session
            )
    return await self._search_recent_by_embedding(
        query_embedding, hours, limit, session
    )

async def _search_recent_by_embedding(
    self,
    query_embedding: list[float],
    hours: int,
    limit: int,
    session: AsyncSession,
) -> list[tuple[UUID, float]]:
    from sqlalchemy import text

    sql = text("""
        SELECT id, 1 - (embedding <=> :embedding::vector) AS cosine_sim
        FROM heart.episodes
        WHERE agent_id = :agent_id
          AND active = true
          AND embedding IS NOT NULL
          AND started_at > NOW() - make_interval(hours => :hours)
        ORDER BY embedding <=> :embedding::vector
        LIMIT :limit
    """)
    result = await session.execute(sql, {
        "embedding": str(query_embedding),
        "agent_id": self.agent_id,
        "hours": hours,
        "limit": limit,
    })
    return [(row[0], float(row[1])) for row in result.fetchall()]
```

Add delegation in `heart.py`:

```python
async def search_recent_episodes_by_embedding(
    self,
    query_embedding: list[float],
    hours: int = 48,
    limit: int = 1,
    session: AsyncSession | None = None,
) -> list[tuple[UUID, float]]:
    """Search recent episodes by direct cosine similarity for dedup."""
    return await self._episodes.search_recent_by_embedding(
        query_embedding, hours=hours, limit=limit, session=session
    )
```

## Phase C: Decision Noise Filter (#18) (~1h)

### C1: Tighten Frame Instructions

Update `runner.py` `_get_frame_instructions()`:

**Decision frame** (line 501-509):

```python
if frame_id == "decision":
    return (
        "## Tool Instructions\n\n"
        "You are in a DECISION frame. You MUST call `record_decision` "
        "to record your decision before responding. Include your reasoning, "
        "confidence level, and category.\n\n"
        "**What IS a decision:** A choice between alternatives — architecture "
        "choices, tool selections, process changes, trade-offs with pros/cons.\n\n"
        "**What is NOT a decision:** Status reports, routine completions, "
        "simple observations, task acknowledgments, greetings. "
        "Do NOT record these.\n\n"
        "Use `recall_deep` to search for relevant past decisions. "
        "Use `web_search` and `web_fetch` to research options before deciding."
    )
```

**Task frame** (line 510-519):

```python
elif frame_id == "task":
    return (
        "## Tool Instructions\n\n"
        "You are in a TASK frame. If you make a meaningful choice between "
        "alternatives during this task, call `record_decision` to record it. "
        "Do NOT record routine task completions, status updates, or simple "
        "observations as decisions — a decision requires choosing between "
        "alternatives with trade-offs.\n\n"
        "Use `recall_deep` to search for relevant past decisions and knowledge. "
        "Use `learn_fact` to store any new facts discovered. You can also use "
        "`bash`, `read_file`, and `write_file` for system operations. "
        "Use `web_search` and `web_fetch` for research."
    )
```

**Debug frame** (line 520-528):

```python
elif frame_id == "debug":
    return (
        "## Tool Instructions\n\n"
        "You are in a DEBUG frame. Use `recall_deep` to search for relevant "
        "past decisions and procedures. Record meaningful debugging decisions "
        "(e.g., root cause identified, fix approach chosen) with "
        "`record_decision`. Do NOT record routine debug steps or status "
        "observations. Store root cause findings with `learn_fact`. "
        "Use `bash` and `read_file` for investigation. Use `web_search` and "
        "`web_fetch` to look up documentation or error messages."
    )
```

### C2: Brain Pre-Check Filter

Add to `brain.py`, called at the start of `_record()`:

```python
# Noise indicators — short descriptions with no alternatives/reasoning signal
_NOISE_KEYWORDS = frozenset({
    "completed", "done", "finished", "success", "started",
    "status", "progress", "update", "checked", "confirmed",
})

def _is_noise_decision(self, description: str, reasons: list[ReasonInput]) -> bool:
    """Lightweight pre-check to detect obvious non-decisions.

    Returns True if the description looks like a status report
    rather than a real decision. Checks:
    1. Very short description (<20 chars) with no reasons
    2. Description is mostly noise keywords with no reasoning

    This is a SOFT filter — it logs a warning but does not block.
    The frame instruction changes (C1) are the primary fix.
    """
    desc_lower = description.lower().strip()

    # Very short with no reasons — almost certainly noise
    if len(desc_lower) < 20 and not reasons:
        return True

    # R-P1-4: Use regex tokenization to strip punctuation
    # "completed." -> "completed" (matches _NOISE_KEYWORDS)
    import re
    words = set(re.findall(r'\w+', desc_lower))
    if not words:
        return True
    noise_count = len(words & _NOISE_KEYWORDS)
    # If >50% of words are noise keywords and no reasons provided
    if noise_count / len(words) > 0.5 and not reasons:
        return True

    return False
```

Wire into `_record()`:

```python
async def _record(self, input: RecordInput, session: AsyncSession) -> DecisionDetail:
    # C2: Noise check — warn but still record (soft filter)
    if self._is_noise_decision(input.description, input.reasons):
        logger.warning(
            "Possible noise decision detected: '%s' — "
            "consider if this is a real choice between alternatives",
            input.description[:80],
        )

    # ... existing record logic continues unchanged ...
```

## Phase D: Tests (~1h)

### D1: Test Cases

```python
# tests/test_noise_reduction.py

class TestEpisodeSignificance:
    """Tests for _should_create_episode() in CognitiveLayer."""

    # 1. First turn of session -> True (always create)
    # 2. Second turn, no tools, short content -> False
    # 3. Second turn, tools used -> True
    # 4. Third turn, no tools -> True (multi-turn)
    # 5. Second turn, content > 200 chars -> True
    # 6. Explicit "remember this" in input -> True
    # 7. Trivial episode discarded at end_session
    # 8. Non-trivial episode kept at end_session

class TestEpisodeDedup:
    """Tests for _is_duplicate_episode() in CognitiveLayer."""

    # 9. No similar episode -> returns False
    # 10. Similar episode (>0.85 cosine) -> returns True
    # 11. Similar but below threshold (0.80) -> returns False
    # 12. Embedding failure -> returns False (graceful degradation)
    # 13. Empty search results -> returns False
    # 14. Duplicate skips episode creation (no episode in _active_episodes)
    # 15. Old duplicate (>48h) not matched (time window filter)

class TestDecisionNoise:
    """Tests for _is_noise_decision() in Brain."""

    # 16. Short status report (<20 chars, no reasons) -> True
    # 17. Real decision with alternatives -> False
    # 18. Noise keywords >50% with no reasons -> True
    # 19. Noise keywords but WITH reasons -> False
    # 20. Empty description -> True
    # 21. Normal description with reasons -> False
    # 22. Punctuation-attached keywords detected ("completed." matches)

class TestFrameInstructions:
    """Tests for updated frame instructions in AgentRunner."""

    # 23. Decision frame mentions "what IS a decision"
    # 24. Decision frame mentions "what is NOT a decision"
    # 25. Task frame warns against routine completions
    # 26. Debug frame warns against routine debug steps
```

## Design Decisions

| # | Decision | Rationale |
|---|----------|-----------|
| D1 | Significance filter is soft (create on first turn, discard at end) | Avoids losing the start of conversations that become significant later. Deferred evaluation is safer. |
| D2 | Dedup uses direct cosine similarity with 0.85 threshold + 48h window | R: hybrid_search returns combined scores (0.7*vector+0.3*keyword) that max at ~0.79, making 0.85 unreachable. Direct cosine is consistent with ConversationDeduplicator. 48h window prevents matching ancient episodes. |
| D3 | Brain noise filter is warning-only, not blocking | Frame instruction changes are the primary fix. Hard blocking could lose legitimate decisions. |
| D4 | Session metadata tracks in-memory (not DB) | This is ephemeral per-session data. No persistence needed — it resets each session. |
| D5 | Trivial episodes soft-deleted (active=False) | Consistent with existing soft-delete pattern. Can be recovered if needed. |
| D6 | Remember keywords checked case-insensitive | "Remember this", "REMEMBER THIS", "Remember This" all trigger. Simple and robust. |

## Acceptance Criteria

- [ ] First turn always creates an episode (deferred significance check)
- [ ] Trivial single-turn episodes are soft-deleted at session end
- [ ] Multi-turn conversations DO create episodes
- [ ] Tool usage triggers episode creation
- [ ] Duplicate episodes (>0.85 cosine similarity within 48h) skip creation
- [ ] Decision frame instructions explicitly define what IS/ISN'T a decision
- [ ] Task frame instructions warn against recording routine completions
- [ ] Brain logs warning for noise-like decisions
- [ ] `_should_create_episode()` returns True for "remember this" requests
- [ ] All 26 tests pass
- [ ] Existing episode/decision tests still pass (no regression)

## Non-Goals

- **No cleanup script** for existing duplicates (separate task, not in this spec)
- **No hard blocking** of noise decisions (too risky, could lose real decisions)
- **No LLM-based significance scoring** (would add latency; pattern-based is sufficient)
- **No database schema changes** (all changes are application-level filtering)

## Review Findings (2026-02-24)

3-agent review team (architect, db-specialist, devil's advocate). All P0/P1 fixed above.

### P0 BLOCKERS (Fixed)

| # | Issue | Fix Applied |
|---|-------|-------------|
| P0-1 | First-turn detection broken — `setdefault()` creates metadata before `_should_create_episode` checks `meta is None` | Changed to `meta.turn_count == 0` check (A2) |
| P0-2 | Dedup reuse corrupts historical episodes — storing existing episode_id in `_active_episodes` means `end_session` overwrites/deletes the original | Changed to skip creation entirely, never store reused IDs (B2) |

### P1 CRITICAL (Fixed)

| # | Issue | Fix Applied |
|---|-------|-------------|
| P1-1 | Double-counting user_input in content length | Removed `+ len(user_input)` from total_chars (A2) |
| P1-2 | Hybrid search score (0.7v+0.3k) maxes at 0.79, making 0.85 unreachable | Switched to direct cosine similarity query (B1/B3) |
| P1-3 | No time window on dedup — matches ancient episodes | Added 48h recency filter (B1/B3) |
| P1-4 | `split()` doesn't strip punctuation — "completed." misses match | Changed to `re.findall(r'\w+', ...)` (C2) |
| P1-5 | Missing SessionMetadata import in layer.py + dataclass import in schemas.py | Added import notes (A1) |
| P1-6 | Off-by-one: `turn_count >= 2` requires 3 actual turns | Changed `_MIN_TURNS_WITHOUT_TOOLS` to 1 (A2) |
| P1-7 | Keyword-only fallback makes dedup threshold unreachable | Moot — B1 now uses direct cosine, not hybrid_search |
| P1-8 | `_session_metadata` memory leak on abandoned sessions | Acknowledged as known limitation alongside `_active_episodes` (P2-10 in layer.py) |

### P2 IMPROVEMENTS (Implementation Guidance)

- Use `set[str]` for `tools_used` (not `list`) — O(1) membership check
- Type hint `reasons: list[ReasonInput]` in `_is_noise_decision`
- Narrow "remember" keyword matching to reduce false positives on "save this file"
- Update `end_session` docstring to mention significance-based discard
- Add test for time-window filtering (test #15) and punctuation matching (test #22)
