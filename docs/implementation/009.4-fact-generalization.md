# Spec 009.4: Fact Generalization (Memory Compaction)

**Status:** Draft
**Depends on:** 009.1 (Reactive Lifecycle — dedup), 009.2 (Maintenance Engine)
**Phase:** 4 of 4 (F008 implementation)
**Research:** 013 (LangChain Agent Builder Lessons), 016 (Memory Gap Analysis)

## Problem

Agents accumulate specific facts instead of generalizing. Over time, this creates:

- **Context bloat** — 15 facts about individual vendors to ignore, instead of one rule "ignore cold outreach email"
- **Fragile recall** — specific facts only match specific queries. A general rule matches broadly.
- **Redundant information** — multiple facts expressing the same underlying pattern, each worded slightly differently

### Example

Before generalization:
```
fact_1: "Tim prefers dark mode in VS Code"
fact_2: "Tim uses dark mode in Telegram"
fact_3: "Tim switched Discord to dark mode"
fact_4: "Tim asked for dark theme on the dashboard"
fact_5: "Tim prefers dark backgrounds for code"
```

After generalization:
```
general_fact: "Tim consistently prefers dark mode/themes across all applications"
  ← supersedes fact_1 through fact_5
```

One general fact, better recall, less context tokens.

---

## Architecture

### Trigger-Based Generalization

```
Heart.learn_fact()
  → Event: fact_learned (includes subject/domain)
  → FactDomainMonitor: count active facts in domain
  → If count > threshold (default: 10):
      → Queue domain for generalization
  
MaintenanceEngine (periodic, from 009.2)
  → FactGeneralizer: process queued domains
  → For each domain:
      1. Load all active facts in domain
      2. LLM: merge N specific facts into 1-3 general rules
      3. Store general rules as new CORE facts
      4. Mark originals as superseded_by = general fact ID
      5. Log compaction event
```

### Why Not Fully Event-Driven?

Generalization is expensive (LLM call with many facts as input) and not urgent. Running it in the maintenance engine (009.2) avoids blocking learn_fact() and lets us batch domains.

### Module Structure

```
nous/heart/
  ├── lifecycle/
  │   ├── fact_generalize.py   — FactGeneralizer + FactDomainMonitor
  │   └── ...
  └── maintenance/
      └── engine.py            — add FactGeneralizer as maintenance task
```

---

## Component Details

### 1. Domain Detection

**What is a "domain"?**

Facts don't have an explicit domain field. We derive domain from `subject`:

```python
def _get_domain(self, subject: str | None) -> str | None:
    """Extract domain from fact subject.
    
    Simple approach: use the subject itself as the domain.
    Facts with similar subjects cluster into the same domain.
    
    Future: could use LLM to normalize ("Tim's VS Code" → "Tim preferences")
    """
    if not subject:
        return None
    return subject.strip().lower()
```

**Alternative — cluster by embedding similarity:**

Instead of exact subject match, cluster facts with embedding similarity > 0.7 into domains. More robust but more expensive. Start with subject-based, upgrade if needed.

### 2. Domain Monitor (Event-Driven)

```python
class FactDomainMonitor:
    """Tracks fact counts per domain, queues for generalization when threshold hit."""
    
    GENERALIZATION_THRESHOLD = 10
    
    async def on_fact_learned(
        self,
        event: FactLearnedEvent,
        session: AsyncSession,
    ) -> None:
        domain = self._get_domain(event.subject)
        if not domain:
            return
        
        # Count active facts in this domain
        count = await self._count_domain_facts(domain, event.agent_id, session)
        
        if count >= self.GENERALIZATION_THRESHOLD:
            await self._queue_for_generalization(domain, event.agent_id, session)
    
    async def _queue_for_generalization(
        self,
        domain: str,
        agent_id: str,
        session: AsyncSession,
    ) -> None:
        """Add domain to generalization queue (idempotent)."""
        await session.execute(
            insert(GeneralizationQueue)
            .values(domain=domain, agent_id=agent_id)
            .on_conflict_do_nothing()
        )
```

### 3. Fact Generalizer (Maintenance Task)

```python
class FactGeneralizer(MaintenanceTask):
    name = "fact_generalizer"
    
    MAX_FACTS_PER_DOMAIN = 30  # Cap to control LLM input size
    
    async def run(self, session: AsyncSession) -> dict:
        stats = {"domains_processed": 0, "facts_generalized": 0, "rules_created": 0}
        
        # 1. Get queued domains
        queued = await self._get_queued_domains(session)
        
        for item in queued:
            try:
                result = await self._generalize_domain(
                    domain=item.domain,
                    agent_id=item.agent_id,
                    session=session,
                )
                stats["domains_processed"] += 1
                stats["facts_generalized"] += result.facts_superseded
                stats["rules_created"] += result.rules_created
                
                # Remove from queue
                await self._dequeue(item.id, session)
                
            except Exception:
                logger.exception("Failed to generalize domain: %s", item.domain)
        
        return stats
    
    async def _generalize_domain(
        self,
        domain: str,
        agent_id: str,
        session: AsyncSession,
    ) -> GeneralizationResult:
        # 1. Load all active facts in domain
        facts = await self._load_domain_facts(
            domain, agent_id, session,
            limit=self.MAX_FACTS_PER_DOMAIN,
        )
        
        if len(facts) < 5:
            # Not enough to generalize (might have been deduped since queuing)
            return GeneralizationResult(facts_superseded=0, rules_created=0)
        
        # 2. LLM: merge specific facts into general rules
        general_rules = await self._llm_generalize(facts)
        
        # 3. Store general rules as CORE facts
        rule_ids = []
        for rule in general_rules:
            new_fact = await self._heart.learn_fact(
                content=rule.content,
                subject=rule.subject,
                source="generalization",
                agent_id=agent_id,
                session=session,
            )
            if new_fact:
                # Mark as CORE (high confirmation)
                await self._heart.confirm_fact(new_fact.id, session=session)
                await self._heart.confirm_fact(new_fact.id, session=session)
                await self._heart.confirm_fact(new_fact.id, session=session)
                rule_ids.append(new_fact.id)
        
        # 4. Supersede originals
        for fact in facts:
            if rule_ids:
                await self._heart.supersede_fact(
                    old_id=fact.id,
                    new_id=rule_ids[0],  # Link to primary general rule
                    session=session,
                )
        
        return GeneralizationResult(
            facts_superseded=len(facts),
            rules_created=len(rule_ids),
        )
```

**Generalization prompt:**

```
You have {n} specific facts about "{domain}". Merge them into 1-3 general rules
that capture the same information more concisely.

Requirements:
- Each rule should be a single, clear statement
- Rules should be general enough to cover the specific facts
- Don't lose important distinctions (if two facts genuinely differ, keep separate rules)
- Return JSON array: [{{"subject": "topic", "content": "the general rule"}}]
- Maximum 3 rules

Specific facts:
{numbered_facts}
```

---

## Schema Changes

```sql
-- 012_fact_generalization.sql

-- Generalization queue
CREATE TABLE IF NOT EXISTS heart.generalization_queue (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_id TEXT NOT NULL,
    domain TEXT NOT NULL,
    queued_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(agent_id, domain)
);

-- Track which facts are "general rules" vs specific
ALTER TABLE heart.facts
    ADD COLUMN IF NOT EXISTS is_generalized BOOLEAN DEFAULT false;
```

---

## Generalization Flow (End to End)

```
Day 1: learn_fact("Tim uses dark mode in VS Code", subject="Tim preferences")
Day 2: learn_fact("Tim uses dark mode in Telegram", subject="Tim preferences")
  ...
Day N: learn_fact("Tim prefers dark theme on dashboard", subject="Tim preferences")
  → FactDomainMonitor: count("Tim preferences") = 10 → queue for generalization

Next maintenance run:
  → FactGeneralizer picks up "Tim preferences" domain
  → Loads 10 facts
  → LLM merges into: "Tim consistently prefers dark mode/themes across all applications"
  → Stores as new CORE fact (is_generalized=true)
  → Supersedes all 10 originals (superseded_by = new fact ID)
  → Net result: 10 facts → 1 general rule
```

---

## Safeguards

### Don't Generalize Everything

Some facts should stay specific:
- **Credentials/config** — "CSTP server is at 192.168.1.141:9991" should never be generalized
- **One-off events** — "Tim was in New York on Feb 5" is not generalizable
- **Active project context** — specific facts about current work should persist

**Heuristic:** Only generalize when:
1. Domain has 10+ active facts (threshold)
2. Facts have overlapping content (LLM judges, not just count)
3. Facts are not tagged as `source="user"` (explicitly told facts are sacred)

### Audit Trail

Every generalization creates a clear trail:
- Original facts: `superseded_by = general_fact_id`
- General fact: `is_generalized = true`, `source = "generalization"`
- Maintenance log: records which domain, how many facts, what rules

### Reversibility

Superseded facts aren't deleted. If a generalization is wrong:
1. Deactivate the general fact
2. Reactivate the originals (clear `superseded_by`)
3. Add domain to a "don't generalize" list

---

## Testing Strategy

### Unit tests

- `test_domain_monitor.py`
  - Queues domain when threshold reached
  - Idempotent (doesn't queue same domain twice)
  - Ignores facts without subject

- `test_fact_generalizer.py`
  - 10 related facts → 1-3 general rules + originals superseded
  - < 5 facts in domain → skip (already deduped)
  - General rules stored as CORE (high confirmation_count)
  - Originals all point to general rule via superseded_by

- `test_generalization_safeguards.py`
  - User-sourced facts excluded from generalization
  - Deactivated general fact → originals recoverable

### Integration tests

- Full flow: learn 10+ facts in domain → maintenance runs → domain generalized
- Generalized domain doesn't re-queue after processing
- Subsequent facts in same domain trigger new round if threshold hit again

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Over-generalization loses important details | High | LLM prompt explicitly says "don't lose important distinctions." Max 3 rules preserves some specificity. |
| Wrong generalization supersedes correct facts | High | Audit trail + reversibility. Superseded facts not deleted. |
| Domain detection too coarse | Medium | Start with subject-based. Upgrade to embedding clustering if needed. |
| LLM cost for large domains | Low | MAX_FACTS_PER_DOMAIN cap (30). Expected <5 domains per maintenance run. |
| Generalization of config/credentials | High | Source-based exclusion (source="user" protected). Could add domain blocklist. |

---

## Success Criteria

- Domains with 10+ similar facts auto-compact to 1-3 general rules
- Original facts preserved (superseded, not deleted) for audit
- Context token usage for fact-heavy domains reduced by 50%+
- No loss of critical specific facts (credentials, config, one-off events)
- Generalization runs as part of normal maintenance (no manual trigger needed)

---

## Open Questions

1. **Should generalization be opt-in per domain?**
   Some domains (user preferences) are great candidates. Others (project-specific technical facts) might lose important detail. Consider a domain allowlist/blocklist.

2. **When to re-generalize?**
   After generalization, new specific facts keep arriving. When the domain hits threshold again (10 new facts since last generalization), should we re-generalize including the existing general rule? Or merge new facts into the existing rule?

3. **How to handle conflicting general rules?**
   If two generalization runs produce overlapping rules, the supersession logic from 009.1 should catch it. But worth testing.
