# 004.1: CEL Expression Guardrails ‚Äî Implementation Plan

**Source Spec:** [004.1-cel-guardrails.md](004.1-cel-guardrails.md)
**Date:** 2026-02-23
**Review:** 2026-02-23 ‚Äî Completed by nous-arch, nous-minsky, nous-devil (D1-D13)

## Minsky Context

### Censors as Suppressor Agents (Ch. 9)

Guardrails are Minsky's censors ‚Äî agents that suppress bad actions before they happen. The current JSONB evaluator is a primitive censor: it recognizes a fixed set of patterns and blocks. CEL transforms censors from pattern-matchers into expression-evaluators ‚Äî closer to Minsky's vision of censors that can reason about context.

Key Minsky principles applied:

1. **Censors should suppress, not modify (Ch. 9.3).** CEL guardrails return block/warn ‚Äî they don't transform the decision. The decision either passes or doesn't. This is correct behavior for a censor.

2. **Censors need context to be effective (Ch. 9.4).** The current guardrail system fails because it can't see enough context (only 4 hardcoded fields). CEL opens the full decision context to censors, making them actually useful.

3. **Administrative agents need flexibility (Ch. 10, Papert's Principle).** The JSONB evaluator requires code changes for every new guardrail condition. CEL makes guardrails data-driven ‚Äî an administrative improvement that multiplies the value of existing knowledge.

4. **Parallel bundles for robust decisions (Ch. 18).** Multiple guardrails evaluate independently. A decision must survive ALL of them. This is a parallel bundle ‚Äî failure of any single guardrail blocks the action. CEL makes it practical to have many guardrails (easy to write, cheap to evaluate).

### Multi-Agent Implications

When multiple Nous agents share a Brain (F013 future), guardrails must:
- Evaluate in the context of the requesting agent (not a global context)
- Support agent-specific variables in CEL (e.g., `agent.trust_level`)
- Not leak one agent's context to another's guardrail evaluation

The `decision.*` namespace is already scoped per-evaluation. For multi-agent, we'll add `agent.*` namespace later without breaking existing expressions.

## Architecture Review Questions

For the 3-agent review team:

### Architecture (nous-arch)
1. Is `cel-python` the right dependency? Alternatives? Risk of abandonment?
2. Should CEL compilation happen at guardrail load time (startup) or evaluation time (lazy)?
3. Is the `decision.*` namespace sufficient? Should we add `agent.*` now for future multi-agent?
4. How should CEL errors propagate? Fail-open is specified but has security implications.

### Minsky/Cognitive (nous-minsky)
1. Does CEL align with Minsky's censor model? Are there aspects of censors we're missing?
2. Should guardrails have priority ordering (Ch. 9 ‚Äî "some censors should override others")?
3. Could CEL expressions reference other guardrails' results? (Ch. 18 ‚Äî cross-connected networks)
4. How does this interact with the Cognitive Layer's frame-based guardrail bypass (creative frame has fewer censors)?

### Devil's Advocate (nous-devil)
1. What happens if cel-python has a security vulnerability? Sandboxing claims?
2. Can a malicious CEL expression cause DoS (infinite loop, memory exhaustion)?
3. What if legacy JSONB auto-conversion produces wrong CEL? Silent behavior change?
4. Program cache growth ‚Äî unbounded? Memory leak potential?
5. What if a guardrail expression references a field that doesn't exist? Crash or false?

## Implementation Details

### CEL Environment Setup

```python
import celpy
from celpy import celtypes

# Shared environment ‚Äî compiled once at module load
_CEL_ENV = celpy.Environment()

# Type declarations for the decision namespace
# CEL needs to know the structure for type checking
_DECISION_TYPE = celpy.Environment(
    annotations={
        "decision": celtypes.MapType,
    }
)
```

**Open question:** `cel-python` has two modes:
1. **Untyped** ‚Äî `Environment()` with no annotations. Any field access works, but runtime errors on missing fields.
2. **Typed** ‚Äî `Environment(annotations=...)` with Protobuf-like type declarations. Compile-time checking but requires schema maintenance.

Recommendation: Start untyped for flexibility, add type checking later.

### Activation Context Construction

```python
def _build_activation(
    description: str,
    stakes: str,
    confidence: float,
    category: str | None = None,
    tags: list[str] | None = None,
    reasons: list[dict] | None = None,
    pattern: str | None = None,
    quality_score: float | None = None,
    context: dict | None = None,
) -> dict[str, celtypes.Value]:
    """Build CEL activation with decision.* namespace."""
    decision_map = {
        "description": celtypes.StringType(description),
        "stakes": celtypes.StringType(stakes),
        "confidence": celtypes.DoubleType(confidence),
        "category": celtypes.StringType(category or ""),
        "tags": celtypes.ListType([celtypes.StringType(t) for t in (tags or [])]),
        "reason_count": celtypes.IntType(len(reasons or [])),
        "pattern": celtypes.StringType(pattern or ""),
        "quality_score": celtypes.DoubleType(quality_score if quality_score is not None else 0.0),
        "has_pattern": celtypes.BoolType(bool(pattern)),
        "has_tags": celtypes.BoolType(len(tags or []) > 0),
        "context": _dict_to_cel_map(context or {}),
    }
    return {"decision": celtypes.MapType(
        {celtypes.StringType(k): v for k, v in decision_map.items()}
    )}
```

### Legacy Conversion

```python
_JSONB_TO_CEL = {
    "stakes": lambda v: f"decision.stakes == '{v}'",
    "confidence_lt": lambda v: f"decision.confidence < {v}",
    "reason_count_lt": lambda v: f"decision.reason_count < {v}",
    "quality_lt": lambda v: f"decision.quality_score < {v}",
}

def _jsonb_to_cel(condition: dict) -> str:
    parts = []
    for key, value in condition.items():
        converter = _JSONB_TO_CEL.get(key)
        if converter:
            parts.append(converter(value))
        else:
            logger.warning("Unknown legacy condition key: %s", key)
    return " && ".join(parts) if parts else "false"
```

### Brain.check() Signature Update

```python
# Current:
async def check(self, description, stakes, confidence, tags, reasons, pattern, quality_score, session)

# New ‚Äî add category + context:
async def check(self, description, stakes, confidence, category=None, tags=None, reasons=None, pattern=None, quality_score=None, context=None, session=None)
```

### Seed Data Migration

Current seed guardrails use JSONB:
```json
{"stakes": "high", "confidence_lt": 0.5}
```

New seed guardrails use CEL:
```json
{"cel": "decision.stakes == 'high' && decision.confidence < 0.5"}
```

**Migration strategy:** Don't change existing DB rows. Legacy auto-conversion handles them. Only new seed.sql uses CEL format. Users can migrate at their own pace.

## Test Plan

### Backward Compatibility (Critical)
```python
# test_legacy_jsonb_stakes ‚Äî {"stakes": "high"} still blocks
# test_legacy_jsonb_confidence ‚Äî {"confidence_lt": 0.5} still blocks
# test_legacy_jsonb_compound ‚Äî {"stakes": "high", "confidence_lt": 0.5} still blocks
# test_legacy_jsonb_no_match ‚Äî {"stakes": "high"} doesn't block medium stakes
```

### CEL Expressions
```python
# test_cel_string_condition ‚Äî raw CEL string in condition column
# test_cel_dict_condition ‚Äî {"cel": "expression"} format
# test_cel_context_access ‚Äî decision.context.custom_field works
# test_cel_tags_size ‚Äî size(decision.tags) > 0
# test_cel_has_pattern ‚Äî decision.has_pattern boolean
# test_cel_in_operator ‚Äî decision.stakes in ['high', 'critical']
# test_cel_contains ‚Äî decision.description.contains('trading')
# test_cel_complex_and_or ‚Äî (A && B) || C
# test_cel_negation ‚Äî !decision.context.reviewed
```

### Error Handling
```python
# test_cel_invalid_syntax ‚Äî bad expression ‚Üí no block (fail open)
# test_cel_missing_field ‚Äî decision.nonexistent ‚Üí no block
# test_cel_type_mismatch ‚Äî comparing string to int ‚Üí no block
# test_cel_empty_condition ‚Äî {} ‚Üí no match (safe default)
```

### Performance
```python
# test_cel_program_cached ‚Äî same expression only compiled once
# test_cel_many_guardrails ‚Äî 20 guardrails evaluate in <50ms
```

## Risk Assessment

| Risk | Mitigation |
|------|------------|
| `cel-python` abandoned | Pure Python, we can fork. Simple enough to replace with custom evaluator. |
| CEL DoS (expensive expression) | CEL has no loops. Evaluation is O(expression_size). No infinite execution possible. |
| Silent behavior change from legacy conversion | Comprehensive backward-compat tests. Log all conversions at DEBUG level. |
| Cache memory growth | Cache keyed by expression string. Bounded by number of unique guardrails (~10-50). |
| Fail-open security gap | Log all eval failures. Monitor `guardrail_eval_error` events. Add circuit breaker if failure rate spikes. |

## Dependency Analysis

```
cel-python >= 0.4, < 1.0
‚îú‚îÄ‚îÄ google-re2 (optional, for regex)
‚îú‚îÄ‚îÄ lark-parser (for CEL grammar)
‚îî‚îÄ‚îÄ python-dateutil (for timestamp handling)
```

`lark-parser` is the main transitive dep. Well-maintained, widely used.

## Estimated Effort

- Guardrails rewrite: ~120 lines
- Brain.check() signature: ~10 lines
- Seed data update: ~20 lines
- Tests: ~200 lines
- **Total: ~350 lines, 2-3 hours**

---

## üîç Three-Agent Review (2026-02-23)

### Agent 1: nous-arch (Architecture Review)

#### P1 ‚Äî Blocking Issues

**A1.1: Missing parameters in Brain.check() signature**
- **Issue:** The spec adds `category` and `context` to GuardrailEngine.check(), but Brain.check() in brain.py (line 542) doesn't accept or forward these parameters. CEL expressions can't access them.
- **Impact:** Guardrails can't reference `decision.category` or `decision.context.*` fields, defeating the main value proposition.
- **Fix:** Update Brain.check() signature:
  ```python
  async def check(
      self,
      description: str,
      stakes: str,
      confidence: float,
      category: str | None = None,  # ADD
      tags: list[str] | None = None,
      reasons: list[dict] | None = None,
      pattern: str | None = None,
      quality_score: float | None = None,
      context: dict | None = None,  # ADD
      session: AsyncSession | None = None,
  ) -> GuardrailResult:
  ```
  And forward to guardrails.check() in the implementation.

**A1.2: cel-python dependency not added to pyproject.toml**
- **Issue:** Spec requires `cel-python>=0.4,<1.0` but doesn't show it added to pyproject.toml dependencies.
- **Impact:** Code won't run without manual installation.
- **Fix:** Add to `pyproject.toml`:
  ```toml
  dependencies = [
      ...
      "cel-python>=0.4,<1.0",
  ]
  ```
  Verify no conflicts with existing deps (sqlalchemy, pydantic, etc.).

**A1.3: Type conversion for reasons list incomplete**
- **Issue:** Spec shows `reasons` as `list[dict]` in signature but `_build_activation` only passes `reason_count`. CEL expressions can't access individual reasons or reason types.
- **Impact:** Can't write guardrails like `any(decision.reasons, r, r.type == 'intuition')` to catch decisions without analytical reasons.
- **Fix:** Convert reasons list fully in `_to_cel_value`:
  ```python
  "reasons": celtypes.ListType([
      celtypes.MapType({
          celtypes.StringType("type"): celtypes.StringType(r.get("type", "")),
          celtypes.StringType("text"): celtypes.StringType(r.get("text", "")),
      })
      for r in (reasons or [])
  ]),
  ```

#### P2 ‚Äî Should Fix

**A2.1: Unbounded program cache**
- **Issue:** `_program_cache` is a class-level dict that grows forever. With user-defined guardrails, this could accumulate hundreds of compiled programs.
- **Recommendation:** Use LRU cache with max_size:
  ```python
  from cachetools import LRUCache
  _program_cache: LRUCache = LRUCache(maxsize=100)
  ```
  Or use `functools.lru_cache` as a decorator on `_get_program`.

**A2.2: Lazy compilation adds latency to first evaluation**
- **Issue:** CEL compilation happens on first evaluation, adding ~1-5ms latency to the first decision that triggers each guardrail.
- **Recommendation:** Precompile all active guardrails at startup. Add method:
  ```python
  async def precompile_guardrails(self, session: AsyncSession) -> None:
      """Compile all active guardrail expressions at startup."""
      result = await session.execute(select(Guardrail).where(Guardrail.active.is_(True)))
      for guardrail in result.scalars():
          expression = self._get_expression(guardrail.condition)
          if expression:
              try:
                  self._get_program(expression)  # Warms cache
              except Exception:
                  logger.error("Failed to precompile guardrail %s", guardrail.name)
  ```

**A2.3: No validation when creating guardrails**
- **Issue:** Users can save guardrails with invalid CEL syntax. Error only discovered at first evaluation (fail-open means silent failure).
- **Recommendation:** Add validation endpoint/method:
  ```python
  def validate_expression(self, expression: str) -> tuple[bool, str | None]:
      """Validate CEL expression syntax. Returns (valid, error_message)."""
      try:
          self._get_program(expression)
          return True, None
      except Exception as e:
          return False, str(e)
  ```
  Call this when creating/updating guardrails via API.

#### P3 ‚Äî Nice to Have

**A3.1: Add type declarations to CEL environment**
- **Enhancement:** Typed CEL would catch type mismatches at compile time (e.g., `decision.confidence == "high"` instead of comparing to string).
- **Recommendation:** Define decision schema with celpy annotations (later, when stable).

**A3.2: Metrics on cache performance**
- **Enhancement:** Track cache hit rate, compilation time, evaluation time for performance monitoring.
- **Recommendation:** Add metrics to `_get_program` and `_evaluate`.

---

### Agent 2: nous-minsky (Cognitive/Minsky Review)

#### P1 ‚Äî Blocking Issues

**M1.1: Fail-open contradicts censor safety model**
- **Issue:** Spec says "fail open" for CEL evaluation errors (D1). Minsky's censors are SUPPRESSORS ‚Äî they exist to prevent bad actions. A broken censor shouldn't mean "allow everything through." This is like a smoke detector that says "if I malfunction, assume there's no fire."
- **Impact:** Security vulnerability. A typo in a critical guardrail expression means no protection at all.
- **Fix:** **Fail-closed for block-severity guardrails:**
  ```python
  def _evaluate(self, expression: str, activation: dict) -> bool:
      try:
          program = self._get_program(expression)
          result = program.evaluate(activation)
          return bool(result)
      except Exception:
          logger.error("CEL evaluation failed for: %s", expression, exc_info=True)
          # Fail closed for safety ‚Äî treat as triggered
          return True  # BLOCK when broken, don't fail open
  ```
  Add guardrail metadata field `fail_mode` (default="closed" for block, "open" for warn).

**M1.2: No priority ordering of censors**
- **Issue:** Society of Mind Ch 9 describes censor hierarchies ‚Äî some censors override others. Current implementation evaluates guardrails in arbitrary DB order. A low-priority censor could block when a high-priority one should have allowed it through.
- **Impact:** Can't implement "architectural decisions must be reviewed UNLESS in creative frame" (priority conditional).
- **Fix:** Add `priority` column to guardrails table (integer, default 100). Evaluate in ascending priority order:
  ```python
  result = await session.execute(
      select(Guardrail)
      .where(Guardrail.agent_id == agent_id, Guardrail.active.is_(True))
      .order_by(Guardrail.priority.asc())  # Evaluate high-priority (low number) first
  )
  ```

**M1.3: No learning/evolution of censors**
- **Issue:** Minsky's censors evolve based on experience. Current guardrails are static ‚Äî `activation_count` increments but doesn't affect behavior. Missing the feedback loop that makes censors intelligent.
- **Impact:** Can't auto-disable noisy censors (high false-positive rate) or strengthen effective ones.
- **Fix:** Track outcomes:
  - Add `false_positive_count`, `false_negative_count` to guardrails table
  - Add `confidence_score` field (0.0-1.0, computed from FP/FN ratio)
  - Auto-disable guardrails when confidence < 0.3 (too many false positives)
  - Add API for marking guardrail activation as FP: `brain.review_guardrail(guardrail_id, was_correct=False)`

#### P2 ‚Äî Should Fix

**M2.1: No cross-connected censor network**
- **Issue:** Minsky Ch 18 describes cross-connected censors that reference each other's outputs. CEL could support this: "Block if high-stakes AND no-review-guardrail didn't trigger."
- **Recommendation:** Add namespace for other guardrails in CEL:
  ```python
  activation["triggered_guardrails"] = celtypes.ListType([
      celtypes.StringType(g) for g in previously_triggered
  ])
  ```
  Then expressions can use: `decision.stakes == 'critical' && !('reviewed' in triggered_guardrails)`

**M2.2: Frame-based censor activation missing**
- **Issue:** Spec mentions Cognitive Layer frames but doesn't integrate them. Creative frame should have fewer active censors (Ch 9 ‚Äî "exploratory thinking needs fewer suppressors").
- **Recommendation:** Add `frame_id` column to guardrails (nullable). When checking guardrails, filter by current frame:
  ```python
  .where(
      (Guardrail.frame_id == current_frame_id) | (Guardrail.frame_id.is_(None))
  )
  ```
  Guardrails with `frame_id=None` are always active. Frame-specific ones activate only in that frame.

#### P3 ‚Äî Nice to Have

**M3.1: Temporal censors**
- **Enhancement:** Some suppressions should be time-based: "Don't make critical decisions after 10 PM" or "high-stakes decisions on weekends require extra review."
- **Recommendation:** Add timestamp to CEL context:
  ```python
  "timestamp": celtypes.TimestampType(datetime.now(UTC)),
  "hour": celtypes.IntType(datetime.now(UTC).hour),
  "is_weekend": celtypes.BoolType(datetime.now(UTC).weekday() >= 5),
  ```
  Then: `decision.stakes == 'critical' && hour >= 22` (no critical decisions after 10 PM).

**M3.2: Reason diversity guardrails**
- **Enhancement:** Minsky advocates for multiple perspectives (Ch 18 ‚Äî parallel bundles). Could enforce: "Block if all reasons are the same type."
- **Recommendation:** Add helper to CEL context:
  ```python
  "reason_types": celtypes.ListType([celtypes.StringType(r["type"]) for r in reasons]),
  "reason_type_diversity": celtypes.IntType(len(set(r["type"] for r in reasons))),
  ```
  Then: `decision.reason_type_diversity < 2` (need at least 2 different reason types).

---

### Agent 3: nous-devil (Devil's Advocate / Security)

#### P1 ‚Äî Blocking Issues

**D1.1: cel-python sandboxing not verified**
- **Issue:** Spec claims "no I/O, no side effects, sandboxed" but doesn't cite security audit or verification. What if cel-python has an escape hatch via reflection, `__import__`, or other Python internals?
- **Impact:** Malicious guardrail could exfiltrate data or modify state.
- **Fix:** 
  1. Review cel-python source for sandbox escapes (especially `celpy.evaluation` module)
  2. OR: Wrap evaluation in subprocess with resource limits (extreme but safe):
     ```python
     import subprocess
     result = subprocess.run(
         ["python", "-c", f"import celpy; ..."],
         timeout=1,
         capture_output=True,
     )
     ```
  3. Document security assumptions and cel-python version used.

**D1.2: DoS via complex expressions**
- **Issue:** Even without loops, deeply nested expressions can cause exponential evaluation time. Example:
  ```cel
  (a && b) || (c && d) || (e && f) || (g && h) || ...
  ```
  With 100 clauses, evaluation could take seconds (expression tree explosion).
- **Impact:** A single malicious guardrail could hang decision-making.
- **Fix:** Add timeout to evaluation:
  ```python
  import signal
  
  def _evaluate_with_timeout(self, expression: str, activation: dict, timeout_sec: float = 0.1) -> bool:
      def timeout_handler(signum, frame):
          raise TimeoutError("CEL evaluation timeout")
      
      signal.signal(signal.SIGALRM, timeout_handler)
      signal.alarm(int(timeout_sec))
      try:
          program = self._get_program(expression)
          result = program.evaluate(activation)
          return bool(result)
      except TimeoutError:
          logger.error("CEL evaluation timeout for: %s", expression)
          return True  # Fail closed ‚Äî treat timeout as trigger
      finally:
          signal.alarm(0)
  ```
  Or use `stopit` library for cross-platform timeout.

**D1.3: Legacy conversion silently changes behavior**
- **Issue:** `_jsonb_to_cel` auto-conversion might not preserve exact semantics. Users depend on current (possibly buggy) behavior (Hyrum's Law). Example: old code might have edge case where `None` confidence matches `confidence_lt`, but CEL might handle it differently.
- **Impact:** Silent breaking change that's hard to debug.
- **Fix:** Add opt-in flag for legacy conversion:
  1. Add `legacy_mode` boolean to guardrails table (default=True for existing rows)
  2. Only auto-convert when `legacy_mode=True`
  3. Add migration script that converts all JSONB to CEL and sets `legacy_mode=False`
  4. Log all legacy conversions at WARNING level with exact CEL output:
     ```python
     cel_expr = self._jsonb_to_cel(condition)
     logger.warning("Legacy JSONB converted to CEL: %s ‚Üí %s", condition, cel_expr)
     ```

#### P2 ‚Äî Should Fix

**D2.1: Cache invalidation on cel-python version change**
- **Issue:** Compiled CEL programs are cached forever. If cel-python changes evaluation semantics in a minor version (e.g., 0.4.5 ‚Üí 0.4.6), cached programs might behave differently than freshly compiled ones.
- **Recommendation:** Include cel-python version in cache key:
  ```python
  import celpy
  
  cache_key = f"{expression}::{celpy.__version__}"
  if cache_key not in self._program_cache:
      ...
  ```
  Or clear cache on startup (simpler, slight perf hit).

**D2.2: Malformed DB data crashes evaluation**
- **Issue:** `_get_expression` assumes `condition` is valid JSON (dict or string). If DB has corrupted data or wrong type, code crashes.
- **Recommendation:** Wrap in try/except:
  ```python
  def _get_expression(self, condition: dict | str) -> str | None:
      try:
          if isinstance(condition, str):
              return condition
          if isinstance(condition, dict):
              if "cel" in condition:
                  return condition["cel"]
              return self._jsonb_to_cel(condition)
      except Exception:
          logger.error("Malformed guardrail condition: %s", condition, exc_info=True)
      return None  # Invalid condition = no match (fail open for malformed data)
  ```

**D2.3: Context dict is unvalidated user input**
- **Issue:** Callers can pass arbitrary `context` dict. If it contains non-JSON-serializable types (functions, objects, etc.), `_to_cel_value` might crash or behave unexpectedly.
- **Recommendation:** Sanitize context before conversion:
  ```python
  def _sanitize_context(ctx: dict | None) -> dict:
      """Remove non-JSON-serializable values from context."""
      if not ctx:
          return {}
      clean = {}
      for k, v in ctx.items():
          if isinstance(v, (str, int, float, bool, list, dict, type(None))):
              clean[k] = v
          else:
              logger.warning("Dropping non-JSON context key: %s (type=%s)", k, type(v))
      return clean
  ```

#### P3 ‚Äî Nice to Have

**D3.1: Rate limiting on guardrail activations**
- **Issue:** A runaway guardrail that triggers constantly could spam the events table with `guardrail_blocked` events.
- **Recommendation:** Add rate limit per guardrail (max N activations per hour). Track in Redis or in-memory counter.

**D3.2: Document CEL exposure limitations**
- **Issue:** Some decision fields (like `embedding`) are too large for CEL. Not documented.
- **Recommendation:** Add section to spec:
  > **Fields NOT exposed to CEL:**
  > - `embedding` (too large, use semantic search instead)
  > - `search_tsv` (internal full-text search field)
  > - ORM relationships (tags/reasons are converted to lists)

---

## Synthesized Issues & Decisions

### All P1 Issues (Must Fix Before Merge)

| ID | Agent | Issue | Fix |
|----|-------|-------|-----|
| **A1.1** | arch | Missing category/context in Brain.check() | Add params to signature, forward to guardrails.check() |
| **A1.2** | arch | cel-python not in pyproject.toml | Add dependency `cel-python>=0.4,<1.0` |
| **A1.3** | arch | Reasons list not fully converted to CEL | Convert full list with type+text fields |
| **M1.1** | minsky | Fail-open unsafe for censors | Change to fail-closed for block-severity (see D1) |
| **M1.2** | minsky | No priority ordering | Add priority column, evaluate in order (see D2) |
| **M1.3** | minsky | No censor learning | Add FP/FN tracking, confidence score, auto-disable (see D3) |
| **D1.1** | devil | cel-python security not verified | Review source or subprocess isolation (see D4) |
| **D1.2** | devil | DoS via complex expressions | Add timeout to evaluation (see D5) |
| **D1.3** | devil | Legacy conversion silent behavior change | Add legacy_mode flag, log conversions (see D6) |

### All P2 Issues (Should Fix)

| ID | Agent | Issue | Recommendation |
|----|-------|-------|----------------|
| **A2.1** | arch | Unbounded program cache | Use LRUCache(maxsize=100) |
| **A2.2** | arch | Lazy compilation latency | Precompile at startup (see D7) |
| **A2.3** | arch | No validation on guardrail creation | Add validate_expression() method (see D8) |
| **M2.1** | minsky | No cross-connected censors | Add triggered_guardrails to CEL context (see D9) |
| **M2.2** | minsky | Frame-based activation missing | Add frame_id column (see D10) |
| **D2.1** | devil | Cache invalidation on version change | Include cel-python version in cache key (see D11) |
| **D2.2** | devil | Malformed DB data crashes | Wrap _get_expression in try/except (see D12) |
| **D2.3** | devil | Context dict unvalidated | Sanitize context before CEL conversion (see D13) |

### All P3 Issues (Nice to Have)

- **A3.1** (arch): Add type declarations to CEL (later)
- **A3.2** (arch): Add cache performance metrics
- **M3.1** (minsky): Temporal censors (timestamp, hour, is_weekend in CEL)
- **M3.2** (minsky): Reason diversity guardrails
- **D3.1** (devil): Rate limiting on activations
- **D3.2** (devil): Document CEL exposure limitations

---

## Design Decisions

### D1: Fail-Closed for Block-Severity Guardrails
**Status:** REQUIRED (M1.1)

Minsky's censor model requires suppressors to be fail-safe. A broken censor shouldn't mean "allow everything."

**Implementation:**
```python
class Guardrail:
    fail_mode: str = "closed"  # "closed" | "open"

def _evaluate(self, expression: str, activation: dict, fail_mode: str = "closed") -> bool:
    try:
        program = self._get_program(expression)
        result = program.evaluate(activation)
        return bool(result)
    except Exception:
        logger.error("CEL evaluation failed: %s", expression, exc_info=True)
        if fail_mode == "closed":
            return True  # Treat error as trigger (BLOCK)
        else:
            return False  # Fail open (no block)
```

**Migration:** Add `fail_mode` column to guardrails table. Default:
- `block` and `absolute` severity ‚Üí `fail_mode="closed"`
- `warn` severity ‚Üí `fail_mode="open"`

---

### D2: Priority-Ordered Censor Evaluation
**Status:** REQUIRED (M1.2)

Guardrails must evaluate in priority order to implement censor hierarchies.

**Schema change:**
```sql
ALTER TABLE brain.guardrails ADD COLUMN priority INTEGER DEFAULT 100;
CREATE INDEX idx_guardrails_priority ON brain.guardrails(agent_id, active, priority);
```

**Query:**
```python
result = await session.execute(
    select(Guardrail)
    .where(Guardrail.agent_id == agent_id, Guardrail.active.is_(True))
    .order_by(Guardrail.priority.asc(), Guardrail.created_at.asc())
)
```

Lower priority number = higher priority (Unix convention). Priority 1-10 for critical censors, 100 for normal, 1000 for low-priority warnings.

---

### D3: Censor Learning (False Positive/Negative Tracking)
**Status:** REQUIRED (M1.3)

Track guardrail accuracy to auto-disable noisy censors.

**Schema changes:**
```sql
ALTER TABLE brain.guardrails ADD COLUMN false_positive_count INTEGER DEFAULT 0;
ALTER TABLE brain.guardrails ADD COLUMN false_negative_count INTEGER DEFAULT 0;
ALTER TABLE brain.guardrails ADD COLUMN confidence_score FLOAT DEFAULT 1.0;
ALTER TABLE brain.guardrails ADD COLUMN auto_disabled BOOLEAN DEFAULT FALSE;
```

**API for review:**
```python
async def review_guardrail(
    self,
    guardrail_id: UUID,
    activation_id: UUID,  # Event ID from guardrail_blocked event
    was_correct: bool,
    session: AsyncSession,
) -> None:
    """Mark a guardrail activation as correct (TP) or incorrect (FP)."""
    if was_correct:
        # True positive ‚Äî no change
        pass
    else:
        # False positive ‚Äî increment FP counter
        await session.execute(
            update(Guardrail)
            .where(Guardrail.id == guardrail_id)
            .values(false_positive_count=Guardrail.false_positive_count + 1)
        )
    
    # Recompute confidence: TP / (TP + FP)
    # TP = activation_count - false_positive_count
    # confidence = TP / activation_count if activation_count > 0 else 1.0
    
    # Auto-disable if confidence < 0.3 and activation_count > 20
```

**Background job:** Weekly scan to auto-disable low-confidence guardrails.

---

### D4: cel-python Security Verification
**Status:** REQUIRED (D1.1)

Two-phase approach:

**Phase 1 (before merge):**
- Manual source review of cel-python v0.4.x evaluation module
- Check for `eval()`, `exec()`, `__import__`, `getattr()` usage
- Verify no reflection-based escapes

**Phase 2 (post-merge):**
- Consider subprocess isolation for untrusted guardrails (user-defined)
- Add integration test that attempts sandbox escape:
  ```python
  # Attempt escape via __import__
  expression = "__import__('os').system('echo hacked')"
  # Should raise exception, not execute
  ```

**Documentation:** Add security assumptions to spec:
> **Security Model:** CEL expressions are sandboxed via cel-python's interpreter. No file I/O, network access, or Python stdlib imports are possible. However, cel-python has not undergone formal security audit. Treat guardrail expressions as semi-trusted code ‚Äî admin-created only, not user-input.

---

### D5: Timeout for CEL Evaluation
**Status:** REQUIRED (D1.2)

Prevent DoS from complex expressions.

**Implementation (Unix):**
```python
import signal

class GuardrailEngine:
    CEL_TIMEOUT_SEC = 0.1  # 100ms max per expression
    
    def _evaluate(self, expression: str, activation: dict, fail_mode: str = "closed") -> bool:
        def timeout_handler(signum, frame):
            raise TimeoutError("CEL evaluation timeout")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(1)  # 1 second (signal.alarm only takes int)
        
        try:
            program = self._get_program(expression)
            result = program.evaluate(activation)
            return bool(result)
        except TimeoutError:
            logger.error("CEL evaluation timeout for: %s", expression)
            return True if fail_mode == "closed" else False
        except Exception:
            logger.error("CEL evaluation failed: %s", expression, exc_info=True)
            return True if fail_mode == "closed" else False
        finally:
            signal.alarm(0)
```

**Cross-platform alternative:** Use `stopit` library:
```python
import stopit

with stopit.ThreadingTimeout(0.1) as ctx:
    result = program.evaluate(activation)

if not ctx:
    # Timeout occurred
    logger.error("CEL timeout")
    return True if fail_mode == "closed" else False
```

---

### D6: Legacy Conversion Opt-In
**Status:** REQUIRED (D1.3)

Prevent silent behavior changes from JSONB‚ÜíCEL auto-conversion.

**Schema:**
```sql
ALTER TABLE brain.guardrails ADD COLUMN legacy_mode BOOLEAN DEFAULT TRUE;
```

**Logic:**
```python
def _get_expression(self, condition: dict | str, legacy_mode: bool = True) -> str | None:
    if isinstance(condition, str):
        return condition
    if isinstance(condition, dict):
        if "cel" in condition:
            return condition["cel"]
        if legacy_mode:
            cel_expr = self._jsonb_to_cel(condition)
            logger.warning(
                "Legacy JSONB‚ÜíCEL conversion: %s ‚Üí %s",
                condition, cel_expr
            )
            return cel_expr
    return None
```

**Migration script:**
```sql
-- Convert all existing guardrails to native CEL
UPDATE brain.guardrails
SET condition = jsonb_build_object('cel', <converted_expression>),
    legacy_mode = FALSE
WHERE legacy_mode = TRUE;
```

Provide SQL helper function or Python script to do batch conversion with validation.

---

### D7: Precompile Guardrails at Startup
**Status:** RECOMMENDED (A2.2)

Eliminate first-evaluation latency.

**Implementation:**
```python
async def precompile_guardrails(self, session: AsyncSession) -> None:
    """Compile all active guardrail expressions. Call at startup."""
    result = await session.execute(
        select(Guardrail).where(Guardrail.active.is_(True))
    )
    compiled = 0
    failed = 0
    
    for guardrail in result.scalars():
        expression = self._get_expression(guardrail.condition)
        if expression:
            try:
                self._get_program(expression)
                compiled += 1
            except Exception:
                logger.error(
                    "Failed to precompile guardrail %s: %s",
                    guardrail.name, expression
                )
                failed += 1
    
    logger.info(
        "Precompiled %d guardrails (%d failed)",
        compiled, failed
    )
```

**Call in Brain.__init__ or app startup:**
```python
async with brain.db.session() as session:
    await brain.guardrails.precompile_guardrails(session)
```

---

### D8: Validate CEL Expressions on Creation
**Status:** RECOMMENDED (A2.3)

Fail fast on invalid syntax.

**API:**
```python
def validate_expression(self, expression: str) -> tuple[bool, str | None]:
    """Validate CEL expression. Returns (is_valid, error_message)."""
    try:
        self._get_program(expression)
        return True, None
    except Exception as e:
        return False, str(e)
```

**Usage in guardrail creation endpoint:**
```python
@router.post("/guardrails")
async def create_guardrail(
    data: GuardrailCreate,
    brain: Brain = Depends(get_brain),
):
    # Extract CEL expression
    if isinstance(data.condition, str):
        expr = data.condition
    elif "cel" in data.condition:
        expr = data.condition["cel"]
    else:
        expr = None  # Legacy JSONB, no validation needed
    
    if expr:
        valid, error = brain.guardrails.validate_expression(expr)
        if not valid:
            raise HTTPException(400, f"Invalid CEL expression: {error}")
    
    # Create guardrail...
```

---

### D9: Cross-Connected Censors (Triggered Guardrails Context)
**Status:** RECOMMENDED (M2.1)

Allow guardrails to reference other guardrails' outputs.

**Implementation:**
```python
async def check(self, ...):
    ...
    triggered_names: list[str] = []
    
    for guardrail in guardrails:
        # Build activation with list of previously triggered guardrails
        activation = _build_activation(..., triggered=triggered_names)
        
        if self._evaluate(expression, activation):
            if guardrail.severity in ("block", "absolute"):
                blocked_by.append(guardrail.name)
                triggered_names.append(guardrail.name)
            ...
```

**In _build_activation:**
```python
decision["triggered_guardrails"] = celtypes.ListType([
    celtypes.StringType(name) for name in triggered
])
```

**Example expression:**
```cel
decision.stakes == 'critical' && !('manual-review' in decision.triggered_guardrails)
```
(Block critical stakes UNLESS manual-review guardrail already triggered and allowed it through.)

---

### D10: Frame-Based Censor Activation
**Status:** RECOMMENDED (M2.2)

Different frames have different active guardrails.

**Schema:**
```sql
ALTER TABLE brain.guardrails ADD COLUMN frame_id TEXT REFERENCES nous_system.frames(id);
CREATE INDEX idx_guardrails_frame ON brain.guardrails(frame_id);
```

**Query:**
```python
async def check(self, ..., current_frame_id: str | None = None):
    query = select(Guardrail).where(
        Guardrail.agent_id == agent_id,
        Guardrail.active.is_(True),
    )
    
    if current_frame_id:
        # Include frame-specific + frame-agnostic guardrails
        query = query.where(
            (Guardrail.frame_id == current_frame_id) | (Guardrail.frame_id.is_(None))
        )
    else:
        # Only frame-agnostic guardrails
        query = query.where(Guardrail.frame_id.is_(None))
    
    result = await session.execute(query.order_by(Guardrail.priority.asc()))
    ...
```

**Usage:** Creative frame has `frame_id=NULL` (all global guardrails) or specific low-priority ones. Task frame has strict guardrails active.

---

### D11: Version-Aware Program Cache
**Status:** RECOMMENDED (D2.1)

Prevent stale cached programs after cel-python upgrade.

**Implementation:**
```python
import celpy

class GuardrailEngine:
    _CEL_VERSION = celpy.__version__
    _program_cache: dict[str, Runner] = {}
    
    def _get_program(self, expression: str) -> Runner:
        cache_key = f"{self._CEL_VERSION}::{expression}"
        if cache_key not in self._program_cache:
            ...
```

**Simpler alternative:** Clear cache on app startup (acceptable perf hit for safety).

---

### D12: Error Handling for Malformed Conditions
**Status:** RECOMMENDED (D2.2)

Prevent crashes from corrupted DB data.

**Implementation:**
```python
def _get_expression(self, condition: Any) -> str | None:
    try:
        if isinstance(condition, str):
            return condition
        if isinstance(condition, dict):
            if "cel" in condition:
                cel_val = condition["cel"]
                if isinstance(cel_val, str):
                    return cel_val
                else:
                    logger.error("CEL value is not string: %s", cel_val)
                    return None
            return self._jsonb_to_cel(condition)
        logger.error("Invalid condition type: %s", type(condition))
        return None
    except Exception:
        logger.error("Failed to parse condition: %s", condition, exc_info=True)
        return None
```

---

### D13: Context Dict Sanitization
**Status:** RECOMMENDED (D2.3)

Only allow JSON-serializable types in context.

**Implementation:**
```python
def _sanitize_context(ctx: dict | None) -> dict:
    """Remove non-JSON-serializable values."""
    if not ctx:
        return {}
    
    clean = {}
    for k, v in ctx.items():
        if isinstance(v, (str, int, float, bool, type(None))):
            clean[k] = v
        elif isinstance(v, list):
            # Recursively sanitize lists
            clean[k] = [item for item in v if isinstance(item, (str, int, float, bool, type(None)))]
        elif isinstance(v, dict):
            # Recursively sanitize dicts
            clean[k] = _sanitize_context(v)
        else:
            logger.warning(
                "Dropping non-JSON context key '%s' (type=%s)",
                k, type(v).__name__
            )
    
    return clean
```

**Call in _build_activation:**
```python
context = _sanitize_context(context)
```

---

## Summary & Recommendation

### Verdict: **APPROVED WITH REQUIRED CHANGES**

The CEL migration is architecturally sound and aligns well with Minsky's censor model. However, **9 P1 issues must be fixed before merge:**

1. ‚úÖ **A1.1-A1.3** ‚Äî Architecture fixes (Brain signature, dependency, reasons conversion)
2. ‚úÖ **M1.1-M1.3** ‚Äî Censor model fixes (fail-closed, priority, learning)
3. ‚úÖ **D1.1-D1.3** ‚Äî Security fixes (sandbox verification, timeout, legacy opt-in)

### Implementation Order

**Phase 1: Core CEL (required for merge)**
- [ ] D1: Fail-closed for block-severity
- [ ] D2: Priority-ordered evaluation
- [ ] D4: cel-python security review
- [ ] D5: Timeout for evaluation
- [ ] D6: Legacy conversion opt-in
- [ ] A1.1: Update Brain.check() signature
- [ ] A1.2: Add cel-python dependency
- [ ] A1.3: Full reasons list conversion

**Phase 2: Censor Learning (required for production)**
- [ ] D3: FP/FN tracking + auto-disable
- [ ] D7: Precompile at startup
- [ ] D8: Validation on creation

**Phase 3: Advanced Censors (post-MVP)**
- [ ] D9: Cross-connected censors
- [ ] D10: Frame-based activation
- [ ] D11: Version-aware cache
- [ ] D12: Malformed data handling
- [ ] D13: Context sanitization

### Revised Effort Estimate

- **Original estimate:** 2-3 hours (350 lines)
- **With P1 fixes:** 6-8 hours (600 lines)
- **With P2 fixes:** 10-12 hours (850 lines)
- **Full implementation (all decisions):** 16-20 hours (1200 lines)

**Recommendation:** Implement Phase 1 + Phase 2 before shipping to production. Phase 3 can follow iteratively.

---

üîç **Nous Review Team**  
*nous-arch ‚Ä¢ nous-minsky ‚Ä¢ nous-devil*  
2026-02-23
