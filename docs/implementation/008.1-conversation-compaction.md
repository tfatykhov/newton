# Spec 008.1: Conversation Compaction

**Status:** Draft
**Depends on:** 008 (Tiered Context Model)
**Issue:** #68

## Problem

Spec 008 manages the system prompt well (Tier 1 always-on, Tier 3 semantic search with budgets), but **conversation history has zero token management**. Currently:

- `MAX_HISTORY_MESSAGES = 20` — hard cutoff by message count, not tokens
- Full messages sent including long bash outputs and tool results
- No token counting before API calls
- No summarization of old messages
- No truncation of long tool outputs
- Conversation is in-memory only — lost on container restart

A 20-message conversation with bash tool outputs can easily exceed 50K+ input tokens. With Sonnet 4.6 (200K context), this works but is expensive and wasteful.

## Goals

1. Prevent context window overflow with graceful degradation
2. Preserve important context (decisions, facts, preferences) through compaction
3. Keep recent conversation intact for natural dialogue flow
4. Integrate with existing Heart/Brain systems for durable storage
5. Minimal latency impact on user-facing turns

## Non-Goals

- Cross-session conversation persistence (separate feature)
- Multi-user conversation management
- Real-time token counting during streaming

## Architecture

### Token Budget Model

| Component | Budget | Notes |
|-----------|--------|-------|
| System prompt | ~5K | Identity + Tier 1 + Tier 3 context |
| Conversation summary | ~2K | Rolling compacted summary of older messages |
| Recent messages | ~30K | Last N messages verbatim (recency window) |
| Output headroom | ~16K | `max_tokens` setting |
| Safety margin | ~20K | Buffer for tool results within turn |
| **Total target** | **~73K** | Well within 200K limit |

### Compaction Trigger

```
INPUT_TOKEN_BUDGET = 80_000  # configurable via NOUS_INPUT_TOKEN_BUDGET

Before each API call:
  estimated_tokens = estimate(system_prompt) + estimate(messages)
  if estimated_tokens > INPUT_TOKEN_BUDGET:
      compact(conversation)
```

### Compaction Algorithm

```
compact(conversation):
  1. Split messages into [old] + [recent]
     - recent = last RECENCY_WINDOW messages (default: 6)
     - old = everything before that

  2. Extract durable knowledge from old messages:
     - Scan for decisions → already in Brain (from post_turn)
     - Scan for facts → already in Heart (from fact_extractor handler)
     - Scan for user preferences → flag for review
     - This step is a safety net — most extraction already happens via handlers

  3. Summarize old messages using cheap model (Haiku/flash):
     Prompt: "Summarize this conversation segment. Preserve:
       - Decisions made and their rationale
       - Facts learned or preferences expressed
       - Open questions or pending tasks
       - Key context needed for continuation
       Discard: tool outputs, greetings, routine acks."

  4. Build rolling summary:
     - If conversation.summary exists: include as context for summarizer
     - New summary = merge(old_summary, new_segment_summary)
     - Cap at SUMMARY_TOKEN_BUDGET (default: 2000 tokens)

  5. Replace conversation history:
     conversation.summary = new_summary
     conversation.messages = recent_messages_only
```

### Message Format After Compaction

```python
# Before compaction - sent to API:
messages = [
    {"role": "user", "content": "msg 1"},      # old
    {"role": "assistant", "content": "msg 2"},   # old
    ...
    {"role": "user", "content": "msg 19"},       # recent
    {"role": "assistant", "content": "msg 20"},  # recent
]

# After compaction - sent to API:
messages = [
    {"role": "user", "content": "[Conversation Summary]\n{summary}"},  # synthetic
    {"role": "assistant", "content": "I understand the context. Continuing..."},  # synthetic
    {"role": "user", "content": "msg 15"},       # recent (verbatim)
    {"role": "assistant", "content": "msg 16"},  # recent (verbatim)
    {"role": "user", "content": "msg 17"},       # recent (verbatim)
    {"role": "assistant", "content": "msg 18"},  # recent (verbatim)
    {"role": "user", "content": "msg 19"},       # recent (verbatim)
    {"role": "assistant", "content": "msg 20"},  # recent (verbatim)
]
```

### Token Estimation

Use character-based estimation (no external dependency):

```python
def estimate_tokens(text: str) -> int:
    """Rough estimate: 1 token ≈ 4 chars for English text."""
    return len(text) // 4
```

This is intentionally conservative. Exact counting (tiktoken) adds a dependency and latency for marginal accuracy gain. The budget has a 20K safety margin to absorb estimation error.

## Implementation Plan

### Phase 1: Safety Net (Low Risk)

**Files changed:** `runner.py`

1. **Token estimation function** — `_estimate_input_tokens(system_prompt, messages) -> int`
2. **Tool output truncation** — cap tool results at `TOOL_OUTPUT_MAX_CHARS = 4000` before appending to messages
3. **Warning log** — log when estimated input tokens exceed 60K (early warning)
4. **Metric tracking** — track `input_tokens_estimated` per turn in TurnContext

### Phase 2: Basic Compaction

**Files changed:** `runner.py`, `config.py`, `schemas.py`

1. **Conversation.summary field** — add `summary: str | None` to Conversation dataclass
2. **`_compact_conversation()` method** on AgentRunner:
   - Split old/recent by RECENCY_WINDOW
   - Call summarization model (Haiku via separate httpx call)
   - Replace messages with [summary_pair] + [recent]
3. **Config additions:**
   - `NOUS_INPUT_TOKEN_BUDGET` (default: 80000)
   - `NOUS_RECENCY_WINDOW` (default: 6)
   - `NOUS_SUMMARY_MODEL` (default: "claude-haiku-3-5-20241022")
4. **Integration point** — call `_maybe_compact()` in `run_turn_streaming()` before first `_stream_api()` call

### Phase 3: Durable Integration

**Files changed:** `runner.py`, `heart.py`, `episodes.py`

1. **Episode boundary** — compaction triggers episode close for the compacted segment
2. **Conversation persistence** — store conversation state in DB (new table `heart.conversations`)
3. **Restart recovery** — load conversation from DB on session reconnect
4. **Working memory update** — extract `current_task` from summary, update working memory

### Phase 4: Adaptive Budgets

1. **Dynamic recency window** — expand/contract based on message sizes
2. **Priority-based retention** — keep messages with decisions/facts longer than routine chat
3. **Cascade summarization** — summary of summaries for very long sessions
4. **Integration with Tier 3** — compacted decisions/facts boost Tier 3 recall scores

## Configuration

```python
# config.py additions
input_token_budget: int = 80000          # Trigger compaction above this
recency_window: int = 6                   # Messages to keep verbatim
summary_token_budget: int = 2000          # Max tokens for rolling summary
tool_output_max_chars: int = 4000         # Truncate tool outputs
summary_model: str = "claude-haiku-3-5-20241022"  # Cheap model for summarization
```

All configurable via `NOUS_` prefixed env vars.

## API Changes

None. Compaction is internal to the runner. No REST API or MCP changes needed.

## Database Changes

### Phase 1-2: None

### Phase 3: New table

```sql
CREATE TABLE heart.conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_id TEXT NOT NULL REFERENCES nous_system.agents(id),
    session_id TEXT NOT NULL,
    summary TEXT,
    messages JSONB NOT NULL DEFAULT '[]',
    turn_count INT NOT NULL DEFAULT 0,
    total_tokens_estimated BIGINT NOT NULL DEFAULT 0,
    created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    UNIQUE(agent_id, session_id)
);
```

## Testing Strategy

1. **Unit tests:**
   - `test_estimate_tokens` — verify estimation accuracy within 20%
   - `test_tool_output_truncation` — verify long outputs are capped
   - `test_compact_conversation` — verify message split, summary injection, recent retention
   - `test_compact_preserves_alternation` — user/assistant pairs stay valid after compaction
   - `test_no_compact_under_budget` — verify compaction doesn't trigger unnecessarily

2. **Integration tests:**
   - Multi-turn conversation that triggers compaction
   - Verify post-compaction conversation still works (valid message format)
   - Verify summary captures key context from evicted messages

3. **Manual testing:**
   - Long Telegram conversation → check logs for compaction trigger
   - Verify Nous remembers context from before compaction
   - Verify tool outputs are truncated but still useful

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Summary loses critical context | Medium | High | Recency window keeps last 6 messages; Heart/Brain already capture decisions/facts |
| Summarization latency | Low | Medium | Haiku is fast (~500ms); only triggers when over budget |
| Token estimation inaccuracy | Medium | Low | 20K safety margin absorbs errors; conservative estimate |
| Summary model API failure | Low | Medium | Fallback: truncate old messages without summary |
| Invalid message format after compaction | Low | High | Synthetic summary pair maintains user/assistant alternation |

## Success Metrics

- No context window overflow errors in production
- Compaction latency < 1 second (P95)
- Post-compaction conversation quality: Nous maintains context from compacted messages
- Token cost reduction: ~30-50% for long conversations vs current approach
