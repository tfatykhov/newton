# Spec 008.1: Conversation Compaction (v5)

**Status:** Approved v5 (3/3 reviewer approval)
**Depends on:** 008 (Tiered Context Model)
**Issue:** #68

## Problem

Conversation history has zero token management:

- `MAX_HISTORY_MESSAGES = 20` — hard cutoff by count, not tokens
- Tool outputs within a turn are local variables — never stored in `conversation.messages`
- No token counting, no truncation of tool outputs, no persistence

### What Causes Token Bloat

**In-turn:** Tool loops accumulate tool_use + tool_result in local `messages`. A 10-iteration bash loop → 30K+ tokens. Compaction can't touch these — only pruning can.

**Across turns:** `conversation.messages` stores plain text user/assistant pairs only. With 20-message cap, history is ~6K tokens. The cap means history rarely exceeds 10K today, but removing it (necessary for longer conversations) makes compaction essential.

---

## Architecture

### Two-Layer Approach

```
Layer 1: Tool Output Pruning (per-request, cheap, no LLM)
  ├── Runs before every API call during tool loops
  ├── Soft-trim old tool results (head + tail + marker)
  ├── Hard-clear very old tool results (→ placeholder)
  └── Only touches tool_result content, never user/assistant text

Layer 2: History Compaction (rare, LLM-powered)
  ├── Triggered when estimated input tokens exceed budget
  ├── Pre-compaction event via CognitiveLayer
  ├── Structured checkpoint summary
  ├── Iterative update with size cap
  └── Persisted in Conversation state
```

### Module Structure

```
nous/api/
  ├── runner.py          — orchestration, API calls, tool loops
  ├── compaction.py      — NEW: ConversationCompactor, TokenEstimator
  └── ...
```

### ConversationCompactor Class

```python
class ConversationCompactor:
    """Manages tool result pruning (Layer 1) and history compaction (Layer 2).
    
    Owns a TokenEstimator instance shared with AgentRunner for calibration.
    """
    
    def __init__(self, settings: Settings) -> None:
        self._settings = settings
        self.estimator = TokenEstimator()  # Public: runner accesses for calibration
    
    # Layer 1
    @staticmethod
    def is_tool_result_message(msg: dict[str, Any]) -> bool: ...
    def prune_tool_results(self, messages: list[dict[str, Any]]) -> None: ...
    
    # Layer 2
    def should_compact(self, system_tokens: int, history_tokens: int) -> bool: ...
    def find_cut_point(self, messages: list[dict[str, Any]], keep_recent_tokens: int) -> int: ...
    async def compact(self, conversation: Conversation, messages: list[dict[str, Any]],
                      call_api: ApiCaller, cut_point: int) -> None: ...
    
    # Utilities (on compactor, not runner)
    @staticmethod
    def extract_text(content: list[dict[str, Any]]) -> str: ...
    def _serialize_for_summary(self, messages: list[dict[str, Any]]) -> str: ...
    def _validate_summary(self, summary: str) -> bool: ...
```

### AgentRunner Init Changes

```python
def __init__(self, cognitive, brain, heart, settings):
    ...
    # Compaction
    self._compactor: ConversationCompactor | None = None
    if settings.tool_pruning_enabled or settings.compaction_enabled:
        self._compactor = ConversationCompactor(settings=settings)
    self._compaction_locks: dict[str, asyncio.Lock] = {}
```

**Note:** `Conversation`, `Message`, and `ApiResponse` dataclasses should be extracted from `runner.py` to `nous/api/models.py` to avoid circular imports between `runner.py` and `compaction.py`.

### Protocol Types for Callable Injection

```python
# In compaction.py
from typing import Protocol

class ApiCaller(Protocol):
    async def __call__(
        self, system_prompt: str, messages: list[dict[str, Any]],
        tools: list[dict[str, Any]] | None = None,
        skip_thinking: bool = False,
        model_override: str | None = None,
    ) -> ApiResponse: ...
```

### Token Budget (Sonnet 4.6, 200K context)

| Component | Budget | Notes |
|-----------|--------|-------|
| System prompt | ~5K | Identity + Tier 1 + Tier 3 |
| Conversation summary | ~4K | Structured checkpoint |
| Recent messages | ~20K | Token-based window |
| In-turn tool results | ~30K | Layer 1 pruning |
| Output headroom | ~16K | `max_tokens` |
| Safety margin | ~20K | Estimation error buffer |

---

## Layer 1: Tool Output Pruning

### Identifying Tool Result Messages

Matches how tool results are constructed in `stream_chat()` (line ~775) and `_tool_loop()` (line ~881):

```python
@staticmethod
def is_tool_result_message(msg: dict[str, Any]) -> bool:
    content = msg.get("content")
    return (
        msg.get("role") == "user"
        and isinstance(content, list)
        and len(content) > 0
        and isinstance(content[0], dict)
        and content[0].get("type") == "tool_result"
    )
```

### Algorithm

1. Identify all tool-result messages in `messages`
2. Mark last `keep_last_tool_results` as protected
3. **Soft-trim** unprotected results with `len(content) > tool_soft_trim_chars`:
   ```
   [First 1500 chars]
   --- trimmed (kept 1500 head + 1500 tail of 24000 chars) ---
   [Last 1500 chars]
   ```
4. **Hard-clear** results older than `tool_hard_clear_after` tool-result messages:
   ```
   [Tool output cleared — content was processed in earlier turns]
   ```
5. Skip image content, never modify assistant blocks or user text

### Integration Points

```python
# In _tool_loop() after line ~881, and stream_chat() after line ~775:
# After: messages.append({"role": "user", "content": tool_results_for_message})
if self._compactor:
    self._compactor.prune_tool_results(messages)
# Before: next API call
```

---

## Layer 2: History Compaction

### Token Estimation

```python
class TokenEstimator:
    """Token count estimation with optional calibration from API usage."""
    
    def __init__(self) -> None:
        self._ratio: float = 0.25  # tokens per char (chars/4 default)
        self._samples: int = 0
    
    def estimate(self, text: str | Any) -> int:
        if isinstance(text, str):
            return max(1, int(len(text) * self._ratio))
        return max(1, int(len(str(text)) * self._ratio))
    
    def estimate_messages(self, messages: list[dict[str, Any]]) -> int:
        return sum(self.estimate(m.get("content", "")) + 4 for m in messages)
    
    def calibrate(self, input_chars: int, actual_tokens: int) -> None:
        """Update ratio from actual API input_tokens. EMA with alpha=0.1."""
        if input_chars <= 0 or actual_tokens <= 0:
            return
        observed = actual_tokens / input_chars
        self._ratio = 0.1 * observed + 0.9 * self._ratio
        self._samples += 1
```

**Calibration limitations (acknowledged):**
- Resets on container restart (ephemeral). Persisting the ratio is a Phase 4 improvement.
- `actual_tokens` from API includes system prompt overhead — learned ratio is approximate, not exact per-message.
- Alpha=0.1 means ~10 samples to ~65% convergence. First few compaction decisions use near-default ratio. The 20K safety margin absorbs estimation error.

**Calibration hook in runner.py:** After each `_call_api()` response:
```python
if self._compactor and api_response.usage:
    input_chars = sum(len(str(m.get("content", ""))) for m in messages)
    self._compactor.estimator.calibrate(input_chars, api_response.usage.get("input_tokens", 0))
```

For streaming, calibrate after the turn completes using accumulated `total_usage`.

### Trigger and Integration (Explicit in Runner)

Compaction is triggered **explicitly** at orchestration level. `_format_messages()` stays sync and pure.

```python
# In stream_chat() and _tool_loop(), before first API call:
messages = self._format_messages(conversation)

if self._compactor and self._settings.compaction_enabled:
    system_tokens = self._compactor.estimator.estimate(system_prompt)
    history_tokens = self._compactor.estimator.estimate_messages(messages)
    
    if self._compactor.should_compact(system_tokens, history_tokens):
        lock = self._compaction_locks.setdefault(session_id, asyncio.Lock())
        async with lock:
            # Re-check under lock (double-check pattern)
            messages = self._format_messages(conversation)
            history_tokens = self._compactor.estimator.estimate_messages(messages)
            if self._compactor.should_compact(system_tokens, history_tokens):
                # Pre-compaction: find cut point, emit event, then compact
                cut_point = self._compactor.find_cut_point(
                    messages, self._settings.keep_recent_tokens
                )
                if cut_point > 0:
                    # Snapshot messages for event handlers (decoupled from mutation)
                    snapshot = messages[:cut_point]
                    await self._cognitive.pre_compaction(
                        agent_id=_agent_id,
                        session_id=session_id,
                        message_snapshot=snapshot,
                    )
                    await self._compactor.compact(
                        conversation, messages,
                        call_api=self._call_api,
                        cut_point=cut_point,
                    )
                messages = self._format_messages(conversation)
```

**Key design decisions:**
- `find_cut_point()` called by runner (not inside `compact()`) so cut_point is available for the pre-compaction event
- Message snapshot passed in event data — handlers don't race against mutation
- Compactor receives pre-computed `cut_point` — no sequencing ambiguity

### Lock Cleanup

Locks are cleaned up alongside conversation LRU eviction:

```python
# In _get_or_create_conversation(), LRU eviction path:
while len(self._conversations) >= MAX_CONVERSATIONS:
    evicted_id, _ = self._conversations.popitem(last=False)
    self._compaction_locks.pop(evicted_id, None)  # Clean up lock
```

### Compaction Algorithm

```python
async def compact(
    self,
    conversation: Conversation,
    messages: list[dict[str, Any]],
    call_api: ApiCaller,
    cut_point: int,  # Pre-computed by caller
) -> None:
    """Compact conversation history via structured summarization.
    
    Caller must verify cut_point > 0 before calling.
    """
    if cut_point <= 0:
        raise ValueError("cut_point must be > 0; caller should guard")
    if len(messages) != len(conversation.messages):
        raise ValueError(f"Index alignment required: {len(messages)} != {len(conversation.messages)}")
    
    old_messages = messages[:cut_point]
    
    # Skip synthetic summary prefix from previous compaction in serialization
    serialize_start = 2 if conversation.summary and len(old_messages) > 2 else 0
    
    try:
        checkpoint_text = await self._summarize(
            old_messages[serialize_start:], conversation.summary, call_api
        )
        if not self._validate_summary(checkpoint_text):
            raise ValueError("Summary failed validation")
    except Exception as e:
        logger.error("Compaction failed: %s — falling back to truncation", e)
        conversation.messages = conversation.messages[cut_point:]
        return
    
    # Rebuild messages
    compacted_prefix = [
        Message(role="user", content=f"[Previous conversation summary]\n\n{checkpoint_text}"),
        Message(role="assistant", content="I have the context. Let's continue."),
    ]
    
    recent_msgs = conversation.messages[cut_point:]
    if recent_msgs and recent_msgs[0].role != "user":
        found = False
        for i, msg in enumerate(recent_msgs):
            if msg.role == "user":
                recent_msgs = recent_msgs[i:]
                found = True
                break
        if not found:
            recent_msgs = []
    
    conversation.summary = checkpoint_text
    conversation.messages = compacted_prefix + recent_msgs
    conversation.compaction_count += 1
    
    # Clean up turn_contexts
    keep_contexts = max(1, len(recent_msgs) // 2)
    if len(conversation.turn_contexts) > keep_contexts:
        conversation.turn_contexts = conversation.turn_contexts[-keep_contexts:]
    
    logger.info("Compacted conversation %s: %d messages → %d + summary (%d chars)",
                conversation.session_id, len(messages), len(conversation.messages),
                len(checkpoint_text))
```

### Cut Point Selection

```python
def find_cut_point(self, messages: list[dict[str, Any]], keep_recent_tokens: int) -> int:
    """Walk backwards accumulating tokens. Returns index of old/recent split.
    Returns 0 if all messages fit (no compaction needed).
    Always snaps to user message boundary.
    """
    accumulated = 0
    for i in range(len(messages) - 1, -1, -1):
        accumulated += self.estimator.estimate(messages[i].get("content", ""))
        if accumulated >= keep_recent_tokens:
            for j in range(i, len(messages)):
                if messages[j].get("role") == "user":
                    return j
            return 0  # No user message found — keep everything
    return 0
```

### Summarization

```python
async def _summarize(
    self,
    old_messages: list[dict[str, Any]],
    existing_summary: str | None,
    call_api: ApiCaller,
) -> str:
    """Generate structured checkpoint summary."""
    if existing_summary:
        user_content = (
            f"## Existing Summary\n\n{existing_summary}\n\n"
            f"## New Conversation\n\n{self._serialize_for_summary(old_messages)}"
        )
        system = UPDATE_SYSTEM_PROMPT
    else:
        user_content = self._serialize_for_summary(old_messages)
        system = CHECKPOINT_SYSTEM_PROMPT
    
    response = await call_api(
        system_prompt=system,
        messages=[{"role": "user", "content": user_content}],
        tools=None,
        skip_thinking=True,
        model_override=self._settings.background_model,
    )
    return self.extract_text(response.content)
```

### `extract_text` (Static, on Compactor)

Duplicated from `AgentRunner._extract_text` as a static utility:

```python
@staticmethod
def extract_text(content: list[dict[str, Any]]) -> str:
    """Extract text from API response content blocks."""
    return "".join(
        block.get("text", "")
        for block in content
        if block.get("type") == "text"
    )
```

### `_call_api` Signature Changes (in runner.py)

```python
async def _call_api(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
    skip_thinking: bool = False,
    model_override: str | None = None,  # NEW
) -> ApiResponse:
    payload = self._build_api_payload(
        system_prompt, messages, tools,
        skip_thinking=skip_thinking,
        model_override=model_override,
    )
    ...

def _build_api_payload(
    self, ...,
    model_override: str | None = None,  # NEW
) -> dict[str, Any]:
    payload = {
        "model": model_override or self._settings.model,
        ...
    }
```

`_call_api_stream` does NOT need `model_override` — streaming is only for primary turns.

### Summary Validation

```python
def _validate_summary(self, summary: str) -> bool:
    """Basic format + length check. Not content validation (acknowledged limitation)."""
    if len(summary) < 200:
        logger.warning("Summary too short (%d chars)", len(summary))
        return False
    if len(summary) > 8000:
        logger.warning("Summary exceeds 8000 chars (%d) — accepting with warning", len(summary))
    # Case-insensitive section check with minor variants
    import re
    sections = [r"##\s*goals?\b", r"##\s*progress\b", r"##\s*critical\s*context\b"]
    found = sum(1 for pat in sections if re.search(pat, summary, re.IGNORECASE))
    if found < 2:
        logger.warning("Summary missing sections (%d/3)", found)
        return False
    return True
```

**Acknowledged limitation:** This is format validation, not content validation. A summary with correct headers but wrong facts passes. The real safety net is the fallback to truncation — wrong summary is discarded, recent messages preserved. Content validation (keyword extraction, second LLM call) is a Phase 4 consideration.

### Checkpoint Prompts

Live in `compaction.py` as module constants, co-located with the code that uses them.

```python
CHECKPOINT_SYSTEM_PROMPT = """You are a conversation summarizer. Output ONLY a structured summary.
TARGET LENGTH: 800-1200 words. Prioritize precision over completeness.

## Format

## Goal
[1-2 sentences]

## Constraints & Preferences  
- [Requirements, technical constraints]

## Progress
### Done
- [x] [Completed items]
### In Progress
- [ ] [Current work]

## Key Decisions
- **[Decision]**: [Rationale]

## Conversation Dynamics
- [User tone, frustration, preferences expressed]
- [Communication style, behavioral instructions given]
- [Unresolved questions]

## Next Steps
1. [Ordered list]

## Critical Context
- [File paths, error messages, API endpoints, variable names]
"""

UPDATE_SYSTEM_PROMPT = """You are updating a conversation summary with new messages.
TARGET LENGTH: 800-1200 words. If exceeding, prioritize:
1. Recent progress and decisions
2. Critical context (paths, errors, APIs)  
3. Active constraints
4. Conversation dynamics
Drop older completed "Done" items if needed.

RULES:
1. PRESERVE existing info unless explicitly superseded
2. ADD new progress, decisions, context
3. MOVE In Progress → Done when completed
4. UPDATE Conversation Dynamics with new signals
5. PRESERVE exact file paths, function names, error messages
6. Use SAME format as existing summary

Output ONLY the updated summary."""
```

**Acknowledged limitation (Conversation Dynamics):** LLMs tend to produce generic filler here ("User was engaged"). The section costs ~100-200 tokens per summary. Keeping it because: (a) occasionally captures real behavioral instructions ("stop being verbose"), (b) low cost, (c) removal is easy if proven useless. Not a substitute for real relational memory.

### Serialization

```python
def _serialize_for_summary(self, messages: list[dict[str, Any]]) -> str:
    """Serialize messages as readable text. conversation.messages is plain text only."""
    lines = []
    for msg in messages:
        role = "User" if msg.get("role") == "user" else "Assistant"
        content = msg.get("content", "")
        if isinstance(content, str):
            lines.append(f"**{role}:** {content}")
        elif isinstance(content, list):
            # Defensive: shouldn't happen with conversation.messages
            parts = [
                (item.get("content") or item.get("text") or str(item))
                if isinstance(item, dict) else str(item)
                for item in content
            ]
            lines.append(f"**{role}:** {chr(10).join(parts)}")
    return "\n\n".join(lines)
```

**Acknowledged limitation:** The summarizer only sees plain text user/assistant exchanges, not tool outputs. The assistant's responses reference tool results ("I found X in the file") which partially captures tool context, but detail is lost. This is inherent to the `conversation.messages` storage model and would require architectural change to fix.

### Pre-Compaction Event via CognitiveLayer

```python
# In CognitiveLayer:
async def pre_compaction(
    self, agent_id: str, session_id: str,
    message_snapshot: list[dict[str, Any]],
) -> None:
    """Emit pre-compaction event. Message snapshot decouples handlers from mutation timing."""
    if self._bus:  # Guard: bus may be None
        await self._bus.emit(Event(
            type="conversation_compacting",
            agent_id=agent_id,
            session_id=session_id,
            data={"message_snapshot": message_snapshot},
        ))
```

**Naming:** Uses `pre_compaction` (not `on_pre_compaction`) to match existing lifecycle pattern (`pre_turn`, `post_turn`).

**Event bus timing (acknowledged):** `EventBus.emit()` is fire-and-forget (queues event, handlers run in background `_process_loop`). For Phase 2, this is fine — no handlers exist yet. For Phase 3 knowledge extraction, this means handlers may not complete before compaction mutates the conversation. The message snapshot in event data mitigates this: handlers operate on the snapshot copy, not on live conversation state. If synchronous dispatch is needed in Phase 3, add `EventBus.emit_and_wait()` at that time.

---

## Data Model Changes

### Conversation Dataclass

```python
@dataclass
class Conversation:
    session_id: str
    messages: list[Message] = field(default_factory=list)
    turn_contexts: list[TurnContext] = field(default_factory=list)
    summary: str | None = None          # NEW
    compaction_count: int = 0           # NEW
```

### `_format_messages` (Unchanged — Sync and Pure)

```python
def _format_messages(self, conversation: Conversation) -> list[dict[str, Any]]:
    """Pure formatting, no side effects."""
    if not self._settings.compaction_enabled:
        return [{"role": m.role, "content": m.content}
                for m in conversation.messages[-MAX_HISTORY_MESSAGES:]]
    return [{"role": m.role, "content": m.content} for m in conversation.messages]
```

When compaction is enabled, returns all messages 1:1 with `conversation.messages`. This alignment is critical — `find_cut_point` returns an index into `messages` that maps directly to `conversation.messages`. An assertion in `compact()` guards this.

### `_format_history_text` (Summary-Aware)

```python
def _format_history_text(self, conversation: Conversation) -> str:
    lines = []
    if conversation.summary:
        lines.append(f"[Previous context summary]\n{conversation.summary}\n")
    for msg in conversation.messages[-20:]:
        role_label = "User" if msg.role == "user" else "Assistant"
        lines.append(f"{role_label}: {msg.content}")
    return "\n\n".join(lines)
```

---

## Configuration

```python
# config.py — all with NOUS_ prefixed env var aliases

# Layer 1: Tool Pruning
tool_pruning_enabled: bool = Field(default=True, validation_alias="NOUS_TOOL_PRUNING_ENABLED")
tool_soft_trim_chars: int = Field(default=4000, validation_alias="NOUS_TOOL_SOFT_TRIM_CHARS")
tool_soft_trim_head: int = Field(default=1500, validation_alias="NOUS_TOOL_SOFT_TRIM_HEAD")
tool_soft_trim_tail: int = Field(default=1500, validation_alias="NOUS_TOOL_SOFT_TRIM_TAIL")
tool_hard_clear_after: int = Field(default=6, validation_alias="NOUS_TOOL_HARD_CLEAR_AFTER")
keep_last_tool_results: int = Field(default=2, validation_alias="NOUS_KEEP_LAST_TOOL_RESULTS")

# Layer 2: History Compaction
compaction_enabled: bool = Field(default=False, validation_alias="NOUS_COMPACTION_ENABLED")
compaction_threshold: int = Field(default=100_000, validation_alias="NOUS_COMPACTION_THRESHOLD")
keep_recent_tokens: int = Field(default=20_000, validation_alias="NOUS_KEEP_RECENT_TOKENS")
```

---

## Implementation Plan

### Phase 1: Tool Output Pruning + Token Estimation

**New file:** `nous/api/compaction.py`
**Modified:** `runner.py`, `config.py`
**Risk:** Low
**Estimate:** ~250 lines new + ~30 lines modified

1. Create `compaction.py` with `ConversationCompactor`, `TokenEstimator`, `ApiCaller` Protocol
2. Add Layer 1 config fields to `NousSettings`
3. Implement `is_tool_result_message()`, `prune_tool_results()`, `extract_text()`
4. Instantiate compactor in `AgentRunner.__init__`
5. Hook pruning into `_tool_loop()` and `stream_chat()`
6. Add calibration hook after API responses
7. Tests for pruning + estimation

### Phase 2: History Compaction Core

**Modified:** `compaction.py`, `runner.py`, `config.py`
**Risk:** Medium
**Estimate:** ~300 lines new + ~50 lines modified

1. Add `summary`, `compaction_count` to `Conversation`
2. Add Layer 2 config fields
3. Add `model_override` to `_build_api_payload()` and `_call_api()`
4. Implement `should_compact()`, `find_cut_point()`, `compact()`, `_summarize()`, `_validate_summary()`
5. Add checkpoint prompts as constants
6. Add per-session lock + cleanup on LRU eviction
7. Explicit compaction trigger in `stream_chat()` and `_tool_loop()`
   - **Note:** `_tool_loop()` needs `agent_id` access for pre-compaction events. Either add as parameter or use `self._settings.agent_id`.
8. Update `_format_messages()` and `_format_history_text()`
9. Feature flag: `NOUS_COMPACTION_ENABLED=false`
10. Structured compaction metrics logging (cut count, kept count, summary generation latency, estimation accuracy)
11. Tests for compaction + edge cases + concurrency

### Phase 3: Durable Integration

**Modified:** `compaction.py`, `cognitive/layer.py`, `heart/heart.py`

1. Add `pre_compaction()` to `CognitiveLayer` with `if self._bus:` guard
2. Wire runner → cognitive layer with message snapshot
3. Knowledge extraction handler (facts/decisions from compacted messages)
4. Episode boundary on compaction
5. Conversation state persistence:

```sql
CREATE TABLE heart.conversation_state (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_id TEXT NOT NULL,
    session_id TEXT NOT NULL,
    summary TEXT,
    messages JSONB,
    turn_count INT NOT NULL DEFAULT 0,
    compaction_count INT NOT NULL DEFAULT 0,
    created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    UNIQUE(agent_id, session_id)
);
```

**Why separate from `working_memory`:** Different lifecycle (working_memory clears per session, conversation_state survives restarts), different access patterns (working_memory read every turn, conversation_state read on session resume only).

6. Save/restore on session create/resume

### Phase 4: Adaptive & Advanced

1. Dynamic `compaction_threshold`
2. Priority-based message retention
3. Tool call tracking in checkpoints
4. Versioned summaries (keep last 2 for drift detection)
5. Persist `TokenEstimator` ratio across restarts
6. Content-level summary validation (keyword extraction)

---

## Testing Strategy

### Phase 1
- `test_is_tool_result_message_{positive,negative}` — correct identification
- `test_soft_trim_{large,small}` — trim format, threshold behavior
- `test_hard_clear_old` — placeholder replacement
- `test_protect_last_n` — recency zone
- `test_never_modify_{assistant,user_text}` — role protection
- `test_skip_image_results` — image preservation
- `test_{empty,all_protected}_noop` — edge cases
- `test_estimator_{initial,calibration}` — ratio behavior

### Phase 2
- `test_should_compact_{under,over}_threshold` — trigger logic
- `test_find_cut_point_{keeps_recent,returns_zero,snaps_to_user,no_user_returns_zero}` — all paths
- `test_cut_point_zero_guard` — compact() assertion
- `test_alternation_{preserved,no_user_empty}` — post-compaction validity
- `test_iterative_update_prompt` — UPDATE vs CHECKPOINT selection
- `test_iterative_skips_synthetic_prefix` — no duplicate summary
- `test_summary_validation_{short,missing_sections,case_insensitive}` — validation
- `test_summarization_failure_fallback` — truncation on error
- `test_conversation_state_updated` — summary + count
- `test_turn_contexts_cleaned` — old contexts removed
- `test_format_history_text_with_summary` — reflection prefix
- `test_concurrency_lock` — no corruption
- `test_config_threshold` — env var override
- `test_index_alignment_assertion` — catches misalignment
- `test_lock_cleanup_on_eviction` — no memory leak

---

## Risk Assessment

| Risk | Mitigation |
|------|------------|
| Tool trimming hides needed data | Trim marker prompts re-request; protect last 2 |
| Summary loses context | Structured format; Heart extraction; iterative update |
| Summary drift (5+ compactions) | Size cap; Phase 4 versioned summaries |
| Token estimation error | 20K margin; API calibration; conservative default |
| Alternation violation | Boundary snapping + for/else + assertion + tests |
| Bad summary accepted | Format validation + fallback to truncation; content validation deferred to Phase 4 |
| Concurrent compaction race | Per-session lock + double-check + lock cleanup |
| Pre-compaction handler race | Message snapshot in event data (decoupled from mutation) |

---

## v4 → v5 Changes

| v4 Issue | v5 Fix |
|----------|--------|
| `_extract_text` not on compactor (P1) | Added as `@staticmethod extract_text()` on `ConversationCompactor` |
| `on_pre_compaction` missing `if self._bus:` guard (P1) | Added guard, matches existing CognitiveLayer pattern |
| Pre-compaction event needs `cut_point` computed inside `compact()` (P1) | Runner calls `find_cut_point()` first, passes `cut_point` to `compact()` |
| EventBus.emit() fire-and-forget claim (P1/P2) | Corrected: acknowledged fire-and-forget, pass message snapshot in event data |
| Lock dict memory leak (P2) | Clean up on LRU eviction |
| TokenEstimator ownership ambiguous (P2) | Compactor owns it, exposes as `compactor.estimator` |
| Compactor instantiation not shown (P2) | `AgentRunner.__init__` changes shown |
| Unparameterized `Callable` (P3) | `ApiCaller` and `PreCompactCallback` Protocol types |
| Summary validation exact string match (P2) | Regex with case-insensitive matching |
| Duplicate summary in iterative update (P3) | Skip synthetic prefix (first 2 messages) when serializing |
| `calibrate()` param name (P3) | Renamed to `input_chars` |
| `on_pre_compaction` naming (P3) | Renamed to `pre_compaction` matching lifecycle pattern |
| `conversation_state` vs `working_memory` (P3) | Separate tables with documented justification |
| `call_api` per-call vs init-time (P3) | Kept per-call: Protocol type makes signature explicit |
| Streaming calibration hook (P3) | Documented: calibrate after turn using accumulated `total_usage` |
| Index alignment fragility (P3) | `assert len(messages) == len(conversation.messages)` in `compact()` |
| Conversation Dynamics generic filler concern | Acknowledged limitation in spec |
| Summary drift inherent to fixed-size compression | Acknowledged; Phase 4 versioned summaries |
| 34KB spec too complex | Trimmed from 34KB to ~24KB; removed redundant prose |

## References

- Claude Code SDK `compaction.js` — structured checkpoint, iterative update, token-based cut points
- pi-coding-agent `compaction.js` — split-turn handling, UPDATE prompt pattern
- OpenClaw session pruning — two-layer approach, soft-trim/hard-clear
- Nous: `runner.py` lines 564-810 (stream_chat), 811-930 (_tool_loop), 1082-1100 (_format_messages)
