# Spec 008.1: Conversation Compaction (v2)

**Status:** Draft v2 (revised from 3-agent review + Claude Code SDK analysis)
**Depends on:** 008 (Tiered Context Model)
**Issue:** #68

## Problem

Spec 008 manages the system prompt well (Tier 1 always-on, Tier 3 semantic search with budgets), but **conversation history has zero token management**. Currently:

- `MAX_HISTORY_MESSAGES = 20` — hard cutoff by message count, not tokens
- Tool outputs within a turn are local variables — never stored in conversation history
- No token counting before API calls
- No truncation of tool outputs during tool loops
- Conversation is in-memory only — lost on container restart

### What Actually Causes Token Bloat

**Within a turn (in-turn):** Tool loops accumulate tool_use + tool_result messages in the local `messages` list passed to API calls. A 10-iteration bash loop can easily generate 30K+ tokens. These are rebuilt each turn from `conversation.messages` + current tool results. **Compaction cannot touch these.**

**Across turns (history):** `conversation.messages` stores plain text user/assistant pairs (tool outputs are NOT stored). With `MAX_HISTORY_MESSAGES = 20` and ~300 tokens per Telegram message, history is typically ~6K tokens. **The 20-message cap means history rarely exceeds 10K tokens.**

### Implication

The spec must address BOTH layers:
1. **In-turn tool output management** — truncate/prune tool results during tool loops
2. **History compaction** — for when the 20-message cap is removed and conversations grow

## Goals

1. Prevent context window overflow with graceful degradation
2. Preserve important context through structured checkpoints (not lossy summarization)
3. Integrate with existing Heart/Brain for durable knowledge extraction
4. Minimal latency impact on user-facing turns

## Non-Goals

- Cross-session conversation persistence (separate feature)
- Multi-user conversation management

## Architecture

### Two-Layer Approach (Inspired by OpenClaw/Claude Code)

```
Layer 1: Tool Output Pruning (per-request, cheap, no LLM)
  - Runs before every API call during tool loops
  - Soft-trim old tool results (head + tail + marker)
  - Hard-clear very old tool results (replace with placeholder)
  - Only touches tool_result messages, never user/assistant

Layer 2: History Compaction (rare, LLM-powered)
  - Triggered when estimated input tokens exceed budget
  - Pre-compaction extraction turn (save to Heart/Brain)
  - Structured checkpoint summary (not free-form prose)
  - Iterative update (merge with previous summary)
  - Persisted in conversation state
```

### Token Budget Model (Sonnet 4.6, 200K context)

| Component | Budget | Notes |
|-----------|--------|-------|
| System prompt | ~5K | Identity + Tier 1 + Tier 3 context |
| Conversation summary | ~3K | Structured checkpoint |
| Recent messages | ~20K tokens | Token-based window, not message count |
| In-turn tool results | ~30K | Managed by Layer 1 pruning |
| Output headroom | ~16K | `max_tokens` setting |
| Safety margin | ~20K | Buffer |

---

## Layer 1: Tool Output Pruning

### Design

Runs inside the tool loop (`stream_chat()` and `_tool_loop()`), BEFORE each subsequent API call. Trims the accumulated tool_result messages in the local `messages` list.

```python
def _prune_tool_results(self, messages: list[dict], context_window: int) -> list[dict]:
    """Prune old tool results from in-turn message accumulation.
    
    Two-phase approach:
    1. Soft-trim: Keep head + tail of oversized results (> TOOL_SOFT_TRIM_CHARS)
    2. Hard-clear: Replace very old results with placeholder
    
    Never modifies user or assistant messages.
    Protected zone: last N tool results (KEEP_LAST_TOOL_RESULTS).
    """
```

### Configuration

```python
# config.py additions
tool_soft_trim_chars: int = 4000          # Soft-trim results larger than this
tool_soft_trim_head: int = 1500           # Keep first N chars
tool_soft_trim_tail: int = 1500           # Keep last N chars
tool_hard_clear_ratio: float = 0.5        # Hard-clear when > 50% of context window
tool_hard_clear_placeholder: str = "[Tool result cleared — content was already processed]"
keep_last_tool_results: int = 3           # Never prune last N tool results
```

### Soft-Trim Format

```
[First 1500 chars of output]
...
[Last 1500 chars of output]

[Tool result trimmed: kept first 1500 and last 1500 of 24000 chars.]
```

### Integration Point

In `stream_chat()`, after tool execution and before the next `_stream_api()` call:

```python
# After tool results are appended to messages
messages = self._prune_tool_results(messages, self._context_window_tokens)
```

### Key Rules

- Only prune `tool_result` messages (role check)
- Never prune user or assistant messages
- Protect last `keep_last_tool_results` results (the model just generated tool calls based on these)
- Skip results containing image content
- Append truncation marker so Claude knows to re-request if needed

---

## Layer 2: History Compaction

### Trigger

Replace `MAX_HISTORY_MESSAGES = 20` with token-based management:

```python
HISTORY_TOKEN_BUDGET = 40_000  # configurable via NOUS_HISTORY_TOKEN_BUDGET

def _should_compact(self, system_prompt: str, messages: list) -> bool:
    total = self._estimate_tokens_all(system_prompt, messages)
    return total > self._context_window - self._settings.compaction_reserve_tokens
```

Default `compaction_reserve_tokens = 30000` (output headroom + safety margin).

### Compaction Algorithm

```
1. Estimate total input tokens (system_prompt + history + current message)
2. If under budget → send as-is
3. If over budget:
   a. Pre-compaction extraction (optional — emit event for handlers)
   b. Find cut point: walk backwards from newest, accumulate tokens
      until hitting keep_recent_tokens (default 20K)
   c. Split at valid boundary (user message, never mid-tool-result)
   d. Summarize old messages into structured checkpoint
   e. Replace history: [checkpoint_pair] + [recent messages]
```

### Cut Point Selection

Token-based, not message-count-based (learned from Claude Code SDK):

```python
def _find_cut_point(self, messages: list[Message], keep_recent_tokens: int) -> int:
    """Walk backwards accumulating tokens. Return index of cut point.
    
    Cut at user message boundaries only (never split tool pairs).
    If cutting mid-turn, include the full turn in 'recent'.
    """
    accumulated = 0
    for i in range(len(messages) - 1, -1, -1):
        accumulated += self._estimate_tokens(messages[i].content)
        if accumulated >= keep_recent_tokens:
            # Snap to nearest user message boundary
            while i < len(messages) and messages[i].role != "user":
                i += 1
            return i
    return 0  # Keep everything
```

### Structured Checkpoint Format (Learned from Claude Code)

NOT free-form prose. Structured format that an LLM can parse and continue from:

```
## Goal
[What the user is trying to accomplish]

## Constraints & Preferences
- [Requirements, preferences mentioned]

## Progress
### Done
- [x] [Completed items]

### In Progress
- [ ] [Current work]

### Blocked
- [Issues, if any]

## Key Decisions
- **[Decision]**: [Brief rationale]

## Next Steps
1. [Ordered list]

## Critical Context
- [Data, file paths, error messages needed to continue]
```

### Iterative Update (Avoids "Photocopy of Photocopy")

When a previous checkpoint exists, use an UPDATE prompt instead of re-summarizing everything:

```python
INITIAL_SUMMARIZATION_PROMPT = """...(create new checkpoint)..."""

UPDATE_SUMMARIZATION_PROMPT = """The messages above are NEW conversation since the last checkpoint.
Update the existing summary with new information. RULES:
- PRESERVE all existing information from the previous summary
- ADD new progress, decisions, and context
- UPDATE Progress: move items from In Progress to Done when completed
- UPDATE Next Steps based on what was accomplished
- PRESERVE exact file paths, function names, error messages"""
```

### Summarization Model

Use the **same model** via `_call_api()` with a `model_override` parameter (not a separate Haiku client). Rationale from Claude Code SDK: the summary IS the agent's memory — quality matters more than cost.

```python
# Add model_override to _build_api_payload()
def _build_api_payload(self, ..., model_override: str | None = None) -> dict:
    payload = {
        "model": model_override or self._settings.model,
        ...
    }
```

Use `_call_api(skip_thinking=True, model_override=self._settings.background_model)`. The `background_model` setting already exists in config.

### Conversation Serialization

Convert messages to plain text before sending to summarizer (Claude Code pattern). The summarizer should not try to "continue" the conversation:

```python
SUMMARIZATION_SYSTEM_PROMPT = """You are a context summarization assistant. 
Read the conversation and produce a structured summary following the exact format specified.
Do NOT continue the conversation. Do NOT respond to questions. ONLY output the structured summary."""

def _serialize_for_summary(self, messages: list[Message]) -> str:
    lines = []
    for msg in messages:
        role = "User" if msg.role == "user" else "Assistant"
        lines.append(f"{role}: {msg.content}")
    return "\n\n".join(lines)
```

### Message Format After Compaction

```python
# Synthetic summary pair preserving alternation
compacted_messages = [
    Message(role="user", content=f"[Conversation Summary]\n{checkpoint_text}"),
    Message(role="assistant", content="I have the context from our previous conversation. Continuing."),
]

# Ensure recent window starts with user message
recent = messages[cut_point:]
while recent and recent[0].role != "user":
    recent = recent[1:]  # Drop leading assistant messages

conversation.summary = checkpoint_text
conversation.messages = compacted_messages + recent
```

### Pre-Compaction Knowledge Extraction (Phase 2)

Before summarizing, emit an event so handlers can extract durable knowledge:

```python
# In CognitiveLayer.on_compaction()
await self._bus.emit(Event(
    type="conversation_compacting",
    data={"messages": old_messages, "session_id": session_id}
))
# Handlers can extract decisions, facts, episode boundaries
```

This mirrors OpenClaw's "memory flush" — a chance to save important context before it's compressed.

---

## Implementation Plan

### Phase 1: Tool Output Pruning + Token Estimation

**Files:** `runner.py`, `config.py`
**Risk:** Low — only affects in-turn tool results, doesn't change conversation history

1. `_estimate_tokens(text) -> int` — chars/4 estimate (conservative)
2. `_prune_tool_results(messages, context_window) -> list` — soft-trim + hard-clear
3. Wire into `stream_chat()` and `_tool_loop()` tool result processing
4. Config: `tool_soft_trim_chars`, `tool_soft_trim_head`, `tool_soft_trim_tail`, `keep_last_tool_results`
5. Log warning when estimated input > 60% of context window
6. Feature flag: `NOUS_TOOL_PRUNING_ENABLED=true` (default on)

### Phase 2: History Compaction

**Files:** `runner.py`, `config.py`, `schemas.py` (Conversation dataclass)
**Depends on:** Phase 1 (token estimation)

1. Remove `MAX_HISTORY_MESSAGES = 20` cap
2. Add `summary: str | None` to Conversation dataclass
3. `_should_compact()` — token-based trigger
4. `_find_cut_point()` — token-based backwards walk
5. `_compact_conversation()` — serialize + summarize + rebuild
6. Add `model_override` parameter to `_build_api_payload()` and `_call_api()`
7. Structured checkpoint prompt (initial + update)
8. Alternation safety: ensure recency window starts with user message
9. Fallback: if summarization fails, truncate without summary
10. Config: `compaction_reserve_tokens`, `keep_recent_tokens`, `compaction_enabled`
11. Feature flag: `NOUS_COMPACTION_ENABLED=false` (default off, opt-in)

### Phase 3: Durable Integration

**Files:** `runner.py`, `cognitive/layer.py`, `heart/heart.py`

1. `CognitiveLayer.on_compaction(session_id, summary, old_messages)` hook
2. Pre-compaction event emission for knowledge extraction
3. Episode boundary: compaction closes the current episode segment
4. Working memory update from summary
5. Conversation state persistence (new table `heart.conversation_state` — metadata only, NOT raw messages)

```sql
CREATE TABLE heart.conversation_state (
    agent_id TEXT NOT NULL,
    session_id TEXT NOT NULL,
    summary TEXT,
    turn_count INT NOT NULL DEFAULT 0,
    total_compactions INT NOT NULL DEFAULT 0,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    UNIQUE(agent_id, session_id)
);
```

### Phase 4: Adaptive & Advanced

1. Dynamic keep_recent_tokens based on conversation pattern
2. Priority-based retention (decision messages kept longer)
3. File operation tracking from tool calls (append to checkpoint)
4. Split-turn awareness (if compaction cuts mid-tool-loop)

---

## Configuration

```python
# config.py additions (all NOUS_ prefixed env vars)

# Layer 1: Tool Pruning
tool_pruning_enabled: bool = True
tool_soft_trim_chars: int = 4000
tool_soft_trim_head: int = 1500
tool_soft_trim_tail: int = 1500
tool_hard_clear_ratio: float = 0.5
tool_hard_clear_placeholder: str = "[Tool result cleared]"
keep_last_tool_results: int = 3

# Layer 2: History Compaction
compaction_enabled: bool = False  # Opt-in initially
compaction_reserve_tokens: int = 30000
keep_recent_tokens: int = 20000
```

## Testing Strategy

### Phase 1 Tests
- `test_soft_trim_large_result` — verify head+tail+marker format
- `test_hard_clear_old_results` — verify placeholder replacement
- `test_never_prune_user_assistant` — role protection
- `test_protect_last_n_results` — recency protection
- `test_skip_image_results` — image content preservation

### Phase 2 Tests
- `test_should_compact_trigger` — token threshold calculation
- `test_find_cut_point_token_based` — backwards walk accuracy
- `test_cut_at_user_boundary` — never cut at tool results
- `test_alternation_preserved` — user/assistant alternation after compaction
- `test_alternation_with_odd_messages` — recency window starting with assistant (P1 from review)
- `test_iterative_update` — previous checkpoint merged, not re-summarized
- `test_summarization_failure_fallback` — truncate without summary on error
- `test_empty_old_messages` — guard against compacting nothing

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Tool result truncation hides needed data | Medium | Medium | Truncation marker tells Claude to re-request; protect last 3 results |
| Summary loses critical context | Medium | High | Structured format preserves key info; Heart/Brain capture decisions/facts; iterative update preserves old context |
| Summarization latency blocks response | Low | Medium | Timeout with fallback to truncation; runs rarely (only when budget exceeded) |
| Alternation violation after compaction | High (if not handled) | High | Explicit boundary snapping + test coverage for odd-message case |
| Context window estimation inaccuracy | Medium | Low | 30K reserve tokens absorbs estimation error; chars/4 is conservative |
| Summary model API failure | Low | Medium | Fallback: truncate old messages without summary |

## Key Differences from v1

| Aspect | v1 (Original) | v2 (Revised) |
|--------|---------------|--------------|
| Scope | History only | Two layers: tool pruning + history compaction |
| Tool outputs | Assumed in conversation.messages (wrong) | Correctly identified as in-turn local variables |
| Trigger threshold | 80K tokens (would never fire with 20-msg cap) | Token-based with cap removed |
| Summary format | Free-form prose | Structured checkpoint (Goal/Progress/Decisions/Next) |
| Summary model | Separate Haiku client | Same model via `_call_api(model_override)` |
| Update strategy | Re-summarize everything | Iterative update (preserve + merge) |
| Alternation handling | Not addressed | Explicit boundary snapping + guard |
| Pre-compaction flush | Not addressed | Event emission for knowledge extraction |
| Cut point | Message count (RECENCY_WINDOW=6) | Token-based backwards walk |
| Concurrency | Not addressed | Per-session lock (Phase 3) |

## References

- Claude Code SDK `compaction.js` — structured checkpoint format, iterative update, token-based cut points, split-turn handling
- OpenClaw session pruning — two-layer approach (pruning + compaction), soft-trim/hard-clear, TTL-based
- OpenClaw auto-compaction — pre-compaction memory flush, retry logic, safety timeout
