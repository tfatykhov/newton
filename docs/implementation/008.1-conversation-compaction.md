# Spec 008.1: Conversation Compaction (v3)

**Status:** Draft v3
**Depends on:** 008 (Tiered Context Model)
**Issue:** #68

## Problem

Spec 008 manages the system prompt well (Tier 1 always-on, Tier 3 semantic search with budgets), but **conversation history has zero token management**. Currently:

- `MAX_HISTORY_MESSAGES = 20` — hard cutoff by message count, not tokens
- Tool outputs within a turn are local variables — never stored in `conversation.messages`
- No token counting before API calls
- No truncation of tool outputs during tool loops
- Conversation is in-memory only — lost on container restart

### What Actually Causes Token Bloat

**Within a turn (in-turn):** Tool loops accumulate tool_use + tool_result messages in the local `messages` list passed to API calls. A 10-iteration bash loop can generate 30K+ tokens. These are rebuilt each turn from `conversation.messages` + current tool results. **Compaction cannot touch these — only pruning can.**

**Across turns (history):** `conversation.messages` stores plain text user/assistant pairs (tool outputs are NOT stored). With `MAX_HISTORY_MESSAGES = 20` and ~300 tokens per Telegram message, history is typically ~6K tokens. **The 20-message cap means history rarely exceeds 10K tokens today.**

### Why Both Layers Matter

Layer 1 (tool pruning) addresses the immediate bloat problem — tool results that accumulate within a single turn. This is where context window overflows actually happen today.

Layer 2 (history compaction) becomes essential once the 20-message cap is removed. Without it, long conversations will eventually exceed the context window. It also enables persistent conversation state across container restarts (Phase 3).

---

## Architecture

### Two-Layer Approach

```
Layer 1: Tool Output Pruning (per-request, cheap, no LLM)
  ├── Runs before every API call during tool loops
  ├── Soft-trim old tool results (head + tail + marker)
  ├── Hard-clear very old tool results (→ placeholder)
  └── Only touches tool_result content, never user/assistant text

Layer 2: History Compaction (rare, LLM-powered)
  ├── Triggered when estimated input tokens exceed budget
  ├── Pre-compaction event emission (save to Heart/Brain)
  ├── Structured checkpoint summary (not free-form prose)
  ├── Iterative update (merge with previous summary)
  └── Persisted in Conversation state
```

### Token Budget Model (Sonnet 4.6, 200K context)

| Component | Budget | Notes |
|-----------|--------|-------|
| System prompt (Tier 1 + 3) | ~5K | Identity, user profile, censors, recalled context |
| Conversation summary | ~3K | Structured checkpoint (when compaction active) |
| Recent messages | ~20K | Token-based window, not message count |
| In-turn tool results | ~30K | Managed by Layer 1 pruning |
| Thinking tokens | variable | Not in input, but model allocates from output budget |
| Output headroom | ~16K | `max_tokens` setting |
| Safety margin | ~20K | Buffer for estimation error |

---

## Layer 1: Tool Output Pruning

### Design

Runs inside the tool loop, BEFORE each subsequent API call. Trims the accumulated `tool_result` content in the local `messages` list.

```python
def _prune_tool_results(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Prune old tool results from in-turn message accumulation.
    
    Two-phase approach:
    1. Soft-trim: Keep head + tail of oversized results (> tool_soft_trim_chars)
    2. Hard-clear: Replace very old results with placeholder
    
    Never modifies user text or assistant content blocks.
    Protected zone: last keep_last_tool_results user messages containing tool results.
    """
```

### Configuration

```python
# config.py additions (all NOUS_ prefixed env vars)
tool_pruning_enabled: bool = Field(default=True, validation_alias="NOUS_TOOL_PRUNING_ENABLED")
tool_soft_trim_chars: int = Field(default=4000, validation_alias="NOUS_TOOL_SOFT_TRIM_CHARS")
tool_soft_trim_head: int = Field(default=1500, validation_alias="NOUS_TOOL_SOFT_TRIM_HEAD")
tool_soft_trim_tail: int = Field(default=1500, validation_alias="NOUS_TOOL_SOFT_TRIM_TAIL")
tool_hard_clear_after: int = Field(default=6, validation_alias="NOUS_TOOL_HARD_CLEAR_AFTER")
keep_last_tool_results: int = Field(default=2, validation_alias="NOUS_KEEP_LAST_TOOL_RESULTS")
```

### How It Works

**Identify tool result messages:** A user message is a tool result message when its `content` is a list of dicts with `type: "tool_result"`. Regular user messages have string content.

```python
def _is_tool_result_message(self, msg: dict) -> bool:
    content = msg.get("content")
    return (
        msg.get("role") == "user"
        and isinstance(content, list)
        and len(content) > 0
        and isinstance(content[0], dict)
        and content[0].get("type") == "tool_result"
    )
```

**Protection zone:** The last `keep_last_tool_results` tool result messages are never touched. The model just generated tool calls based on these results — pruning them would confuse it.

**Soft-trim:** For tool results outside the protection zone with `len(content_str) > tool_soft_trim_chars`:

```
[First 1500 chars of output]

--- trimmed (kept 1500 head + 1500 tail of 24000 chars) ---

[Last 1500 chars of output]
```

**Hard-clear:** For tool results more than `tool_hard_clear_after` tool-result messages old (counting from the end), replace content entirely:

```
[Tool output cleared — content was processed in earlier turns]
```

### Integration Points

**In `_tool_loop()` (line ~841):** After tool results are appended, before the next `_call_api()`:

```python
# After: messages.append({"role": "user", "content": tool_results_for_message})
if self._settings.tool_pruning_enabled:
    self._prune_tool_results(messages)
# Before: api_response = await self._call_api(...)
```

**In `stream_chat()` (line ~775):** Same pattern, after tool results appended, before next stream call:

```python
# After: messages.append({"role": "user", "content": tool_results_for_message})
if self._settings.tool_pruning_enabled:
    self._prune_tool_results(messages)
# Before: async for event in self._call_api_stream(...)
```

### Key Rules

1. Only modify `tool_result` content (never assistant blocks, never user text messages)
2. Protect last `keep_last_tool_results` tool-result messages
3. Skip results containing image content (check for `type: "image"` blocks)
4. Append trim marker so Claude knows to re-request if needed
5. Mutates in-place for efficiency (no copy of the full messages list)

---

## Layer 2: History Compaction

### Prerequisites

Remove `MAX_HISTORY_MESSAGES = 20` and replace with token-based management. This is what makes compaction necessary.

### Token Estimation

Use actual API usage when available, chars/4 as fallback:

```python
def _estimate_tokens(self, text: str) -> int:
    """Estimate token count. Conservative chars/4 heuristic."""
    if isinstance(text, str):
        return len(text) // 4
    # For list content (tool results), serialize first
    return len(str(text)) // 4

def _estimate_messages_tokens(self, messages: list[dict]) -> int:
    """Estimate total tokens for a message list."""
    total = 0
    for msg in messages:
        content = msg.get("content", "")
        total += self._estimate_tokens(content)
        total += 4  # role + structural overhead per message
    return total
```

**Why chars/4 is good enough:** With a 20K safety margin, even 30-60% underestimation on code/JSON won't cause overflow. The trigger threshold is deliberately conservative. If we had tiktoken available we'd use it, but chars/4 with large margins works.

### Trigger

```python
COMPACTION_THRESHOLD = 100_000  # trigger when estimated input > this

def _should_compact(self, system_tokens: int, history_tokens: int) -> bool:
    """Check if compaction is needed before a turn."""
    if not self._settings.compaction_enabled:
        return False
    total = system_tokens + history_tokens + self._settings.max_tokens
    return total > COMPACTION_THRESHOLD
```

Checked in `_format_messages()` — the single chokepoint where conversation becomes API messages.

### Integration Point: `_format_messages()` Becomes the Gate

Currently `_format_messages` is a simple truncation. It becomes the compaction integration point:

```python
def _format_messages(self, conversation: Conversation, system_prompt: str = "") -> list[dict[str, Any]]:
    """Format conversation history for API calls.
    
    Token-aware: triggers compaction when history exceeds budget.
    Returns: list of message dicts ready for API.
    """
    messages = [{"role": m.role, "content": m.content} for m in conversation.messages]
    
    if not self._settings.compaction_enabled:
        # Legacy behavior: message-count cap
        return messages[-MAX_HISTORY_MESSAGES:]
    
    system_tokens = self._estimate_tokens(system_prompt)
    history_tokens = self._estimate_messages_tokens(messages)
    
    if self._should_compact(system_tokens, history_tokens):
        # Run compaction synchronously (called from async context via await)
        # This is rare — only when history actually exceeds threshold
        messages = await self._compact_conversation(conversation, messages)
    
    return messages
```

**Note:** This means `_format_messages` becomes async. Both callers (`stream_chat` and `_tool_loop`) are already async, so this is straightforward. Rename to `_prepare_messages()` to signal the behavior change.

### Compaction Algorithm

```python
async def _compact_conversation(
    self, 
    conversation: Conversation,
    messages: list[dict[str, Any]],
) -> list[dict[str, Any]]:
    """Compact conversation history via structured summarization.
    
    1. Emit pre-compaction event (for knowledge extraction)
    2. Find cut point (keep recent ~20K tokens)
    3. Summarize old messages into structured checkpoint
    4. Rebuild: [checkpoint pair] + [recent messages]
    5. Update conversation state
    """
```

### Cut Point Selection

Token-based backwards walk from newest messages:

```python
def _find_cut_point(self, messages: list[dict[str, Any]], keep_recent_tokens: int) -> int:
    """Walk backwards accumulating tokens. Return index where old/recent split occurs.
    
    Rules:
    - Never cut between assistant and its preceding user message
    - Always cut at a user message boundary (start of a turn)
    - If all messages fit in keep_recent_tokens, return 0 (no compaction needed)
    """
    accumulated = 0
    for i in range(len(messages) - 1, -1, -1):
        accumulated += self._estimate_tokens(messages[i].get("content", ""))
        if accumulated >= keep_recent_tokens:
            # Snap forward to next user message (start of a turn)
            for j in range(i, len(messages)):
                if messages[j].get("role") == "user":
                    return j
            return i  # Fallback: cut here
    return 0
```

### Structured Checkpoint Format

NOT free-form prose. Structured format that preserves key context across compaction:

```python
CHECKPOINT_SYSTEM_PROMPT = """You are a conversation summarizer. Read the conversation below and produce a structured summary. Follow the EXACT format. Do NOT continue the conversation. Output ONLY the summary.

## Format

## Goal
[What the user is trying to accomplish — 1-2 sentences]

## Constraints & Preferences  
- [Requirements, preferences, technical constraints mentioned]

## Progress
### Done
- [x] [Completed items with key details]

### In Progress
- [ ] [Current work items]

## Key Decisions
- **[Decision]**: [Brief rationale — what was chosen and why]

## Next Steps
1. [Ordered list of what comes next]

## Critical Context
- [File paths, error messages, variable names, API endpoints — anything needed to continue work]
"""
```

### Iterative Update (When Previous Checkpoint Exists)

When compacting a conversation that already has a summary (second+ compaction), use an UPDATE prompt that merges new info into the existing summary rather than re-summarizing everything:

```python
UPDATE_SYSTEM_PROMPT = """You are updating a conversation summary. The existing summary and NEW conversation messages are provided below.

RULES:
1. PRESERVE all existing information unless explicitly superseded
2. ADD new progress, decisions, and context from the new messages
3. MOVE items from "In Progress" to "Done" when completed
4. UPDATE "Next Steps" based on what was accomplished
5. PRESERVE exact file paths, function names, error messages
6. Use the SAME format as the existing summary

Output ONLY the updated summary."""
```

Usage:
```python
if conversation.summary:
    # Iterative update: merge new messages into existing summary
    summary_messages = [
        {"role": "user", "content": (
            f"## Existing Summary\n\n{conversation.summary}\n\n"
            f"## New Conversation\n\n{self._serialize_for_summary(old_messages)}"
        )}
    ]
    system = UPDATE_SYSTEM_PROMPT
else:
    # First compaction: create new summary
    summary_messages = [
        {"role": "user", "content": self._serialize_for_summary(old_messages)}
    ]
    system = CHECKPOINT_SYSTEM_PROMPT
```

### Summarization Call

Use the existing `_call_api()` with model override — NOT a separate httpx client. This ensures retry logic, auth, thinking config, etc. are all inherited.

```python
# Add model_override to _build_api_payload and _call_api
def _build_api_payload(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
    stream: bool = False,
    skip_thinking: bool = False,
    model_override: str | None = None,  # NEW
) -> dict[str, Any]:
    payload = {
        "model": model_override or self._settings.model,
        ...
    }
```

Summarization call:
```python
summary_response = await self._call_api(
    system_prompt=system,
    messages=summary_messages,
    tools=None,
    skip_thinking=True,
    model_override=self._settings.background_model,
)
checkpoint_text = self._extract_text(summary_response.content)
```

### Conversation Serialization

Convert messages to readable text for the summarizer:

```python
def _serialize_for_summary(self, messages: list[dict[str, Any]]) -> str:
    """Serialize messages as readable text for summarization."""
    lines = []
    for msg in messages:
        role = "User" if msg.get("role") == "user" else "Assistant"
        content = msg.get("content", "")
        if isinstance(content, list):
            # Tool results — serialize as text
            parts = []
            for item in content:
                if isinstance(item, dict):
                    parts.append(item.get("content", item.get("text", str(item))))
            content = "\n".join(parts)
        lines.append(f"**{role}:** {content}")
    return "\n\n".join(lines)
```

### Message Rebuild After Compaction

```python
# Build synthetic summary pair preserving user/assistant alternation
compacted_prefix = [
    {"role": "user", "content": f"[Previous conversation summary]\n\n{checkpoint_text}"},
    {"role": "assistant", "content": "I have the context from our previous conversation. Let's continue."},
]

# Recent window must start with user message
recent = messages[cut_point:]
if recent and recent[0].get("role") != "user":
    # Walk forward to next user message
    for i, msg in enumerate(recent):
        if msg.get("role") == "user":
            recent = recent[i:]
            break

# Update conversation state
conversation.summary = checkpoint_text
conversation.messages = (
    [Message(role="user", content=compacted_prefix[0]["content"]),
     Message(role="assistant", content=compacted_prefix[1]["content"])]
    + [Message(role=m.role, content=m.content) 
       for m in conversation.messages[cut_point_msg_index:]]
)
conversation.compaction_count += 1

# Return formatted for immediate API use
return compacted_prefix + recent
```

### Summarization Failure Fallback

If the summarization call fails (timeout, API error), fall back to simple truncation:

```python
try:
    checkpoint_text = await self._summarize(old_messages, conversation.summary)
except Exception as e:
    logger.error("Compaction summarization failed: %s — falling back to truncation", e)
    # Keep recent messages only, no summary
    return messages[cut_point:]
```

### Pre-Compaction Event

Before summarizing, emit an event so cognitive handlers can extract durable knowledge:

```python
await self._bus.emit(Event(
    type="conversation_compacting",
    data={
        "session_id": conversation.session_id,
        "messages_being_compacted": len(old_messages),
        "compaction_number": conversation.compaction_count + 1,
    }
))
```

Handlers (Phase 3) can use this to extract decisions, facts, or episode boundaries before context is compressed.

---

## Data Model Changes

### Conversation Dataclass

```python
@dataclass
class Conversation:
    """Tracks a multi-turn conversation."""
    session_id: str
    messages: list[Message] = field(default_factory=list)
    turn_contexts: list[TurnContext] = field(default_factory=list)
    summary: str | None = None          # NEW: latest checkpoint summary
    compaction_count: int = 0           # NEW: number of compactions performed
```

### turn_contexts Cleanup

`turn_contexts` also grows unbounded. Add cleanup alongside compaction:

```python
# In _compact_conversation, after rebuilding messages:
# Keep only turn_contexts for messages in the recent window
if len(conversation.turn_contexts) > len(recent_messages):
    conversation.turn_contexts = conversation.turn_contexts[-(len(recent_messages) // 2):]
```

### _format_history_text Update

Used for reflection (post-turn). Must also respect compaction:

```python
def _format_history_text(self, conversation: Conversation) -> str:
    """Format conversation history as readable text for reflection."""
    lines = []
    if conversation.summary:
        lines.append(f"[Previous context summary]\n{conversation.summary}\n")
    for msg in conversation.messages[-20:]:  # Still cap reflection context
        role_label = "User" if msg.role == "user" else "Assistant"
        lines.append(f"{role_label}: {msg.content}")
    return "\n\n".join(lines)
```

---

## Configuration Summary

```python
# config.py additions — all with NOUS_ prefixed env var aliases

# Layer 1: Tool Pruning
tool_pruning_enabled: bool = True
tool_soft_trim_chars: int = 4000        # Soft-trim results larger than this
tool_soft_trim_head: int = 1500         # Keep first N chars when trimming
tool_soft_trim_tail: int = 1500         # Keep last N chars when trimming  
tool_hard_clear_after: int = 6          # Hard-clear results older than N tool-result messages
keep_last_tool_results: int = 2         # Never prune last N tool-result messages

# Layer 2: History Compaction
compaction_enabled: bool = False        # Opt-in initially (feature flag)
compaction_threshold: int = 100_000     # Trigger when estimated input tokens exceed this
keep_recent_tokens: int = 20_000        # How much recent history to preserve
```

---

## Implementation Plan

### Phase 1: Tool Output Pruning + Token Estimation

**Files:** `runner.py`, `config.py`
**Risk:** Low — only affects in-turn tool results, doesn't change stored conversation
**Estimate:** ~200 lines

1. Add config fields to `NousSettings` with env var aliases
2. Add `_estimate_tokens(content) -> int` (chars/4)
3. Add `_is_tool_result_message(msg) -> bool`
4. Add `_prune_tool_results(messages) -> None` (mutates in place)
5. Hook into `_tool_loop()` after tool results appended (before next `_call_api`)
6. Hook into `stream_chat()` after tool results appended (before next `_call_api_stream`)
7. Add structured logging: `logger.info("Pruned %d tool results (soft: %d, hard: %d)")`

### Phase 2: History Compaction Core

**Files:** `runner.py`, `config.py`
**Depends on:** Phase 1 (token estimation)
**Risk:** Medium — changes conversation flow, needs careful testing
**Estimate:** ~350 lines

1. Add `summary` and `compaction_count` to `Conversation` dataclass
2. Add compaction config fields to `NousSettings`
3. Add `_estimate_messages_tokens(messages) -> int`
4. Add `_should_compact(system_tokens, history_tokens) -> bool`
5. Add `_find_cut_point(messages, keep_recent_tokens) -> int`
6. Add `_serialize_for_summary(messages) -> str`
7. Add `model_override` parameter to `_build_api_payload()` and `_call_api()`
8. Add `_compact_conversation(conversation, messages) -> list[dict]`
9. Rename `_format_messages()` → `_prepare_messages()` (now async, token-aware)
10. Update callers: `stream_chat()` and `_tool_loop()` to `await self._prepare_messages()`
11. Update `_format_history_text()` to include summary
12. Structured checkpoint prompts (initial + update)
13. Fallback on summarization failure (truncate without summary)
14. Clean up `turn_contexts` during compaction
15. Feature flag: `NOUS_COMPACTION_ENABLED=false` (default off)

### Phase 3: Durable Integration

**Files:** `runner.py`, `cognitive/layer.py`, `heart/heart.py`
**Depends on:** Phase 2
**Risk:** Medium — involves DB schema and event bus

1. Pre-compaction event emission (`conversation_compacting`)
2. `CognitiveLayer.on_compaction()` handler — extract facts/decisions from compacted messages
3. Episode boundary: compaction triggers episode segment close
4. Conversation state persistence table:

```sql
CREATE TABLE heart.conversation_state (
    agent_id TEXT NOT NULL,
    session_id TEXT NOT NULL,
    summary TEXT,
    turn_count INT NOT NULL DEFAULT 0,
    compaction_count INT NOT NULL DEFAULT 0,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    UNIQUE(agent_id, session_id)
);
```

5. Save/restore summary on session resume (post container restart)

### Phase 4: Adaptive & Advanced (Future)

1. Dynamic `keep_recent_tokens` based on conversation pattern
2. Priority-based retention (keep decision-containing messages longer)
3. Tool call tracking (which tools used, files modified — append to checkpoint)
4. Usage-based token counting (use actual `input_tokens` from API response as calibration for estimation)

---

## Testing Strategy

### Phase 1 Tests
- `test_is_tool_result_message` — correctly identifies tool result vs regular user messages
- `test_soft_trim_large_result` — verify head+tail+marker format
- `test_soft_trim_preserves_small` — results under threshold untouched
- `test_hard_clear_old_results` — placeholder replacement for old results
- `test_protect_last_n_results` — recency protection zone
- `test_never_modify_assistant_blocks` — assistant thinking/text/tool_use blocks untouched
- `test_never_modify_user_text` — regular user text messages untouched
- `test_skip_image_results` — image content preservation
- `test_empty_messages_noop` — no crash on empty list
- `test_all_protected_noop` — when all results are in protection zone

### Phase 2 Tests
- `test_should_compact_under_threshold` — no compaction when under budget
- `test_should_compact_over_threshold` — triggers compaction
- `test_find_cut_point_keeps_recent` — correct token-based split
- `test_cut_at_user_boundary` — never cut mid-turn (between user and assistant)
- `test_alternation_preserved` — user/assistant alternation after compaction
- `test_alternation_leading_assistant` — recency window starting with assistant snaps forward
- `test_iterative_update_prompt` — second compaction uses UPDATE prompt with existing summary
- `test_summarization_failure_fallback` — graceful truncation on API error
- `test_conversation_state_updated` — summary and compaction_count set after compaction
- `test_turn_contexts_cleaned` — old turn_contexts removed during compaction
- `test_format_history_text_with_summary` — reflection includes summary prefix
- `test_prepare_messages_async` — renamed method works correctly

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Tool result trimming hides needed data | Medium | Medium | Trim marker tells Claude to re-request; protect last 2 results |
| Summary loses critical context | Medium | High | Structured format; Heart/Brain extraction; iterative update |
| Summarization latency blocks response | Low | Medium | Timeout + fallback to truncation; runs rarely |
| Alternation violation after compaction | High (if untested) | High | Explicit boundary snapping + dedicated tests |
| Token estimation inaccuracy | Medium | Low | 20K safety margin absorbs error; conservative chars/4 |
| Summary model API failure | Low | Medium | Fallback: truncate without summary |
| `_format_messages` → async breaks callers | Low | Medium | Both callers already async; rename signals change |

---

## Key Differences from v2

| Aspect | v2 | v3 |
|--------|-----|-----|
| Tool result identification | Not specified | Explicit `_is_tool_result_message` with list-of-dicts check |
| Hard-clear trigger | Ratio of context window | Message age (simpler, predictable) |
| Integration point (Layer 2) | Vague "in `_format_messages`" | Explicit: rename to `_prepare_messages`, make async, single gate |
| Token estimation | chars/4 only | chars/4 + future calibration from actual API usage |
| `turn_contexts` cleanup | Not mentioned | Cleaned during compaction |
| `_format_history_text` | Not mentioned | Updated to include summary for reflection |
| Thinking blocks | Not mentioned | Explicitly preserved (never modified in assistant content) |
| Protection zone size | 3 results | 2 results (sufficient, saves more context) |
| Compaction threshold | Derived from reserve | Simple 100K threshold (easier to reason about) |

## References

- Claude Code SDK `compaction.js` — structured checkpoint format, iterative update, token-based cut points
- pi-coding-agent `compaction.js` — split-turn handling, file operation tracking, UPDATE prompt pattern
- OpenClaw session pruning — two-layer approach, soft-trim/hard-clear, TTL-based tool result management
- Nous codebase: `runner.py` lines 564-810 (stream_chat), 811-930 (_tool_loop), 1082-1100 (_format_messages)
