# Spec 008.1: Conversation Compaction (v4)

**Status:** Draft v4 (revised from 3-agent review of v3)
**Depends on:** 008 (Tiered Context Model)
**Issue:** #68

## Problem

Spec 008 manages the system prompt well (Tier 1 always-on, Tier 3 semantic search with budgets), but **conversation history has zero token management**. Currently:

- `MAX_HISTORY_MESSAGES = 20` — hard cutoff by message count, not tokens
- Tool outputs within a turn are local variables — never stored in `conversation.messages`
- No token counting before API calls
- No truncation of tool outputs during tool loops
- Conversation is in-memory only — lost on container restart

### What Actually Causes Token Bloat

**Within a turn (in-turn):** Tool loops accumulate tool_use + tool_result messages in the local `messages` list passed to API calls. A 10-iteration bash loop can generate 30K+ tokens. These are rebuilt each turn from `conversation.messages` + current tool results. **Compaction cannot touch these — only pruning can.**

**Across turns (history):** `conversation.messages` stores plain text user/assistant pairs (tool outputs are NOT stored). With `MAX_HISTORY_MESSAGES = 20` and ~300 tokens per Telegram message, history is typically ~6K tokens. **The 20-message cap means history rarely exceeds 10K tokens today.**

### Why Both Layers Matter

Layer 1 (tool pruning) addresses the immediate bloat problem — tool results that accumulate within a single turn. This is where context window overflows actually happen today.

Layer 2 (history compaction) becomes essential once the 20-message cap is removed. Without it, long conversations will eventually exceed the context window. It also enables persistent conversation state across container restarts (Phase 3).

---

## Architecture

### Two-Layer Approach

```
Layer 1: Tool Output Pruning (per-request, cheap, no LLM)
  ├── Runs before every API call during tool loops
  ├── Soft-trim old tool results (head + tail + marker)
  ├── Hard-clear very old tool results (→ placeholder)
  └── Only touches tool_result content, never user/assistant text

Layer 2: History Compaction (rare, LLM-powered)
  ├── Triggered when estimated input tokens exceed budget
  ├── Pre-compaction event via CognitiveLayer (save to Heart/Brain)
  ├── Structured checkpoint summary with conversation dynamics
  ├── Iterative update with size cap (merge with previous summary)
  └── Persisted in Conversation state
```

### Module Structure

Compaction logic lives in a **separate module** — `runner.py` (1098 lines) stays focused on orchestration.

```
nous/api/
  ├── runner.py          — orchestration, API calls, tool loops
  ├── compaction.py      — NEW: ConversationCompactor class
  └── ...
```

`ConversationCompactor` owns all pruning, estimation, and compaction logic. `AgentRunner` holds a `self._compactor` instance and calls it at the right points.

### Token Budget Model (Sonnet 4.6, 200K context)

| Component | Budget | Notes |
|-----------|--------|-------|
| System prompt (Tier 1 + 3) | ~5K | Identity, user profile, censors, recalled context |
| Conversation summary | ~4K | Structured checkpoint (when compaction active) |
| Recent messages | ~20K | Token-based window, not message count |
| In-turn tool results | ~30K | Managed by Layer 1 pruning |
| Thinking tokens | variable | Not in input, but model allocates from output budget |
| Output headroom | ~16K | `max_tokens` setting |
| Safety margin | ~20K | Buffer for estimation error |

---

## Layer 1: Tool Output Pruning

### Design

Lives in `ConversationCompactor`. Runs inside the tool loop, BEFORE each subsequent API call. Trims the accumulated `tool_result` content in the local `messages` list.

### Identifying Tool Result Messages

A user message is a tool result message when its `content` is a list of dicts with `type: "tool_result"`. Regular user messages have string content. This matches exactly how tool results are constructed in both `stream_chat()` (lines 755-756) and `_tool_loop()` (lines 880-881).

```python
@staticmethod
def is_tool_result_message(msg: dict[str, Any]) -> bool:
    """Check if a message contains tool results (not regular user text)."""
    content = msg.get("content")
    return (
        msg.get("role") == "user"
        and isinstance(content, list)
        and len(content) > 0
        and isinstance(content[0], dict)
        and content[0].get("type") == "tool_result"
    )
```

### Pruning Algorithm

```python
def prune_tool_results(self, messages: list[dict[str, Any]]) -> None:
    """Prune old tool results from in-turn message accumulation. Mutates in place.
    
    Two-phase approach:
    1. Soft-trim: Keep head + tail of oversized results (> tool_soft_trim_chars)
    2. Hard-clear: Replace very old results with placeholder
    
    Never modifies user text or assistant content blocks.
    Protected zone: last keep_last_tool_results tool-result messages.
    """
```

**Protection zone:** The last `keep_last_tool_results` tool result messages are never touched. The model just generated tool calls based on these results — pruning them would confuse it.

**Soft-trim:** For tool results outside the protection zone with `len(content_str) > tool_soft_trim_chars`:

```
[First 1500 chars of output]

--- trimmed (kept 1500 head + 1500 tail of 24000 chars) ---

[Last 1500 chars of output]
```

**Hard-clear:** For tool results more than `tool_hard_clear_after` tool-result messages old (counting from the end), replace content entirely:

```
[Tool output cleared — content was processed in earlier turns]
```

### Key Rules

1. Only modify `tool_result` content (never assistant blocks, never user text messages)
2. Protect last `keep_last_tool_results` tool-result messages
3. Skip results containing image content (check for `type: "image"` blocks in content)
4. Append trim marker so Claude knows to re-request if needed
5. Mutates in-place for efficiency (no copy of the full messages list)

### Integration Points in `runner.py`

**In `_tool_loop()` (after line ~881):** After tool results appended, before next `_call_api()`:

```python
# After: messages.append({"role": "user", "content": tool_results_for_message})
if self._compactor:
    self._compactor.prune_tool_results(messages)
# Before: api_response = await self._call_api(...)
```

**In `stream_chat()` (after line ~775):** Same pattern:

```python
# After: messages.append({"role": "user", "content": tool_results_for_message})
if self._compactor:
    self._compactor.prune_tool_results(messages)
# Before: async for event in self._call_api_stream(...)
```

---

## Layer 2: History Compaction

### Prerequisites

Remove `MAX_HISTORY_MESSAGES = 20` and replace with token-based management. This is what makes compaction necessary.

### Token Estimation

Use actual API usage (from previous turn's response) for calibration, chars/4 as initial estimate:

```python
class TokenEstimator:
    """Estimates token counts with optional calibration from actual API usage."""
    
    def __init__(self) -> None:
        self._calibration_ratio: float = 0.25  # chars per token (starts at 1/4)
        self._samples: int = 0
    
    def estimate(self, text: str | Any) -> int:
        """Estimate token count for text content."""
        if isinstance(text, str):
            return max(1, int(len(text) * self._calibration_ratio))
        # For list content (tool results), serialize first
        return max(1, int(len(str(text)) * self._calibration_ratio))
    
    def estimate_messages(self, messages: list[dict[str, Any]]) -> int:
        """Estimate total tokens for a message list."""
        total = 0
        for msg in messages:
            total += self.estimate(msg.get("content", ""))
            total += 4  # role + structural overhead per message
        return total
    
    def calibrate(self, estimated_chars: int, actual_tokens: int) -> None:
        """Update calibration ratio from actual API usage data.
        
        Called after each API response using input_tokens from usage.
        Exponential moving average to smooth out variance.
        """
        if estimated_chars <= 0 or actual_tokens <= 0:
            return
        observed_ratio = actual_tokens / estimated_chars
        alpha = 0.1  # Smoothing factor
        self._calibration_ratio = (
            alpha * observed_ratio + (1 - alpha) * self._calibration_ratio
        )
        self._samples += 1
```

**Calibration hook in `runner.py`:** After each `_call_api()` / `_call_api_stream()` response, feed the actual `input_tokens` back to the estimator. This is near-zero cost and improves accuracy over the session lifetime.

### Trigger

Checked explicitly in `stream_chat()` and `_tool_loop()` before calling `_format_messages()` — NOT hidden inside formatting.

```python
# In ConversationCompactor:
def should_compact(self, system_tokens: int, history_tokens: int) -> bool:
    """Check if compaction is needed before a turn."""
    total = system_tokens + history_tokens
    return total > self._settings.compaction_threshold
```

### Integration in `runner.py` (Explicit, Not Hidden)

Compaction is triggered **explicitly** at the orchestration level, not hidden inside message formatting. This makes the potentially expensive LLM call visible:

```python
# In stream_chat() and _tool_loop(), before first API call:
messages = self._format_messages(conversation)  # stays sync, pure

if self._compactor and self._compactor.should_compact(
    self._estimator.estimate(system_prompt),
    self._estimator.estimate_messages(messages),
):
    async with self._compaction_lock(session_id):
        # Re-check under lock (double-check pattern)
        messages = self._format_messages(conversation)
        if self._compactor.should_compact(...):
            await self._compactor.compact(
                conversation, messages, system_prompt,
                call_api=self._call_api,  # pass API caller, don't import httpx
                agent_id=_agent_id,
            )
            messages = self._format_messages(conversation)  # re-format after compaction
```

`_format_messages()` stays **sync and pure** — just formats `Conversation` → `list[dict]`.

### Concurrency: Per-Session Lock

Two concurrent requests for the same session could both trigger compaction. A per-session `asyncio.Lock` with double-check pattern prevents this:

```python
# In AgentRunner:
def __init__(self, ...):
    self._compaction_locks: dict[str, asyncio.Lock] = {}

def _compaction_lock(self, session_id: str) -> asyncio.Lock:
    if session_id not in self._compaction_locks:
        self._compaction_locks[session_id] = asyncio.Lock()
    return self._compaction_locks[session_id]
```

### Compaction Algorithm

```python
async def compact(
    self,
    conversation: Conversation,
    messages: list[dict[str, Any]],
    system_prompt: str,
    call_api: Callable,  # AgentRunner._call_api passed in
    agent_id: str,
) -> None:
    """Compact conversation history via structured summarization.
    
    1. Find cut point (keep recent ~20K tokens)
    2. Guard: if cut_point == 0, nothing to compact — return
    3. Emit pre-compaction event via cognitive layer
    4. Summarize old messages into structured checkpoint
    5. Rebuild conversation: [checkpoint pair] + [recent messages]
    """
```

### Cut Point Selection

Token-based backwards walk from newest messages:

```python
def find_cut_point(self, messages: list[dict[str, Any]], keep_recent_tokens: int) -> int:
    """Walk backwards accumulating tokens. Return index where old/recent split.
    
    Returns 0 if all messages fit in keep_recent_tokens (no compaction needed).
    
    Rules:
    - Never cut between assistant and its preceding user message
    - Always snap to a user message boundary (start of a turn)
    """
    accumulated = 0
    for i in range(len(messages) - 1, -1, -1):
        accumulated += self._estimator.estimate(messages[i].get("content", ""))
        if accumulated >= keep_recent_tokens:
            # Snap forward to next user message (start of a turn)
            for j in range(i, len(messages)):
                if messages[j].get("role") == "user":
                    return j
            # No user message found after i — keep everything
            return 0
    return 0  # Everything fits
```

**Guard in `compact()`:** If `cut_point == 0`, return immediately — nothing to compact.

```python
cut_point = self.find_cut_point(messages, self._settings.keep_recent_tokens)
if cut_point == 0:
    logger.debug("Compaction triggered but cut_point=0 — nothing to compact")
    return
```

### Structured Checkpoint Format

Structured format with a **Conversation Dynamics** section to preserve relational/tonal context that pure task-oriented formats lose:

```python
CHECKPOINT_SYSTEM_PROMPT = """You are a conversation summarizer. Read the conversation below and produce a structured summary. Follow the EXACT format. Do NOT continue the conversation. Output ONLY the summary.

TARGET LENGTH: 800-1200 words (roughly 3-4K tokens). Prioritize precision over completeness.

## Format

## Goal
[What the user is trying to accomplish — 1-2 sentences]

## Constraints & Preferences  
- [Requirements, preferences, technical constraints mentioned]

## Progress
### Done
- [x] [Completed items with key details]

### In Progress
- [ ] [Current work items]

## Key Decisions
- **[Decision]**: [Brief rationale — what was chosen and why]

## Conversation Dynamics
- [User tone, frustration points, behavioral preferences expressed]
- [Rapport markers, recurring jokes/references, communication style]
- [Unresolved questions the user expressed uncertainty about]
- [Any explicit instructions about assistant behavior]

## Next Steps
1. [Ordered list of what comes next]

## Critical Context
- [File paths, error messages, variable names, API endpoints — anything needed to continue work]
"""
```

### Iterative Update with Size Cap

When a previous checkpoint exists, use an UPDATE prompt that merges new info. Enforce a target size to prevent unbounded growth:

```python
UPDATE_SYSTEM_PROMPT = """You are updating a conversation summary. The existing summary and NEW conversation messages are provided below.

TARGET LENGTH: 800-1200 words. If the summary would exceed this, prioritize:
1. Recent progress and decisions (most important)
2. Critical context (file paths, errors, API details)
3. Active constraints and preferences
4. Conversation dynamics updates
Drop older completed items from "Done" if needed to stay within target.

RULES:
1. PRESERVE all existing information unless explicitly superseded or space-constrained
2. ADD new progress, decisions, and context from the new messages
3. MOVE items from "In Progress" to "Done" when completed
4. UPDATE "Next Steps" based on what was accomplished
5. UPDATE "Conversation Dynamics" with any new tone/preference signals
6. PRESERVE exact file paths, function names, error messages
7. Use the SAME format as the existing summary

Output ONLY the updated summary."""
```

Usage:
```python
if conversation.summary:
    # Iterative update: merge new messages into existing summary
    user_content = (
        f"## Existing Summary\n\n{conversation.summary}\n\n"
        f"## New Conversation\n\n{self._serialize_for_summary(old_messages)}"
    )
else:
    # First compaction: create new summary
    user_content = self._serialize_for_summary(old_messages)

summary_messages = [{"role": "user", "content": user_content}]
system = UPDATE_SYSTEM_PROMPT if conversation.summary else CHECKPOINT_SYSTEM_PROMPT
```

### Summarization Call

Uses `AgentRunner._call_api()` passed as a callable — NOT a separate httpx client. This inherits retry logic, auth, and thinking config:

```python
# AgentRunner._call_api signature addition:
async def _call_api(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
    skip_thinking: bool = False,
    model_override: str | None = None,  # NEW
) -> ApiResponse:
    ...
    payload = self._build_api_payload(
        system_prompt, messages, tools,
        skip_thinking=skip_thinking,
        model_override=model_override,  # forwarded
    )

# _build_api_payload addition:
def _build_api_payload(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
    stream: bool = False,
    skip_thinking: bool = False,
    model_override: str | None = None,  # NEW
) -> dict[str, Any]:
    payload = {
        "model": model_override or self._settings.model,
        ...
    }
```

Compactor calls it via the passed callable:
```python
summary_response = await call_api(
    system_prompt=system,
    messages=summary_messages,
    tools=None,
    skip_thinking=True,
    model_override=self._settings.background_model,
)
checkpoint_text = self._extract_text(summary_response.content)
```

**Note:** `_call_api_stream` does NOT need `model_override` — streaming is only for primary turns.

### Summary Validation

Basic sanity check before accepting the summary (prevents bad summaries from becoming the only record):

```python
def _validate_summary(self, summary: str) -> bool:
    """Basic sanity check on generated summary."""
    if len(summary) < 200:
        logger.warning("Summary too short (%d chars) — rejecting", len(summary))
        return False
    if len(summary) > 8000:
        logger.warning("Summary too long (%d chars) — truncating", len(summary))
        # Don't reject, just warn — better than losing context
    # Check for at least 2 expected sections
    sections_found = sum(1 for s in ["## Goal", "## Progress", "## Critical Context"]
                        if s in summary)
    if sections_found < 2:
        logger.warning("Summary missing expected sections (%d/3) — rejecting", sections_found)
        return False
    return True
```

### Conversation Serialization

Convert messages to readable text for the summarizer. Only operates on `conversation.messages` which stores plain text content:

```python
def _serialize_for_summary(self, messages: list[dict[str, Any]]) -> str:
    """Serialize messages as readable text for summarization.
    
    Note: conversation.messages stores plain text only (tool results are
    in-turn locals, never persisted). So content is always str here.
    """
    lines = []
    for msg in messages:
        role = "User" if msg.get("role") == "user" else "Assistant"
        content = msg.get("content", "")
        if isinstance(content, str):
            lines.append(f"**{role}:** {content}")
        elif isinstance(content, list):
            # Defensive: handle list content if it ever appears
            parts = []
            for item in content:
                if isinstance(item, dict):
                    # Try common keys in order
                    text = (item.get("content")
                            or item.get("text")
                            or item.get("thinking")
                            or str(item))
                    parts.append(text)
                else:
                    parts.append(str(item))
            lines.append(f"**{role}:** {chr(10).join(parts)}")
    return "\n\n".join(lines)
```

### Message Rebuild After Compaction

```python
# Build synthetic summary pair preserving user/assistant alternation
compacted_prefix = [
    Message(role="user", content=f"[Previous conversation summary]\n\n{checkpoint_text}"),
    Message(role="assistant", content="I have the context from our previous conversation. Let's continue."),
]

# Recent window must start with user message
recent_msgs = conversation.messages[cut_point:]
if recent_msgs and recent_msgs[0].role != "user":
    # Walk forward to next user message
    found = False
    for i, msg in enumerate(recent_msgs):
        if msg.role == "user":
            recent_msgs = recent_msgs[i:]
            found = True
            break
    if not found:
        # No user message in recent window — keep empty (summary is enough)
        recent_msgs = []

# Update conversation state
conversation.summary = checkpoint_text
conversation.messages = compacted_prefix + recent_msgs
conversation.compaction_count += 1

# Clean up turn_contexts (keep only contexts for recent messages)
# Each turn = ~2 messages (user + assistant), so keep len(recent_msgs) // 2
keep_contexts = max(1, len(recent_msgs) // 2)
if len(conversation.turn_contexts) > keep_contexts:
    conversation.turn_contexts = conversation.turn_contexts[-keep_contexts:]
```

### Summarization Failure Fallback

If the summarization call fails or validation rejects the result, fall back to simple truncation:

```python
try:
    checkpoint_text = await self._summarize(old_messages, conversation.summary, call_api)
    if not self._validate_summary(checkpoint_text):
        raise ValueError("Summary failed validation")
except Exception as e:
    logger.error("Compaction summarization failed: %s — falling back to truncation", e)
    # Keep recent messages only, no summary
    conversation.messages = conversation.messages[cut_point:]
    return
```

### Pre-Compaction Event

Emitted through `CognitiveLayer` (which owns the event bus), NOT through `AgentRunner`:

```python
# In AgentRunner, before calling compactor.compact():
await self._cognitive.on_pre_compaction(
    agent_id=_agent_id,
    session_id=session_id,
    messages_being_compacted=cut_point,
)

# In CognitiveLayer:
async def on_pre_compaction(
    self, agent_id: str, session_id: str, messages_being_compacted: int
) -> None:
    """Emit pre-compaction event for knowledge extraction handlers."""
    await self._bus.emit(Event(
        type="conversation_compacting",
        agent_id=agent_id,
        session_id=session_id,
        data={"messages_being_compacted": messages_being_compacted},
    ))
```

Phase 3 handlers can subscribe to `conversation_compacting` to extract durable knowledge from the messages about to be summarized. The event is `await`ed, so handlers complete before compaction proceeds.

---

## Data Model Changes

### Conversation Dataclass

```python
@dataclass
class Conversation:
    """Tracks a multi-turn conversation."""
    session_id: str
    messages: list[Message] = field(default_factory=list)
    turn_contexts: list[TurnContext] = field(default_factory=list)
    summary: str | None = None          # NEW: latest checkpoint summary
    compaction_count: int = 0           # NEW: number of compactions performed
```

### `_format_messages` (Unchanged — Stays Sync and Pure)

```python
def _format_messages(self, conversation: Conversation) -> list[dict[str, Any]]:
    """Format conversation history for API calls. Pure, sync, no side effects."""
    if not self._settings.compaction_enabled:
        # Legacy behavior: message-count cap
        return [{"role": m.role, "content": m.content}
                for m in conversation.messages[-MAX_HISTORY_MESSAGES:]]
    # With compaction enabled, return all messages (compaction manages size)
    return [{"role": m.role, "content": m.content} for m in conversation.messages]
```

### `_format_history_text` (Summary-Aware)

Used for reflection (post-turn). Includes summary prefix:

```python
def _format_history_text(self, conversation: Conversation) -> str:
    """Format conversation history as readable text for reflection."""
    lines = []
    if conversation.summary:
        lines.append(f"[Previous context summary]\n{conversation.summary}\n")
    for msg in conversation.messages[-20:]:  # Still cap reflection context
        role_label = "User" if msg.role == "user" else "Assistant"
        lines.append(f"{role_label}: {msg.content}")
    return "\n\n".join(lines)
```

---

## Configuration

```python
# config.py additions — all with NOUS_ prefixed env var aliases

# Layer 1: Tool Pruning
tool_pruning_enabled: bool = Field(default=True, validation_alias="NOUS_TOOL_PRUNING_ENABLED")
tool_soft_trim_chars: int = Field(default=4000, validation_alias="NOUS_TOOL_SOFT_TRIM_CHARS")
tool_soft_trim_head: int = Field(default=1500, validation_alias="NOUS_TOOL_SOFT_TRIM_HEAD")
tool_soft_trim_tail: int = Field(default=1500, validation_alias="NOUS_TOOL_SOFT_TRIM_TAIL")
tool_hard_clear_after: int = Field(default=6, validation_alias="NOUS_TOOL_HARD_CLEAR_AFTER")
keep_last_tool_results: int = Field(default=2, validation_alias="NOUS_KEEP_LAST_TOOL_RESULTS")

# Layer 2: History Compaction
compaction_enabled: bool = Field(default=False, validation_alias="NOUS_COMPACTION_ENABLED")
compaction_threshold: int = Field(default=100_000, validation_alias="NOUS_COMPACTION_THRESHOLD")
keep_recent_tokens: int = Field(default=20_000, validation_alias="NOUS_KEEP_RECENT_TOKENS")
summary_max_chars: int = Field(default=6000, validation_alias="NOUS_SUMMARY_MAX_CHARS")
```

All thresholds use config fields (not module constants) so they're tunable via env vars.

---

## Implementation Plan

### Phase 1: Tool Output Pruning + Token Estimation

**New file:** `nous/api/compaction.py`
**Modified:** `runner.py`, `config.py`
**Risk:** Low — only affects in-turn tool results, doesn't change stored conversation
**Estimate:** ~250 lines (compaction.py) + ~20 lines (runner.py hooks)

1. Create `nous/api/compaction.py` with `ConversationCompactor` class
2. Add `TokenEstimator` class with calibration support
3. Add config fields to `NousSettings` with env var aliases
4. Implement `is_tool_result_message()`, `prune_tool_results()`
5. Hook into `_tool_loop()` and `stream_chat()` after tool results appended
6. Add calibration hook: after `_call_api()` responses, call `estimator.calibrate()`
7. Structured logging: `logger.info("Pruned %d tool results (soft: %d, hard: %d)")`
8. Tests for all pruning scenarios

### Phase 2: History Compaction Core

**Modified:** `compaction.py`, `runner.py`, `config.py`
**Depends on:** Phase 1 (token estimation + compactor class)
**Risk:** Medium — changes conversation flow
**Estimate:** ~300 lines (compaction.py additions) + ~40 lines (runner.py)

1. Add `summary`, `compaction_count` to `Conversation` dataclass
2. Add compaction config fields to `NousSettings`
3. Add `model_override` parameter to `_build_api_payload()` and `_call_api()`
4. Add `should_compact()`, `find_cut_point()`, `compact()` to `ConversationCompactor`
5. Add `_serialize_for_summary()`, `_validate_summary()` to `ConversationCompactor`
6. Add checkpoint prompts as module constants in `compaction.py`
7. Add per-session `asyncio.Lock` for compaction in `AgentRunner`
8. Trigger compaction explicitly in `stream_chat()` and `_tool_loop()` before first API call
9. Update `_format_messages()` to return all messages when compaction enabled
10. Update `_format_history_text()` to include summary prefix
11. Fallback on summarization failure (truncate without summary)
12. Clean up `turn_contexts` during compaction
13. Feature flag: `NOUS_COMPACTION_ENABLED=false` (default off)
14. Tests for all compaction scenarios including edge cases

### Phase 3: Durable Integration

**Modified:** `compaction.py`, `cognitive/layer.py`, `heart/heart.py`
**Depends on:** Phase 2

1. Add `on_pre_compaction()` to `CognitiveLayer` — emits `conversation_compacting` event
2. Wire `AgentRunner` → `CognitiveLayer.on_pre_compaction()` before compaction
3. Knowledge extraction handler: extract facts/decisions from compacted messages
4. Episode boundary: compaction triggers episode segment close
5. Conversation state persistence:

```sql
CREATE TABLE heart.conversation_state (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_id TEXT NOT NULL,
    session_id TEXT NOT NULL,
    summary TEXT,
    messages JSONB,
    turn_count INT NOT NULL DEFAULT 0,
    compaction_count INT NOT NULL DEFAULT 0,
    created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
    UNIQUE(agent_id, session_id)
);
```

6. Save/restore conversation state on session create/resume

### Phase 4: Adaptive & Advanced (Future)

1. Dynamic `compaction_threshold` based on conversation pattern
2. Priority-based retention (keep decision-containing messages longer)
3. Tool call tracking (which tools used, files modified — append to checkpoint)
4. Versioned summaries (keep last 2 for diffing / drift detection)

---

## Testing Strategy

### Phase 1 Tests
- `test_is_tool_result_message_positive` — list of dicts with type: tool_result
- `test_is_tool_result_message_negative` — string content, empty list, wrong type
- `test_soft_trim_large_result` — verify head+tail+marker format
- `test_soft_trim_preserves_small` — results under threshold untouched
- `test_hard_clear_old_results` — placeholder replacement for old results
- `test_protect_last_n_results` — recency protection zone
- `test_never_modify_assistant_blocks` — assistant thinking/text/tool_use blocks untouched
- `test_never_modify_user_text` — regular user text messages untouched
- `test_skip_image_results` — image content preservation
- `test_empty_messages_noop` — no crash on empty list
- `test_all_protected_noop` — when all results are in protection zone
- `test_token_estimator_calibration` — ratio improves after calibrate() calls
- `test_token_estimator_initial` — chars/4 default behavior

### Phase 2 Tests
- `test_should_compact_under_threshold` — no compaction when under budget
- `test_should_compact_over_threshold` — triggers compaction
- `test_find_cut_point_keeps_recent` — correct token-based split
- `test_find_cut_point_returns_zero_when_fits` — no compaction when all fits
- `test_find_cut_point_snaps_to_user` — never cut mid-turn
- `test_find_cut_point_no_user_after_returns_zero` — graceful fallback
- `test_cut_point_zero_guard` — compact() returns immediately when cut_point=0
- `test_alternation_preserved` — user/assistant alternation after compaction
- `test_alternation_no_user_in_recent` — recent window all assistant → empty recent
- `test_iterative_update_prompt` — second compaction uses UPDATE with existing summary
- `test_summary_validation_too_short` — rejects <200 char summaries
- `test_summary_validation_missing_sections` — rejects missing structure
- `test_summarization_failure_fallback` — graceful truncation on API error
- `test_conversation_state_updated` — summary and compaction_count set
- `test_turn_contexts_cleaned` — old turn_contexts removed
- `test_format_history_text_with_summary` — reflection includes summary prefix
- `test_concurrency_lock` — two concurrent compaction requests don't corrupt state
- `test_config_threshold_used` — env var override works (not module constant)

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Tool result trimming hides needed data | Medium | Medium | Trim marker prompts re-request; protect last 2 |
| Summary loses critical context | Medium | High | Structured format + dynamics section; Heart extraction; iterative update with size cap |
| Summary drift after 5+ compactions | Medium | Medium | Size cap prevents unbounded growth; Phase 4 versioned summaries |
| Summarization latency blocks response | Low | Medium | Timeout + fallback; runs rarely |
| Alternation violation after compaction | Handled | N/A | Explicit boundary snapping + for/else fallback + tests |
| Token estimation inaccuracy | Medium | Low | 20K margin + calibration from actual API usage |
| Summary model produces garbage | Low | Medium | Validation (length + sections check); fallback to truncation |
| Concurrent compaction race | Handled | N/A | Per-session asyncio.Lock with double-check |
| `conversation_state` table design | Low | Low | UUID PK, created_at, messages JSONB — matches Nous conventions |

---

## Key Changes from v3

| Issue | v3 | v4 Fix |
|-------|-----|--------|
| runner.py bloat | All logic in runner.py | Extract `nous/api/compaction.py` with `ConversationCompactor` |
| Hidden LLM call | `_prepare_messages()` hides compaction | Explicit trigger at call sites; `_format_messages` stays sync |
| Concurrency race | Not addressed | Per-session `asyncio.Lock` with double-check pattern |
| `self._bus` on runner | Runner emits events directly | Emit through `CognitiveLayer.on_pre_compaction()` |
| Event missing `agent_id` | `Event(type=..., data=...)` | `Event(type=..., agent_id=..., session_id=..., data=...)` |
| `cut_point == 0` crash | No guard | Early return in `compact()` |
| Alternation silent failure | For-loop without else | `for/else` with `recent_msgs = []` fallback |
| `recent_messages` NameError | Wrong variable name | Corrected to `recent_msgs` throughout |
| `cut_point_msg_index` NameError | Undefined variable | Corrected to `cut_point` |
| `_call_api` signature gap | `model_override` not propagated | Both `_build_api_payload` and `_call_api` signatures shown |
| `def` not `async def` | Code example mismatch | `_format_messages` stays sync; compaction is separate async call |
| Dual threshold | Config field + module constant | Config field only, used via `self._settings.compaction_threshold` |
| Relational context lost | Task-only checkpoint format | Added "Conversation Dynamics" section |
| Summary drift / bloat | No size constraint | Target length in prompts (800-1200 words); validation check |
| Token estimation stuck | chars/4 forever, calibration in Phase 4 | `TokenEstimator` with calibration from API usage in Phase 1 |
| Summary validation | None | `_validate_summary()` checks length + required sections |
| `system_prompt=""` default | Easy to forget | Compaction trigger explicit at call site, system_prompt always available |
| DB table missing fields | No id, created_at, messages | Added UUID PK, created_at, messages JSONB |
| `_format_history_text` | Not addressed | Updated with summary prefix for reflection |

## References

- Claude Code SDK `compaction.js` — structured checkpoint format, iterative update, token-based cut points
- pi-coding-agent `compaction.js` — split-turn handling, file operation tracking, UPDATE prompt pattern
- OpenClaw session pruning — two-layer approach, soft-trim/hard-clear, TTL-based tool result management
- Nous codebase: `runner.py` lines 564-810 (stream_chat), 811-930 (_tool_loop), 1082-1100 (_format_messages)
