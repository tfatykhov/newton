# 009.5 â€” Decision Quality Gate

**Status:** Draft
**Author:** Emerson (spec), Tim (review)
**Date:** 2026-03-01
**Depends on:** 006.2 (deliberation), 007.3 (informational detection), 008.5 (decision review loop)

---

## Problem

Tier 2 review (2026-03-01) found **43% of logged decisions are noise** â€” chat fragments like "Done! âœ…", "On it! ðŸš€", "Review complete..." that aren't decisions at all. This pollutes the database, buries real decisions, and degrades calibration metrics.

**Root cause:** The deliberation pipeline creates a record for every turn in `decision`, `task`, and `debug` frames (`_DELIBERATION_FRAMES`). The `_is_informational()` filter catches some garbage but misses status updates, completion announcements, and intermediate work-in-progress responses.

**Evidence (from Tier 2 review):**
- 35 unreviewed decisions: 18 legitimate, 15 chat fragments, 1 partial, 1 duplicate
- Examples of noise: "Got the format. Now let me write...", "Done! âœ… PR #90 created...", "On it! ðŸš€ Subtask is running..."
- Duplicate: same Astro+Tailwind decision recorded twice, 1 minute apart

---

## Solution: Three-Layer Quality Gate

### Layer 1: Smarter Source Filtering (Pre-Record)

**Location:** `nous/cognitive/layer.py` â€” `_is_informational()` method

Expand the informational detection to catch the patterns that slip through:

#### 1a. Chat fragment patterns (new)

Add to `_INFO_PATTERNS`:
```python
# Completion / status updates (009.5)
"done!", "done.", "completed!", "finished!",
"on it!", "working on", "let me ",
"here's the result", "here are the results",
"created!", "pr #", "pr created", "pushed to",
"review complete", "spec scores", "task is running",

# Transition phrases (not decisions)
"now let me", "next i'll", "moving on to",
"let me check", "let me look", "let me search",
"i'll start", "i'll begin", "starting with",
```

#### 1b. Response-is-action-report heuristic (new)

If the response is primarily reporting what was just done (past tense + tool results), it's not a decision â€” it's a status update. Add a new check:

```python
def _is_action_report(self, turn_result: TurnResult) -> bool:
    """Detect responses that report completed actions, not decisions.
    
    Pattern: tool calls happened + response summarizes what was done.
    """
    if not turn_result.tool_results:
        return False
    
    response_lower = turn_result.response_text[:300].lower()
    report_markers = [
        "done", "created", "updated", "fixed", "merged",
        "pushed", "committed", "deployed", "sent", "saved",
        "completed", "finished", "resolved", "applied",
    ]
    # If 2+ report markers in first 300 chars after tool use â†’ action report
    matches = sum(1 for m in report_markers if m in response_lower)
    return matches >= 2
```

Wire into `_is_informational()`:
```python
# 5. Action report: tools used + response summarizes what was done (009.5)
if self._is_action_report(turn_result):
    return True
```

#### 1c. Frame refinement

Consider removing `task` from `_DELIBERATION_FRAMES`. Task execution is action, not decision. Only `decision` frame should auto-deliberate. `debug` is borderline â€” keep for now but monitor.

```python
# deliberation.py
_DELIBERATION_FRAMES = {"decision"}  # was {"decision", "task", "debug"}
```

**Trade-off:** This is aggressive. Some task-frame responses ARE decisions ("I'll use approach X to solve this"). But the current noise rate (43%) suggests over-recording is worse than under-recording. The explicit `record_decision` tool always works regardless of frame â€” agents that genuinely make decisions during tasks can still log them.

**Recommendation:** Start with removing `task`, keep `debug`. Monitor noise rate for 1 week. If legitimate decisions are missed, add `task` back with a stricter filter.

---

### Layer 2: Deduplication Window (Pre-Record)

**Location:** `nous/cognitive/deliberation.py` â€” `start()` method

Before creating a new deliberation record, check if a similar decision was recorded in the last N minutes for the same session.

```python
async def start(self, agent_id: str, description: str, frame: FrameSelection, 
                session=None) -> UUID:
    """Start deliberation â€” with dedup check (009.5)."""
    
    # Dedup: check last 5 minutes for similar descriptions
    if await self._is_duplicate(agent_id, description, window_minutes=5, session=session):
        logger.debug("Skipping duplicate deliberation: %s", description[:80])
        return None  # No deliberation started
    
    # ... existing logic
```

Similarity check â€” simple but effective:

```python
async def _is_duplicate(self, agent_id: str, description: str, 
                         window_minutes: int = 5, session=None) -> bool:
    """Check if a similar decision was recorded recently.
    
    Uses simple token overlap (Jaccard similarity) â€” no LLM call needed.
    Threshold: 0.6 overlap = duplicate.
    """
    cutoff = datetime.now(UTC) - timedelta(minutes=window_minutes)
    recent = await self._brain.get_recent_decisions(
        agent_id, since=cutoff, limit=5, session=session
    )
    
    desc_tokens = set(description.lower().split())
    for decision in recent:
        existing_tokens = set(decision.description.lower().split())
        if not desc_tokens or not existing_tokens:
            continue
        overlap = len(desc_tokens & existing_tokens) / len(desc_tokens | existing_tokens)
        if overlap > 0.6:
            return True
    return False
```

**Brain API addition needed:**
```python
async def get_recent_decisions(self, agent_id: str, since: datetime, 
                                limit: int = 5, session=None) -> list[DecisionSummary]:
    """Fetch recent decisions since a cutoff time."""
```

---

### Layer 3: Quality Gate at Record Time (Post-Filter)

**Location:** `nous/brain/brain.py` or `nous/cognitive/deliberation.py` â€” `finalize()` method

Even if a decision passes Layers 1 and 2, validate quality before persisting:

```python
QUALITY_RULES = [
    # Rule 1: Description too short
    lambda d: len(d.description.strip()) < 20,
    
    # Rule 2: Default confidence + high stakes (auto-logged, not deliberated)
    lambda d: d.confidence == 0.5 and d.stakes in ("high", "critical"),
    
    # Rule 3: Description starts with chat patterns
    lambda d: any(d.description.lower().startswith(p) for p in [
        "done", "on it", "here's", "got it", "sure", "okay",
        "alright", "working on", "let me", "i'll",
    ]),
    
    # Rule 4: Description is just an error message
    lambda d: "encountered an error processing your request" in d.description.lower(),
]

def validate_decision_quality(decision) -> tuple[bool, str | None]:
    """Returns (is_valid, rejection_reason)."""
    for i, rule in enumerate(QUALITY_RULES):
        if rule(decision):
            reasons = [
                "Description too short (<20 chars)",
                "Default confidence (0.5) with high stakes",
                "Description starts with chat pattern",
                "Description is an error message template",
            ]
            return False, reasons[i]
    return True, None
```

On rejection, **delete** the decision (don't just mark it â€” clean DB is the goal):

```python
async def finalize(self, decision_id, ...):
    # ... existing finalize logic ...
    
    # 009.5: Quality gate
    is_valid, reason = validate_decision_quality(decision)
    if not is_valid:
        logger.info("Rejected decision %s: %s", decision_id, reason)
        await self.delete(decision_id, session=session)
        return None
    
    # ... persist
```

---

## Implementation Plan

### Phase 1: Source Filtering (Layer 1)
**Files:** `nous/cognitive/layer.py`
**Tasks:**
1. Add new patterns to `_INFO_PATTERNS`
2. Implement `_is_action_report()` method
3. Wire into `_is_informational()`
4. Remove `task` from `_DELIBERATION_FRAMES` (deliberation.py)
5. Tests for new patterns

### Phase 2: Deduplication (Layer 2)
**Files:** `nous/cognitive/deliberation.py`, `nous/brain/brain.py`
**Tasks:**
1. Add `get_recent_decisions()` to Brain API
2. Implement `_is_duplicate()` in DeliberationEngine
3. Wire into `start()` â€” return None on duplicate
4. Handle None return in `layer.py` (no decision_id = skip deliberation flow)
5. Tests for dedup

### Phase 3: Quality Gate (Layer 3)
**Files:** `nous/cognitive/deliberation.py` or `nous/brain/brain.py`
**Tasks:**
1. Implement `validate_decision_quality()` 
2. Wire into `finalize()` â€” delete on rejection
3. Add metrics: count rejections by rule (for tuning)
4. Tests for each quality rule

### Phase 4: Cleanup & Monitoring
**Tasks:**
1. One-time cleanup: delete existing noise decisions identified by Tier 2 review
2. Add logging for rejection counts per rule (monitor false positives)
3. After 1 week: review rejection logs, tune thresholds
4. If `task` frame removal caused missed decisions, add back with stricter filter

---

## Success Criteria

| Metric | Before | Target |
|--------|--------|--------|
| Noise rate (non-decisions in DB) | 43% | < 5% |
| Duplicate decisions | Present | 0 |
| Legitimate decisions missed | 0 | < 5% |
| Error template decisions | 4 found | 0 |

---

## Risks

1. **Over-filtering:** Legitimate decisions in `task` frame get dropped. Mitigation: explicit `record_decision` tool always works. Monitor for 1 week.
2. **Dedup false positives:** Similar but distinct decisions within 5 min get merged. Mitigation: Jaccard threshold 0.6 is conservative; raise if needed.
3. **Quality gate too aggressive:** Short but valid decisions rejected. Mitigation: 20-char minimum is very low; log rejections for review.

---

## Related

- **006.2:** Original deliberation pipeline and `_is_informational()` filter
- **007.3:** Expanded informational patterns and `_DELIBERATION_FRAMES`
- **008.5:** Decision review loop (catches what slips through)
- **F027 (CE):** Decision quality scoring (tags, patterns, reasons) â€” inspiration for quality gate
