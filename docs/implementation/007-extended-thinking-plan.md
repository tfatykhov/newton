# 007 — Extended Thinking / Adaptive Thinking Support

**Issue:** #32
**Status:** Plan (v2 — post-review)
**Files changed:** `config.py`, `runner.py`, `telegram_bot.py`, `rest.py`
**Files added:** None
**Tests:** `tests/test_runner.py` (expanded)

## Summary

Add support for Claude's extended thinking and adaptive thinking modes in the Nous runner. Three modes: off (default), adaptive (recommended for 4.6 models), manual (budget_tokens for older models). Includes effort parameter support, streaming thinking events, and thinking block preservation in tool loops.

## Review findings incorporated (3-agent review)

| Finding | Source | Fix |
|---------|--------|-----|
| `redacted_thinking` blocks not handled in streaming | All 3 reviewers (P0) | Phase C + D |
| `max_tokens=4096` < `thinking_budget=10000` default | All 3 reviewers (P0) | Phase A validation |
| Content block ordering breaks with interleaved thinking | All 3 reviewers (P0/P1) | Phase D rewrite |
| Reflection call wastes tokens on thinking | All 3 reviewers (P1) | Phase B `skip_thinking` param |
| Config needs Literal types | All 3 reviewers (P1/P2) | Phase A |
| `budget_tokens >= 1024` minimum | API specialist (P0) | Phase A validation |
| Telegram dead air during thinking | Devil's advocate (P1) | Phase E |
| REST SSE sends signature to clients | Devil's advocate (P1) | Phase D filter |
| Multi-turn history (Message.content) | Devil's advocate P0-2 | **OVERRULED** — docs say "you can omit thinking blocks from prior assistant role turns" |

## API Reference (verified from docs 2026-02-24)

### Payload format

```json
{
  "model": "claude-opus-4-6",
  "max_tokens": 16000,
  "thinking": {"type": "adaptive"},
  "output_config": {"effort": "medium"},
  "messages": [...]
}
```

- **Adaptive**: `"thinking": {"type": "adaptive"}` — Opus 4.6, Sonnet 4.6
- **Manual**: `"thinking": {"type": "enabled", "budget_tokens": N}` — all models (deprecated on 4.6)
- **Off**: omit `thinking` parameter
- **Effort**: `"output_config": {"effort": "low|medium|high|max"}` — Opus 4.6, Sonnet 4.6, Opus 4.5. Goes in `output_config`, NOT top-level.
- **Interleaved thinking beta**: `anthropic-beta: interleaved-thinking-2025-05-14` header. Auto-enabled with adaptive on 4.6 models. Needed for manual mode on Sonnet 4.6 and other Claude 4 models.
- **Constraint**: `budget_tokens >= 1024` and `budget_tokens < max_tokens` for manual mode.

### Response content blocks

```json
{"type": "thinking", "thinking": "...", "signature": "..."}
{"type": "redacted_thinking", "data": "..."}
{"type": "text", "text": "..."}
{"type": "tool_use", "id": "...", "name": "...", "input": {...}}
```

### Streaming events

```
content_block_start: {"type": "thinking", "thinking": ""}
content_block_delta: {"type": "thinking_delta", "thinking": "..."}
content_block_delta: {"type": "signature_delta", "signature": "..."}
content_block_stop

content_block_start: {"type": "redacted_thinking", "data": "..."}
content_block_stop
(no deltas — data is in the start event itself)
```

### Critical constraints

1. **Thinking block preservation in tool loops**: ALL thinking blocks (including redacted_thinking) from the assistant response MUST be passed back unmodified in the next API call within a tool loop.
2. **Block ordering**: "the entire sequence of consecutive thinking blocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks."
3. **Cross-turn**: "you can omit thinking blocks from prior assistant role turns" — so `Message.content: str` is fine for conversation history.

## Phase A: Config (`config.py`)

Add 3 new settings after `max_tokens`, using Literal types for validation:

```python
from typing import Literal

# Extended thinking
thinking_mode: Literal["off", "adaptive", "manual"] = "off"
thinking_budget: int = 10000    # budget_tokens, only used when thinking_mode=manual
effort: Literal["low", "medium", "high", "max"] = "high"
```

### A1: Startup validation (pydantic model_validator)

```python
@model_validator(mode="after")
def _validate_thinking(self) -> "Settings":
    if self.thinking_mode == "manual":
        if self.thinking_budget < 1024:
            raise ValueError("thinking_budget must be >= 1024 (API minimum)")
        if self.thinking_budget >= self.max_tokens:
            raise ValueError(
                f"thinking_budget ({self.thinking_budget}) must be < "
                f"max_tokens ({self.max_tokens}). Increase max_tokens."
            )
    return self
```

**Notes:**
- `thinking_mode="off"` is default — no behavior change for existing users
- `effort="high"` is API default — equivalent to omitting the parameter
- `Literal` types catch typos at config load time (e.g., `NOUS_THINKING_MODE=on` fails immediately)
- Validator ensures `budget < max_tokens` for manual mode — user must explicitly increase `max_tokens`

## Phase B: API Payload + Headers (`runner.py`)

### B1: `_build_api_payload()` — add thinking + effort + skip_thinking param

Add `skip_thinking: bool = False` parameter to `_build_api_payload`. The reflection call in `end_conversation` passes `skip_thinking=True`.

```python
def _build_api_payload(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
    stream: bool = False,
    skip_thinking: bool = False,
) -> dict[str, Any]:
    payload: dict[str, Any] = {
        "model": self._settings.model,
        "max_tokens": self._settings.max_tokens,
        "system": [...],
        "messages": messages,
    }
    if tools:
        payload["tools"] = tools
    if stream:
        payload["stream"] = True

    # Thinking (skip for utility calls like reflection)
    if not skip_thinking:
        if self._settings.thinking_mode == "adaptive":
            payload["thinking"] = {"type": "adaptive"}
        elif self._settings.thinking_mode == "manual":
            payload["thinking"] = {
                "type": "enabled",
                "budget_tokens": self._settings.thinking_budget,
            }

    # Effort (only include if not default "high"; works with or without thinking)
    if self._settings.effort != "high":
        payload["output_config"] = {"effort": self._settings.effort}

    return payload
```

### B2: `_call_api()` — pass skip_thinking through

```python
async def _call_api(
    self, ..., skip_thinking: bool = False,
) -> ApiResponse:
    payload = self._build_api_payload(..., skip_thinking=skip_thinking)
```

### B3: `end_conversation()` — skip thinking for reflection

```python
api_response = await self._call_api(
    system_prompt="You are reviewing a conversation...",
    messages=[...],
    tools=None,
    skip_thinking=True,
)
```

### B4: `start()` — beta headers for interleaved thinking

Replace the inline beta header with list-based approach:

```python
beta_features: list[str] = []

# OAT beta (existing logic)
if is_oat:
    beta_features.append("oauth-2025-04-20")

# Interleaved thinking beta (safe to always send when thinking enabled;
# deprecated on Opus 4.6 but safely ignored)
if self._settings.thinking_mode != "off":
    beta_features.append("interleaved-thinking-2025-05-14")

if beta_features:
    headers["anthropic-beta"] = ",".join(beta_features)
```

**Important**: This replaces ALL three existing inline `headers["anthropic-beta"] = ...` assignments (lines 208, 213, none for non-OAT) with a single list-based approach.

## Phase C: Streaming Parser (`runner.py: _parse_sse_event`)

Handle new event subtypes. All changes are inside the existing `content_block_start` and `content_block_delta` handlers.

### C1: `content_block_start` — handle thinking + redacted_thinking

Insert as elif branches before the text_block_start fallthrough:

```python
if event_type == "content_block_start":
    block = data.get("content_block", {})
    block_index = data.get("index", 0)
    block_type = block.get("type")
    if block_type == "tool_use":
        return StreamEvent(type="tool_start", ...)
    elif block_type == "thinking":
        return StreamEvent(type="thinking_start", block_index=block_index)
    elif block_type == "redacted_thinking":
        # redacted_thinking carries data in the start event, no deltas follow
        return StreamEvent(
            type="redacted_thinking",
            text=block.get("data", ""),
            block_index=block_index,
        )
    return StreamEvent(type="text_block_start", block_index=block_index)
```

### C2: `content_block_delta` — handle thinking_delta + signature_delta

Insert as elif branches before the fallthrough `return None`:

```python
if event_type == "content_block_delta":
    delta = data.get("delta", {})
    block_index = data.get("index", 0)
    delta_type = delta.get("type")
    if delta_type == "text_delta":
        return StreamEvent(type="text_delta", text=delta.get("text", ""))
    elif delta_type == "input_json_delta":
        return StreamEvent(type="tool_input_delta", text=delta.get("partial_json", ""), block_index=block_index)
    elif delta_type == "thinking_delta":
        return StreamEvent(type="thinking_delta", text=delta.get("thinking", ""))
    elif delta_type == "signature_delta":
        return StreamEvent(type="signature_delta", text=delta.get("signature", ""), block_index=block_index)
    return None
```

## Phase D: Streaming Content Block Preservation (`runner.py: stream_chat`)

**This is the critical fix.** The plan uses an **index-ordered block accumulator** to preserve all content blocks in their original order, supporting interleaved thinking correctly.

### D1: Replace separate accumulators with unified ordered approach

Instead of `text_parts: list[str]` + `block_accumulators: dict` + separate thinking lists, use a single `all_blocks: dict[int, dict]` keyed by block_index that tracks all block types:

```python
for turn in range(self._settings.max_turns):
    all_blocks: dict[int, dict[str, Any]] = {}  # block_index -> block data
    tool_calls: list[dict[str, Any]] = []  # finalized tool calls
    text_parts: list[str] = []  # for yielding to client & building response_text
    stop_reason = ""

    async for event in self._call_api_stream(...):
        if event.type == "thinking_start":
            all_blocks[event.block_index] = {
                "type": "thinking",
                "thinking_parts": [],
                "signature": "",
            }

        elif event.type == "redacted_thinking":
            # Complete block — data arrives in start event, no deltas
            all_blocks[event.block_index] = {
                "type": "redacted_thinking",
                "data": event.text,
                "_complete": True,
            }

        elif event.type == "thinking_delta":
            # Find the thinking block (use last thinking block by index)
            for idx in sorted(all_blocks, reverse=True):
                if all_blocks[idx]["type"] == "thinking":
                    all_blocks[idx]["thinking_parts"].append(event.text)
                    break

        elif event.type == "signature_delta":
            # Attach signature to the thinking block at this index
            block = all_blocks.get(event.block_index)
            if block and block["type"] == "thinking":
                block["signature"] = event.text
            # Do NOT yield to client

        elif event.type == "text_block_start":
            all_blocks[event.block_index] = {
                "type": "text",
                "text_parts": [],
            }

        elif event.type == "text_delta":
            text_parts.append(event.text)
            # Also track in the block for content reconstruction
            for idx in sorted(all_blocks, reverse=True):
                if all_blocks[idx]["type"] == "text":
                    all_blocks[idx]["text_parts"].append(event.text)
                    break
            yield event  # Stream to client

        elif event.type == "tool_start":
            all_blocks[event.block_index] = {
                "type": "tool_use",
                "id": event.tool_id,
                "name": event.tool_name,
                "input_parts": [],
            }
            yield event

        elif event.type == "tool_input_delta":
            block = all_blocks.get(event.block_index)
            if block and block["type"] == "tool_use":
                block["input_parts"].append(event.text)

        elif event.type == "block_stop":
            block = all_blocks.get(event.block_index)
            if block and block["type"] == "tool_use":
                input_json = "".join(block["input_parts"])
                try:
                    block["input"] = json.loads(input_json) if input_json else {}
                except json.JSONDecodeError:
                    block["input"] = {}
                tool_calls.append(block)

        elif event.type == "done":
            stop_reason = event.stop_reason
            # ... usage tracking ...
```

### D2: Build content_blocks in index order

When the stream segment ends and tool_calls exist, build content_blocks from `all_blocks` sorted by index:

```python
content_blocks: list[dict[str, Any]] = []
for idx in sorted(all_blocks):
    block = all_blocks[idx]
    if block["type"] == "thinking":
        content_blocks.append({
            "type": "thinking",
            "thinking": "".join(block["thinking_parts"]),
            "signature": block["signature"],
        })
    elif block["type"] == "redacted_thinking":
        content_blocks.append({
            "type": "redacted_thinking",
            "data": block["data"],
        })
    elif block["type"] == "text":
        content_blocks.append({
            "type": "text",
            "text": "".join(block["text_parts"]),
        })
    elif block["type"] == "tool_use":
        content_blocks.append({
            "type": "tool_use",
            "id": block["id"],
            "name": block["name"],
            "input": block.get("input", {}),
        })

messages.append({"role": "assistant", "content": content_blocks})
```

This preserves the exact block order from the API, supporting interleaved thinking correctly.

### D3: Filter internal events from client stream

In the event loop, do NOT yield `thinking_delta`, `thinking_start`, `redacted_thinking`, or `signature_delta` events to the client. These are internal plumbing for block preservation. Only yield: `text_delta`, `tool_start`, `tool_end`, `done`, `error`, `message_start`.

Rationale: signature data should never reach external clients; thinking summaries are internal reasoning.

**Note on REST SSE**: Since `stream_chat` filters these events before yielding, `rest.py:chat_stream` needs no changes — it only sees the filtered events.

## Phase E: Telegram Thinking Indicator (`telegram_bot.py`)

### E1: Re-send typing indicator during thinking

The Telegram typing indicator expires after 5 seconds. With extended thinking, Claude may think for 10-30+ seconds before emitting text. Add periodic typing re-sends.

In `_chat_streaming`, after establishing the SSE connection, track when the last text was received. If no text_delta arrives for >4 seconds, re-send typing indicator:

```python
last_text_time = time.time()

async for line in response.aiter_lines():
    # ... parse event ...
    if event.get("type") == "text_delta":
        last_text_time = time.time()
        await streamer.append_text(event.get("text", ""))
    elif event.get("type") == "tool_start":
        await streamer.append_tool_indicator(event.get("tool_name", ""))
    elif event.get("type") == "done":
        # ...
        break

    # Re-send typing if no text for >4 seconds
    if time.time() - last_text_time > 4.0:
        await self._tg("sendChatAction", params={"chat_id": chat_id, "action": "typing"})
        last_text_time = time.time()  # reset to avoid spam
```

This is lightweight (one Telegram API call every 4s during thinking) and prevents the dead-air UX problem.

## Phase F: Tests (`tests/test_runner.py`)

### F1: Config tests
- `test_thinking_mode_defaults` — off, budget=10000, effort=high
- `test_thinking_mode_literal_validation` — invalid value raises ValidationError
- `test_thinking_budget_minimum` — budget < 1024 raises ValueError
- `test_thinking_budget_max_tokens_constraint` — budget >= max_tokens raises ValueError

### F2: Payload tests
- `test_payload_thinking_off` — no thinking key in payload
- `test_payload_thinking_adaptive` — `{"type": "adaptive"}` in payload
- `test_payload_thinking_manual` — `{"type": "enabled", "budget_tokens": 10000}`
- `test_payload_effort_default` — no output_config when effort=high
- `test_payload_effort_medium` — `output_config: {"effort": "medium"}`
- `test_payload_effort_without_thinking` — effort works when thinking_mode=off
- `test_payload_skip_thinking` — skip_thinking=True omits thinking param

### F3: SSE parser tests
- `test_parse_thinking_block_start` — type="thinking" -> thinking_start event
- `test_parse_thinking_delta` — thinking_delta with thinking text
- `test_parse_signature_delta` — signature_delta event
- `test_parse_redacted_thinking_start` — type="redacted_thinking" -> redacted_thinking event with data
- `test_parse_redacted_thinking_no_fallthrough` — does NOT produce text_block_start

### F4: Beta header tests
- `test_start_thinking_beta_header` — interleaved-thinking header present
- `test_start_oat_plus_thinking_headers` — both OAT + thinking combined
- `test_start_no_thinking_no_beta` — thinking_mode=off, no interleaved header

### F5: Tool loop thinking preservation
- `test_tool_loop_preserves_thinking_blocks` — non-streaming: full content blocks passed back (already works, test confirms)
- `test_tool_loop_preserves_redacted_thinking` — non-streaming: redacted_thinking in content preserved

### F6: Streaming content block ordering
- `test_streaming_blocks_preserve_order` — verify content_blocks built in index order from all_blocks dict
- `test_streaming_interleaved_thinking_order` — [thinking, text, thinking, tool_use] order preserved
- `test_streaming_redacted_thinking_preserved` — redacted_thinking block in streaming tool loop

## NOT in scope (deferred)

- **Deliberation integration** — feeding thinking traces into Brain deliberation records. Deserves its own spec.
- **Per-turn thinking mode switching** — API supports it but adds complexity. Always use the same mode for the session.
- **Thinking content display** — thinking summaries are not shown to users (filtered from stream). Could be exposed later via debug mode.
- **Prompt caching optimization** — thinking affects cache breakpoints but existing cache_control usage is unchanged.

## Implementation order

1. Phase A (config) — Literal types, validator, 3 settings
2. Phase B (payload + headers) — _build_api_payload + skip_thinking + beta headers
3. Phase C (streaming parser) — thinking/redacted_thinking/signature events
4. Phase D (streaming preservation) — unified block accumulator, index-ordered reconstruction
5. Phase E (telegram) — periodic typing re-send during thinking
6. Phase F (tests) — comprehensive coverage including streaming preservation

## Risk assessment

| Risk | Likelihood | Mitigation |
|------|-----------|------------|
| Breaking existing non-thinking flows | Low | thinking_mode="off" is default, no payload changes when off |
| Thinking blocks not preserved in streaming tool loop | **Fixed** | Phase D: unified index-ordered block accumulator |
| Content block reordering with interleaved thinking | **Fixed** | Phase D: blocks sorted by index, not by type |
| redacted_thinking blocks lost | **Fixed** | Phase C + D handle redacted_thinking explicitly |
| Beta header conflict with OAT | Low | List-based header building handles multiple betas |
| max_tokens < thinking_budget | **Fixed** | Phase A: pydantic model_validator at startup |
| Telegram dead air during thinking | **Fixed** | Phase E: periodic typing indicator re-send |
| Signature data leaking to clients | **Fixed** | Phase D: signature_delta not yielded to client |
