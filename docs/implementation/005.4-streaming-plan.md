# 005.4 Streaming Responses â€” Implementation Plan

**Status:** Shipped (PR #23)
**Branch:** `feat/005.4-streaming`
**Based on:** 005.4 spec + 3-agent review (arch: `dd96aaac`, api: `dac0be4f`, devil: `e1b84f31`)

## Review Summary

3 reviewers found **27 unique issues** (12 P0, 8 P1, 7 P2). Root cause: spec was written against an imagined API surface. All P0s are method/attribute mismatches that would cause immediate runtime errors.

**Convergence (3/3 agreement):**
- `_build_headers()` / `self._base_url` don't exist (use `self._http` which has both)
- `_get_or_create_session()` â†’ actual is `_get_or_create_conversation()`
- `pre_turn()` signature wrong (missing `agent_id`, `conversation_messages`)
- `dispatch()` returns `tuple[str, bool]`, not dict
- `post_turn()` needs `TurnResult` + `TurnContext`, not raw strings
- System prompt must use `cache_control` list format
- `model_name` â†’ `model`

## Files Changed

| File | Change | Est. Lines |
|------|--------|-----------|
| `nous/api/runner.py` | Add `StreamEvent`, `_parse_sse_event()`, `_call_api_stream()`, `stream_chat()`, `_build_api_payload()` | ~250 |
| `nous/api/rest.py` | Add `POST /chat/stream` SSE endpoint inside `create_app()` | ~50 |
| `nous/telegram_bot.py` | Add `StreamingMessage` class + `_chat_streaming()` method on `NousTelegramBot` | ~100 |
| `tests/test_streaming.py` | NEW â€” 23 test cases | ~350 |
| **Total** | | ~750 |

---

## Phase A: Runner Streaming Core (~250 lines in runner.py)

### A1: StreamEvent Dataclass

Add after `ApiResponse` (line ~71):

```python
@dataclass
class StreamEvent:
    """A single event from the streaming API response."""
    type: str  # text_delta, tool_start, tool_input_delta, tool_end, block_stop, done, error, message_stop
    text: str = ""
    tool_name: str = ""
    tool_id: str = ""
    tool_input: dict = field(default_factory=dict)
    stop_reason: str = ""
    block_index: int = 0
```

### A2: SSE Event Parser

Add as module-level function:

```python
def _parse_sse_event(data: dict[str, Any]) -> StreamEvent | None:
    """Parse Anthropic SSE event dict into StreamEvent.

    N4: Skip ping keepalives.
    N3: stop_reason is in message_delta.delta, NOT message_start.
    N2: Handle in-stream error events (HTTP 200 but error in body).
    """
    event_type = data.get("type")

    if event_type == "ping":
        return None

    if event_type == "error":
        error = data.get("error", {})
        return StreamEvent(
            type="error",
            text=f"{error.get('type', 'unknown')}: {error.get('message', '')}",
        )

    if event_type == "content_block_start":
        block = data.get("content_block", {})
        block_index = data.get("index", 0)
        if block.get("type") == "tool_use":
            return StreamEvent(
                type="tool_start",
                tool_name=block.get("name", ""),
                tool_id=block.get("id", ""),
                block_index=block_index,
            )
        return StreamEvent(type="text_block_start", block_index=block_index)

    if event_type == "content_block_delta":
        delta = data.get("delta", {})
        block_index = data.get("index", 0)
        if delta.get("type") == "text_delta":
            return StreamEvent(type="text_delta", text=delta.get("text", ""))
        if delta.get("type") == "input_json_delta":
            return StreamEvent(
                type="tool_input_delta",
                text=delta.get("partial_json", ""),
                block_index=block_index,
            )
        return None

    if event_type == "content_block_stop":
        return StreamEvent(type="block_stop", block_index=data.get("index", 0))

    if event_type == "message_delta":
        return StreamEvent(
            type="done",
            stop_reason=data.get("delta", {}).get("stop_reason", ""),
        )

    if event_type == "message_stop":
        return StreamEvent(type="message_stop")

    return None
```

### A3: Shared Payload Builder

Extract from `_call_api` to avoid duplication (review finding A10):

```python
def _build_api_payload(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
    stream: bool = False,
) -> dict[str, Any]:
    """Build Anthropic Messages API request payload.

    Shared by _call_api and _call_api_stream to avoid divergence.
    """
    payload: dict[str, Any] = {
        "model": self._settings.model,
        "max_tokens": self._settings.max_tokens,
        "system": [
            {
                "type": "text",
                "text": system_prompt,
                "cache_control": {"type": "ephemeral"},
            }
        ],
        "messages": messages,
    }
    if tools:
        payload["tools"] = tools
    if stream:
        payload["stream"] = True
    return payload
```

Then refactor `_call_api` to use: `payload = self._build_api_payload(system_prompt, messages, tools)`.

### A4: Streaming API Call

Add as sibling to `_call_api` (D7 â€” keeps `_call_api` untouched for N5):

```python
async def _call_api_stream(
    self,
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None = None,
) -> AsyncGenerator[StreamEvent, None]:
    """Call Anthropic API with streaming enabled.

    Yields StreamEvent objects. Uses self._http which already has
    auth headers and base_url configured (set in start()).

    N2: Yields error event on HTTP errors or in-stream errors.
    N8: Only processes data: lines (skips event: lines naturally).
    """
    if not self._http:
        raise RuntimeError("httpx client not initialized -- call start() first")

    payload = self._build_api_payload(system_prompt, messages, tools, stream=True)

    async with self._http.stream("POST", "/v1/messages", json=payload) as response:
        if response.status_code != 200:
            error_body = await response.aread()
            yield StreamEvent(type="error", text=error_body.decode()[:500])
            return

        async for line in response.aiter_lines():
            if not line.startswith("data: "):
                continue
            data = json.loads(line[6:])
            event = _parse_sse_event(data)
            if event:
                if event.type == "error":
                    yield event
                    return
                yield event
```

### A5: Streaming Chat Method

The core method â€” mirrors `run_turn()` + `_tool_loop()` but with streaming.

**Critical fixes from review:**
- Use `_get_or_create_conversation()` (not `_get_or_create_session`)
- Pass `agent_id` to cognitive hooks
- Build `TurnResult` with tool tracking for `post_turn()`
- Use `_build_system_prompt()` (not raw `turn_context.system_prompt`)
- Use `_dispatcher.available_tools(frame_id)` (not `_get_tools_for_frame`)
- Use `dispatch()` returning `tuple[str, bool]` (not dict)
- Store `Message` objects (not raw dicts) in conversation
- Wrap in try/finally to guarantee `post_turn()` runs
- Call `_check_safety_net()` at end
- Handle max_turns with final no-tools API call

```python
async def stream_chat(
    self,
    session_id: str,
    user_message: str,
    agent_id: str | None = None,
) -> AsyncGenerator[StreamEvent, None]:
    """Full chat turn with streaming, including tool loops.

    Mirrors run_turn() flow but yields StreamEvents as they arrive.
    Tool calls execute between stream segments.
    """
    if not self._dispatcher:
        raise RuntimeError("No tool dispatcher set -- call set_dispatcher() first")

    _agent_id = agent_id or self._settings.agent_id
    conversation = self._get_or_create_conversation(session_id)

    # Pre-turn with conversation dedup (F4)
    recent_messages = [
        m.content for m in conversation.messages if m.role == "user"
    ][-8:]
    turn_context = await self._cognitive.pre_turn(
        _agent_id,
        session_id,
        user_message,
        conversation_messages=recent_messages or None,
    )

    conversation.messages.append(Message(role="user", content=user_message))

    system_prompt = self._build_system_prompt(turn_context)
    tools = self._dispatcher.available_tools(turn_context.frame.frame_id)
    messages = self._format_messages(conversation)

    all_tool_results: list[ToolResult] = []
    response_text = ""
    error = None

    try:
        for turn in range(self._settings.max_turns):
            text_parts: list[str] = []
            tool_calls: list[dict[str, Any]] = []
            block_accumulators: dict[int, dict[str, Any]] = {}
            stop_reason = ""

            async for event in self._call_api_stream(messages, tools if tools else None, system_prompt):
                if event.type == "error":
                    error = event.text
                    yield event
                    return

                elif event.type == "text_delta":
                    text_parts.append(event.text)
                    yield event

                elif event.type == "tool_start":
                    block_accumulators[event.block_index] = {
                        "id": event.tool_id,
                        "name": event.tool_name,
                        "input_parts": [],
                    }
                    yield event

                elif event.type == "tool_input_delta":
                    acc = block_accumulators.get(event.block_index)
                    if acc:
                        acc["input_parts"].append(event.text)

                elif event.type == "block_stop":
                    acc = block_accumulators.pop(event.block_index, None)
                    if acc:
                        input_json = "".join(acc["input_parts"])
                        try:
                            acc["input"] = json.loads(input_json) if input_json else {}
                        except json.JSONDecodeError:
                            acc["input"] = {}
                        tool_calls.append(acc)

                elif event.type == "done":
                    stop_reason = event.stop_reason

            # Stream segment ended â€” decide next action
            if stop_reason == "end_turn" or not tool_calls:
                response_text = "".join(text_parts)
                break

            # Build assistant message with tool_use content blocks
            content_blocks: list[dict[str, Any]] = []
            if text_parts:
                content_blocks.append({"type": "text", "text": "".join(text_parts)})
            for tc in tool_calls:
                content_blocks.append({
                    "type": "tool_use",
                    "id": tc["id"],
                    "name": tc["name"],
                    "input": tc["input"],
                })
            messages.append({"role": "assistant", "content": content_blocks})

            # Execute tools (P1-2: all results in single user message)
            tool_results_for_message: list[dict[str, Any]] = []
            for tc in tool_calls:
                yield StreamEvent(type="tool_start", tool_name=tc["name"])
                start_time = time.monotonic()
                try:
                    result_text, is_error = await self._dispatcher.dispatch(
                        tc["name"], tc["input"]
                    )
                except Exception as e:
                    result_text = str(e)
                    is_error = True
                duration_ms = int((time.monotonic() - start_time) * 1000)

                tool_results_for_message.append({
                    "type": "tool_result",
                    "tool_use_id": tc["id"],
                    "content": result_text,
                    "is_error": is_error,
                })

                all_tool_results.append(ToolResult(
                    tool_name=tc["name"],
                    arguments=tc["input"],
                    result=result_text if not is_error else None,
                    error=result_text if is_error else None,
                    duration_ms=duration_ms,
                ))

                yield StreamEvent(type="tool_end", tool_name=tc["name"])

            messages.append({"role": "user", "content": tool_results_for_message})
        else:
            # Max turns reached â€” final call without tools
            logger.warning("Streaming tool loop reached max_turns=%d", self._settings.max_turns)
            try:
                final = await self._call_api(
                    system_prompt=system_prompt,
                    messages=messages,
                    tools=None,
                )
                response_text = self._extract_text(final.content)
            except Exception:
                response_text = "I reached the maximum number of tool iterations."

        # Store assistant response
        conversation.messages.append(Message(role="assistant", content=response_text))

    except Exception as e:
        logger.error("Streaming error: %s", e)
        error = str(e)
        response_text = "I encountered an error processing your request."
        conversation.messages.append(Message(role="assistant", content=response_text))
    finally:
        # ALWAYS call post_turn (review P1: guaranteed cleanup)
        turn_result = TurnResult(
            response_text=response_text,
            tool_results=all_tool_results,
            error=error,
        )
        await self._cognitive.post_turn(_agent_id, session_id, turn_result, turn_context)
        self._check_safety_net(turn_context, all_tool_results)
        conversation.turn_contexts.append(turn_context)

    yield StreamEvent(type="done", stop_reason="end_turn")
```

**Note on `_call_api_stream` parameter order:** The method signature is `(system_prompt, messages, tools)` but `stream_chat` calls it as `(messages, tools, system_prompt)` in the spec. Implementation MUST use keyword args or match the signature order.

### A6: Additional Import

Add to runner.py imports:

```python
import json
from collections.abc import AsyncGenerator
```

---

## Phase B: REST SSE Endpoint (~50 lines in rest.py)

### B1: Add /chat/stream Inside create_app()

Uses closure pattern matching existing endpoints (review P0: no `app.state.components`):

```python
async def chat_stream(request: Request) -> StreamingResponse:
    """POST /chat/stream - SSE streaming chat."""
    try:
        body = await request.json()
    except Exception:
        return JSONResponse({"error": "Invalid JSON body"}, status_code=400)

    message = body.get("message")
    if not message:
        return JSONResponse({"error": "Missing required field: message"}, status_code=400)

    session_id = body.get("session_id") or str(uuid4())

    async def event_generator():
        try:
            async for event in runner.stream_chat(session_id, message):
                data = json.dumps({
                    "type": event.type,
                    "text": event.text,
                    "tool_name": event.tool_name,
                    "stop_reason": event.stop_reason,
                })
                yield f"data: {data}\n\n"
        except Exception as e:
            logger.error("Stream error: %s", e)
            error_data = json.dumps({"type": "error", "text": str(e)})
            yield f"data: {error_data}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        },
    )
```

### B2: Add Route + Import

Add `StreamingResponse` import:
```python
from starlette.responses import JSONResponse, StreamingResponse
```

Add `import json` to imports.

Add route:
```python
Route("/chat/stream", chat_stream, methods=["POST"]),
```

### B3: Keep /chat Unchanged

Existing `/chat` endpoint stays exactly as-is for backward compatibility.

---

## Phase C: Telegram Progressive Editing (~100 lines in telegram_bot.py)

### C1: StreamingMessage Class

Uses `NousTelegramBot` instance and its `_tg()` method (review P0: NOT python-telegram-bot):

```python
class StreamingMessage:
    """Manages progressive message editing for Telegram streaming."""

    def __init__(self, bot: NousTelegramBot, chat_id: int):
        self._bot = bot
        self.chat_id = chat_id
        self.message_id: int | None = None
        self.text = ""
        self._last_edit = 0.0
        self._min_interval = 1.2  # N6: ~20 edits/msg limit
        self._pending = False

    async def update(self, new_text: str) -> None:
        """Update message text. Creates on first call, edits after."""
        self.text = new_text
        now = time.time()
        if now - self._last_edit < self._min_interval:
            self._pending = True
            return
        await self._send_or_edit()

    async def append_tool_indicator(self, tool_name: str) -> None:
        """Add tool usage indicator."""
        indicators = {
            "web_search": "ðŸ” Searching...",
            "web_fetch": "ðŸŒ Fetching...",
            "recall_deep": "ðŸ§  Remembering...",
            "record_decision": "ðŸ“ Recording...",
            "learn_fact": "ðŸ’¡ Learning...",
            "bash": "âš™ï¸ Running...",
            "read_file": "ðŸ“„ Reading...",
            "write_file": "âœï¸ Writing...",
        }
        indicator = indicators.get(tool_name, f"ðŸ”§ {tool_name}...")
        self.text += f"\n\n{indicator}"
        await self._send_or_edit()

    async def finalize(self) -> None:
        """Send final version of message."""
        if self._pending or self.message_id is None:
            await self._send_or_edit()

    async def _send_or_edit(self) -> None:
        if not self.text.strip():
            return

        display_text = self.text

        # N7: Handle 4096 char overflow
        if len(display_text) > 4000 and self.message_id is not None:
            overflow = display_text[4000:]
            truncated = display_text[:4000] + "\n\n(continued...)"
            await self._bot._tg("editMessageText", params={
                "chat_id": self.chat_id,
                "message_id": self.message_id,
                "text": truncated,
            })
            result = await self._bot._send(self.chat_id, overflow)
            if isinstance(result, dict) and "message_id" in result:
                self.message_id = result["message_id"]
            self.text = overflow
            self._last_edit = time.time()
            self._pending = False
            return

        if self.message_id is None:
            result = await self._bot._send(self.chat_id, display_text)
            if isinstance(result, dict) and "message_id" in result:
                self.message_id = result["message_id"]
        else:
            # No parse_mode during streaming (review B3: partial markdown breaks)
            await self._bot._tg("editMessageText", params={
                "chat_id": self.chat_id,
                "message_id": self.message_id,
                "text": display_text,
            })
        self._last_edit = time.time()
        self._pending = False
```

### C2: Streaming Chat Method on NousTelegramBot

Add as method on `NousTelegramBot` class:

```python
async def _chat_streaming(self, chat_id: int, text: str) -> None:
    """Send message to Nous streaming API and progressively edit Telegram message."""
    await self._tg("sendChatAction", params={"chat_id": chat_id, "action": "typing"})

    session_id = self._sessions.get(chat_id)
    payload: dict[str, Any] = {"message": text}
    if session_id:
        payload["session_id"] = session_id

    streamer = StreamingMessage(self, chat_id)

    try:
        async with self._http.stream(
            "POST",
            f"{self.nous_url}/chat/stream",
            json=payload,
        ) as response:
            if response.status_code != 200:
                error_body = await response.aread()
                await self._send(chat_id, f"âŒ Error: {error_body.decode()[:200]}")
                return

            # Store session (use from response headers or default)
            if not session_id:
                self._sessions[chat_id] = payload.get("session_id", str(chat_id))

            async for line in response.aiter_lines():
                if not line.startswith("data: "):
                    continue
                event = json.loads(line[6:])

                if event.get("type") == "text_delta":
                    streamer.text += event.get("text", "")
                    await streamer.update(streamer.text)
                elif event.get("type") == "tool_start":
                    await streamer.append_tool_indicator(event.get("tool_name", ""))
                elif event.get("type") == "error":
                    await self._send(chat_id, f"âŒ {event.get('text', 'Unknown error')}")
                    return
                elif event.get("type") == "done":
                    break

    except httpx.TimeoutException:
        await self._send(chat_id, "â± Request timed out.")
    except Exception as e:
        logger.error("Streaming chat error: %s", e)
        await self._send(chat_id, f"âŒ Error: {e}")
    finally:
        await streamer.finalize()
```

### C3: Wire Into _handle_update

Modify `_handle_update` to use streaming by default (fallback to non-streaming on error):

```python
# In _handle_update, replace:
#   await self._chat(chat_id, text)
# With:
    await self._chat_streaming(chat_id, text)
```

Keep `_chat()` for `/debug` command (needs response metadata).

### C4: Add Import

```python
import time
```

---

## Phase D: Tests (~350 lines in tests/test_streaming.py)

### Test Structure

```python
# tests/test_streaming.py

import json
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from nous.api.runner import StreamEvent, _parse_sse_event, AgentRunner

# --- SSE Parser Tests ---

class TestParseSSEEvent:
    """Tests for _parse_sse_event() â€” the pure function."""

    def test_text_delta(self):
        """text_delta parsed with correct text."""

    def test_tool_use_start(self):
        """content_block_start with tool_use returns tool_start event."""

    def test_text_block_start(self):
        """content_block_start with text returns text_block_start event."""

    def test_input_json_delta(self):
        """input_json_delta returns tool_input_delta with block_index."""

    def test_message_delta_stop_reason(self):
        """N3: stop_reason from message_delta.delta.stop_reason."""

    def test_block_stop(self):
        """content_block_stop returns block_stop with index."""

    def test_ping_returns_none(self):
        """N4: ping events return None (skip gracefully)."""

    def test_in_stream_error(self):
        """N2: error event parsed with type and message."""

    def test_unknown_event_returns_none(self):
        """Unknown event types return None."""

    def test_message_stop(self):
        """message_stop event parsed."""


# --- Block Accumulator Tests ---

class TestBlockAccumulator:
    """Tests for per-block-index tool input JSON accumulation (N1)."""

    def test_single_tool_fragments_reassembled(self):
        """Single tool: JSON fragments joined and parsed."""

    def test_multiple_tools_separate_accumulators(self):
        """Two parallel tools with different block_index stay separate."""

    def test_empty_tool_input(self):
        """Empty input_parts â†’ {}."""

    def test_malformed_json_fallback(self):
        """Malformed JSON â†’ {} (graceful fallback)."""


# --- StreamEvent Dataclass Tests ---

class TestStreamEvent:
    """Tests for StreamEvent construction."""

    def test_defaults(self):
        """Verify default values."""

    def test_all_fields(self):
        """All fields set correctly."""


# --- Streaming Chat Integration Tests ---

class TestStreamChat:
    """Tests for AgentRunner.stream_chat() â€” mocked API + cognitive layer."""

    @pytest.fixture
    def mock_cognitive(self):
        """Mock CognitiveLayer with pre_turn/post_turn."""

    @pytest.fixture
    def mock_settings(self):
        """Settings with test values."""

    @pytest.fixture
    def runner(self, mock_cognitive, mock_settings):
        """AgentRunner with mocked internals."""

    async def test_simple_text_streams(self, runner):
        """Text-only response yields text_delta events then done."""

    async def test_tool_call_mid_stream(self, runner):
        """Tool call: accumulate input, execute, resume streaming."""

    async def test_multi_tool_parallel(self, runner):
        """Two tool_use blocks in one response dispatched correctly."""

    async def test_api_error_propagated(self, runner):
        """HTTP error yields error event."""

    async def test_in_stream_error_propagated(self, runner):
        """N2: In-stream error event propagated and stream stops."""

    async def test_max_turns_safety(self, runner):
        """Max turns reached â†’ final no-tools call."""

    async def test_post_turn_always_called(self, runner):
        """post_turn fires even on error (try/finally)."""

    async def test_tool_results_tracked(self, runner):
        """ToolResult objects built with timing for post_turn."""

    async def test_safety_net_called(self, runner):
        """_check_safety_net runs after streaming."""

    async def test_conversation_messages_stored(self, runner):
        """Message objects (not dicts) stored in conversation."""

    async def test_agent_id_plumbed(self, runner):
        """agent_id passed to pre_turn and post_turn."""

    async def test_tool_exception_handled(self, runner):
        """Tool dispatch exception caught, is_error=True."""


# --- Telegram StreamingMessage Tests ---

class TestStreamingMessage:
    """Tests for StreamingMessage progressive editing."""

    async def test_first_update_creates_message(self):
        """First call sends new message."""

    async def test_subsequent_updates_edit(self):
        """Later calls edit existing message."""

    async def test_debounce_respects_interval(self):
        """Updates within 1.2s are deferred."""

    async def test_finalize_sends_pending(self):
        """Finalize flushes deferred updates."""

    async def test_tool_indicator_appended(self):
        """Tool indicator text added correctly."""

    async def test_overflow_splits_message(self):
        """N7: Messages >4096 chars split into new message."""
```

### Test Count: 23 cases matching spec acceptance criteria

---

## Implementation Order

| Step | Phase | Description | Depends On |
|------|-------|-------------|-----------|
| 1 | A1 | `StreamEvent` dataclass | â€” |
| 2 | A2 | `_parse_sse_event()` function | A1 |
| 3 | A3 | `_build_api_payload()` helper + refactor `_call_api` | â€” |
| 4 | A4 | `_call_api_stream()` method | A1, A2, A3 |
| 5 | A5 | `stream_chat()` method | A4, all runner internals |
| 6 | B1-B3 | REST `/chat/stream` endpoint | A5 |
| 7 | C1-C4 | Telegram `StreamingMessage` + `_chat_streaming()` | B1 |
| 8 | D | All tests | A-C complete |

## Non-Goals (from spec)

- WebSocket support (SSE is sufficient)
- Streaming for `end_conversation` reflection (N5: stays non-streaming)
- Usage tracking from streaming events (can add later)
- Concurrent session locking (documented limitation)

## Key Review Corrections Applied

| Review Issue | Fix Applied |
|-------------|-------------|
| `_build_headers()` / `self._base_url` | Use `self._http` with relative paths |
| `_get_or_create_session()` | â†’ `_get_or_create_conversation()` |
| `pre_turn(message, session_id)` | â†’ `pre_turn(agent_id, session_id, message, conversation_messages=...)` |
| `dispatch()` returns dict | â†’ `tuple[str, bool]` |
| `post_turn(msg, text, sid)` | â†’ `post_turn(agent_id, session_id, TurnResult, TurnContext)` |
| `model_name` | â†’ `model` |
| System prompt as string | â†’ `cache_control` list format (via `_build_api_payload`) |
| `_get_tools_for_frame()` | â†’ `self._dispatcher.available_tools(frame_id)` |
| `request.app.state.components` | â†’ closure pattern in `create_app()` |
| Telegram `context.bot` / standalone handler | â†’ `NousTelegramBot._tg()` / class method |
| Missing `agent_id` | â†’ plumbed through `stream_chat(agent_id=...)` |
| Missing tool result tracking | â†’ `all_tool_results: list[ToolResult]` with timing |
| Missing try/finally for post_turn | â†’ guaranteed cleanup in finally block |
| Missing safety net | â†’ `_check_safety_net()` in finally block |
| Missing max_turns graceful handling | â†’ final `_call_api(tools=None)` call |
| Markdown parse failure on partial | â†’ no `parse_mode` during streaming edits |
