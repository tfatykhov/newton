# Research Note 010: Summarization & Trimming Strategy

*Pre-computed vs query-time summarization. How memories compress.*

## The Core Tension

**Pre-computed summaries:** Fast, cheap, but one-size-fits-all. The summary of "Chose Postgres" is the same whether you're asking about databases or deployment.

**Query-time summaries:** Perfect relevance, but requires an LLM call per memory per query. Expensive and slow.

**Nous's answer: Both, in layers.**

## Three-Tier Summarization

### Tier 1: Pre-Computed (at write time)

Every record gets a static summary when created or updated. Generated once, stored, reused.

```python
class Summarizer:
    def decision_summary(self, decision) -> str:
        """Template-based, no LLM needed."""
        parts = [decision.description]
        if decision.outcome != "pending":
            parts.append(f"[{decision.outcome}]")
        parts.append(f"[{decision.confidence:.0%} confidence]")
        if decision.pattern:
            parts.append(f"— Pattern: {decision.pattern}")
        return " ".join(parts)
        # "Chose Postgres + pgvector for unified storage [success] [85% confidence] — Pattern: Unify to reduce surface area"
    
    def fact_summary(self, fact) -> str:
        """Facts ARE summaries. Just the content."""
        return fact.content
        # "PostgreSQL pgvector uses ivfflat for ANN indexing"
    
    def episode_summary(self, episode) -> str:
        """LLM-generated at episode close. Stored once."""
        # Generated by LLM when episode ends
        return episode.summary
        # "3-hour session with Tim designing Nous's storage architecture. Decided on Postgres + pgvector. Key insight: unified storage beats multi-service."
    
    def procedure_summary(self, procedure) -> str:
        """Template from name + core patterns."""
        core = "; ".join(procedure.core_patterns[:3])
        eff = f" [{procedure.effectiveness:.0%} effective]" if procedure.activation_count >= 5 else ""
        return f"{procedure.name}: {core}{eff}"
        # "Architecture Decision: Query past decisions; Evaluate 2+ alternatives; Record with confidence [92% effective]"
    
    def censor_summary(self, censor) -> str:
        """Direct from fields."""
        return f"{censor.action.upper()}: {censor.trigger_pattern} — {censor.reason}"
        # "BLOCK: Pushing directly to main — Always use feature branches + PR"
```

**Cost: Zero at query time.** Summaries are just column reads.

### Tier 2: Query-Relevant Extraction (at context assembly time)

When building context, extract the *relevant portion* of a memory for the current query. Not a full re-summarization — a focused extraction.

```python
class RelevantExtractor:
    async def extract_relevant(self, record, query: str, max_tokens: int) -> str:
        """Extract the parts of a record most relevant to the query.
        
        For simple records: just return the pre-computed summary.
        For complex records (long episodes, detailed procedures): 
        LLM extracts the relevant portion.
        """
        
        summary_tokens = count_tokens(record.summary)
        
        # If summary fits and record isn't complex, use it
        if summary_tokens <= max_tokens and not self.is_complex(record):
            return record.summary
        
        # For complex records, do targeted extraction
        if self.is_complex(record):
            return await self.llm_extract(record, query, max_tokens)
        
        # Summary too long, truncate intelligently
        return self.smart_truncate(record.summary, max_tokens)
    
    def is_complex(self, record) -> bool:
        """Does this record benefit from query-specific extraction?"""
        if record.type == "episode" and record.detail and len(record.detail) > 1000:
            return True
        if record.type == "procedure" and len(record.core_patterns) > 5:
            return True
        return False
    
    async def llm_extract(self, record, query: str, max_tokens: int) -> str:
        """Use LLM to extract relevant portion. CACHED."""
        
        cache_key = f"{record.id}:{hash(query)}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        prompt = f"""Extract the parts most relevant to: "{query}"
From this {record.type}:
{record.full_text[:2000]}

Return only the relevant information in {max_tokens} tokens or less."""
        
        result = await self.llm.generate(prompt, max_tokens=max_tokens)
        await self.cache.set(cache_key, result, ttl=3600)  # Cache 1 hour
        return result
```

**Cost: Occasional LLM call for complex records.** Cached aggressively. Most records use Tier 1.

### Tier 3: Expansion (on demand during conversation)

When the agent needs full detail on a specific memory. Triggered explicitly, not automatically.

```python
class ContextManager:
    async def expand(self, memory_id: str) -> str:
        """Agent requests full detail on a specific memory.
        
        This is NOT injected into system prompt.
        It's returned as content the LLM can reference.
        """
        record = await self.get_full_record(memory_id)
        
        if record.type == "decision":
            return self.format_full_decision(record)
            # Includes: all reasons, all thoughts, full context, 
            # outcome details, related decisions
        
        elif record.type == "episode":
            return record.detail or record.summary
            # Full narrative if available
        
        elif record.type == "procedure":
            return self.format_full_procedure(record)
            # All bands: goals, core, implementation notes
```

**Cost: One DB read.** Only happens when agent specifically asks "tell me more about decision X."

## Episode Summarization Pipeline

Episodes are the most complex summarization challenge. They start as raw interaction logs and need progressive compression.

```
Real-time          → Session End           → 30 days            → 90 days
(full transcript)     (LLM summary)          (trimmed)            (archived)
```

### At Session End

```python
async def close_episode(self, episode_id: str, transcript: str):
    """Generate episode summary when session ends."""
    
    # 1. LLM generates structured summary
    summary = await self.llm.generate(f"""
Summarize this interaction session in 100-150 words:

{transcript[:8000]}

Include:
- What was the main task/topic
- What decisions were made (if any)
- What was the outcome
- Any surprises or lessons learned
- Who was involved
""")
    
    # 2. Extract facts
    facts = await self.llm.generate(f"""
Extract discrete facts learned during this session.
Return as a JSON array of strings. Only include NEW information, not common knowledge.

{transcript[:8000]}
""")
    
    # 3. Identify decision points
    decisions = await self.llm.generate(f"""
List any decisions or choices made during this session.
For each, note: what was decided, confidence level, reasoning.
Return as JSON array.

{transcript[:8000]}
""")
    
    # 4. Update episode record
    await self.heart.update_episode(episode_id, 
        summary=summary,
        detail=transcript[:10000],  # Keep first 10K chars of detail
        lessons_learned=extract_lessons(summary),
        ended_at=now()
    )
    
    # 5. Store extracted facts
    for fact in parse_json(facts):
        await self.heart.learn(fact, source_episode_id=episode_id)
    
    # 6. Link decisions
    for dec in parse_json(decisions):
        # Match to Brain decisions recorded during this episode
        await self.heart.link_episode_decision(episode_id, dec.id)
```

### At 30 Days (Trimming)

```python
async def trim_old_episodes(self, days=30):
    """Trim detail from episodes older than threshold."""
    
    old_episodes = await self.heart.list_episodes(
        older_than_days=days,
        has_detail=True
    )
    
    for episode in old_episodes:
        # Ensure summary exists and is good
        if not episode.summary or len(episode.summary) < 50:
            episode.summary = await self.regenerate_summary(episode)
        
        # Ensure facts were extracted
        linked_facts = await self.heart.get_episode_facts(episode.id)
        if not linked_facts:
            await self.extract_facts_from_episode(episode)
        
        # Trim detail to first 2000 chars (keep some for context)
        await self.heart.update_episode(episode.id,
            detail=episode.detail[:2000] if episode.detail else None
        )
```

### At 90 Days (Archiving)

```python
async def archive_old_episodes(self, days=90):
    """Archive episodes — keep only summary and metadata."""
    
    old_episodes = await self.heart.list_episodes(
        older_than_days=days,
        not_archived=True
    )
    
    for episode in old_episodes:
        # Remove detail entirely
        await self.heart.update_episode(episode.id,
            detail=None  # Summary, metadata, and links preserved
        )
```

## Fact Deduplication & Consolidation

Facts accumulate. Without management, you get:
- "Tim likes Celsius" (learned Jan 15)
- "Tim prefers Celsius over Fahrenheit" (learned Jan 20)
- "Use Celsius for Tim" (learned Feb 1)

Three records, one fact.

```python
async def learn_with_dedup(self, content: str, **kwargs) -> str:
    """Learn a new fact, checking for duplicates first."""
    
    embedding = await self.embed(content)
    
    # Search for similar existing facts
    similar = await self.heart.search_facts(
        content, embedding, limit=5, min_score=0.85
    )
    
    if similar:
        best_match = similar[0]
        
        if best_match.score > 0.95:
            # Near-duplicate. Just confirm the existing fact.
            await self.heart.confirm_fact(best_match.id)
            return best_match.id
        
        elif best_match.score > 0.85:
            # Similar but different wording. Check if semantically identical.
            is_same = await self.llm.generate(f"""
Are these two facts saying the same thing? Reply YES or NO.
Fact 1: {best_match.content}
Fact 2: {content}
""")
            if "YES" in is_same.upper():
                # Same fact, better wording? Keep the longer/more specific one.
                if len(content) > len(best_match.content):
                    await self.heart.update_fact(best_match.id, content=content)
                await self.heart.confirm_fact(best_match.id)
                return best_match.id
    
    # Genuinely new fact. Store it.
    return await self.heart.store_fact(content, embedding=embedding, **kwargs)
```

## Procedure Compression

Procedures grow as implementation_notes accumulate. Periodically compress:

```python
async def compress_procedure(self, procedure_id: str):
    """Compress a procedure's lower fringe."""
    
    proc = await self.heart.get_procedure(procedure_id)
    
    if len(proc.implementation_notes) > 10:
        # Too many notes. Consolidate.
        consolidated = await self.llm.generate(f"""
These implementation notes for "{proc.name}" have accumulated over time.
Consolidate them into 5 or fewer essential notes. Remove redundancy.

Notes:
{chr(10).join(f'- {n}' for n in proc.implementation_notes)}
""")
        
        await self.heart.update_procedure(procedure_id,
            implementation_notes=parse_list(consolidated)
        )
    
    # Promote patterns that appear in implementation_notes to core
    # (if they've been useful across multiple activations)
```

## Token Counting

Fast, approximate token counting without calling a tokenizer:

```python
def count_tokens(text: str) -> int:
    """Approximate token count. ~4 chars per token for English."""
    if not text:
        return 0
    return len(text) // 4 + 1

def count_tokens_accurate(text: str) -> int:
    """Accurate count using tiktoken. Use for final budget check."""
    import tiktoken
    enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))
```

Use fast approximation during scoring/ranking. Use accurate count for final assembly check.

## Summary of Approach

| Stage | Method | Cost | When |
|-------|--------|------|------|
| Write time | Template-based summary | Free | Every record |
| Episode close | LLM summary + fact extraction | 1 LLM call | End of session |
| Context assembly | Pre-computed summaries | Free (DB read) | Every turn |
| Complex records | LLM extraction (cached) | Rare LLM call | Complex episodes/procedures |
| Expansion | Full record load | DB read | On explicit request |
| 30-day trim | Remove episode detail | Free | Background job |
| 90-day archive | Strip to summary only | Free | Background job |
| Fact dedup | Semantic match + optional LLM | Occasional LLM | Every fact learned |
| Procedure compress | LLM consolidation | Occasional LLM | When notes > 10 |

**Most context assembly is zero LLM cost** — just database reads of pre-computed summaries. LLM is used for: episode closing, complex extraction (cached), fact dedup (occasional), and procedure compression (rare).

---

*The cheapest token is the one you don't send. The most valuable token is the one that changes the answer.*
